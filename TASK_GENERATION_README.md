# AI Task Generation System - Complete Guide

## ðŸ“‹ Tá»•ng Quan

Há»‡ thá»‘ng **AI Task Generation** tá»± Ä‘á»™ng sinh task tá»« tÃ i liá»‡u requirements, sá»­ dá»¥ng káº¿t há»£p **Machine Learning** vÃ  **Template-based Generation** Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng cao vÃ  á»•n Ä‘á»‹nh.

### Kiáº¿n TrÃºc

```
Input (Requirements Doc)
    â†“
[1] Segmentation â†’ sections + sentences
    â†“
[2] Requirement Detection â†’ ML classifier (binary)
    â†“
[3] Enrichment â†’ type/priority/domain (ML) + role (rule-based)
    â†“
[4] Task Generation â†’ Template + NLP parsing
    â†“
[5] Post-processing â†’ dedupe + filter
    â†“
Output (Tasks JSON) â†’ [Optional] Effort Estimation
```

---

## ðŸš€ Quick Start

### BÆ°á»›c 1: Chuáº©n Bá»‹ Dataset

Báº¡n Ä‘Ã£ cÃ³ 3 datasets sáºµn:
- `dataset_small_10k/` - Äá»ƒ test nhanh
- `dataset_medium_100k/` - Äá»ƒ training baseline
- `dataset_large_1m/` - Äá»ƒ training production model

### BÆ°á»›c 2: Cháº¡y Data Pipeline

```bash
cd /home/dtu/AI-Project/AI-Project

# Activate virtual environment
source venv/bin/activate

# BÆ°á»›c 2.1: Scan dataset (xem cháº¥t lÆ°á»£ng dá»¯ liá»‡u)
python scripts/task_generation/01_scan_dataset.py \
    --dataset requirement_analyzer/dataset_medium_100k \
    --output report/data_quality_report

# BÆ°á»›c 2.2: Clean vÃ  convert sang Parquet (Ä‘á»ƒ training nhanh)
python scripts/task_generation/02_build_parquet.py \
    --input requirement_analyzer/dataset_medium_100k \
    --output data/processed \
    --min-length 10 \
    --max-length 1000

# BÆ°á»›c 2.3: Táº¡o train/val/test splits (stratified)
python scripts/task_generation/03_build_splits.py \
    --input data/processed \
    --output data/splits \
    --train-size 0.8 \
    --val-size 0.1 \
    --test-size 0.1
```

### BÆ°á»›c 3: Train ML Models

```bash
# BÆ°á»›c 3.1: Train Requirement Detector (binary classifier)
python scripts/task_generation/04_train_requirement_detector.py \
    --data-dir data/splits \
    --output-dir models/task_gen \
    --model-type sgd

# BÆ°á»›c 3.2: Train Enrichers (type/priority/domain classifiers)
python scripts/task_generation/05_train_enrichers.py \
    --data-dir data/splits \
    --output-dir models/task_gen \
    --labels type priority domain
```

Káº¿t quáº£ models sáº½ Ä‘Æ°á»£c lÆ°u trong `models/task_gen/`:
```
models/task_gen/
â”œâ”€â”€ requirement_detector_vectorizer.joblib
â”œâ”€â”€ requirement_detector_model.joblib
â”œâ”€â”€ requirement_detector_metrics.json
â”œâ”€â”€ type_vectorizer.joblib
â”œâ”€â”€ type_model.joblib
â”œâ”€â”€ type_classes.json
â”œâ”€â”€ priority_vectorizer.joblib
â”œâ”€â”€ priority_model.joblib
â”œâ”€â”€ priority_classes.json
â”œâ”€â”€ domain_vectorizer.joblib
â”œâ”€â”€ domain_model.joblib
â”œâ”€â”€ domain_classes.json
â””â”€â”€ enrichers_summary.json
```

### BÆ°á»›c 4: Khá»Ÿi Äá»™ng API

```bash
# CÃ i dependencies náº¿u chÆ°a cÃ³
pip install spacy
python -m spacy download en_core_web_sm

# Start API server
cd requirement_analyzer
python api.py
```

Hoáº·c sá»­ dá»¥ng uvicorn:
```bash
uvicorn requirement_analyzer.api:app --host 0.0.0.0 --port 8000 --reload
```

API sáº½ cháº¡y táº¡i: http://localhost:8000

---

## ðŸ“¡ API Endpoints

### 1. Generate Tasks from Text

**POST** `/generate-tasks`

```json
{
  "text": "The system shall support user authentication...",
  "max_tasks": 50,
  "mode": "template",
  "include_story_points": true,
  "domain_hint": "healthcare",
  "epic_name": "User Management"
}
```

**Response:**
```json
{
  "tasks": [
    {
      "task_id": "uuid",
      "title": "Implement user authentication",
      "description": "...",
      "acceptance_criteria": ["...", "..."],
      "type": "security",
      "priority": "High",
      "domain": "healthcare",
      "role": "Backend",
      "story_points": 5,
      "confidence": 0.87,
      "source": {
        "sentence": "The system shall support user authentication...",
        "section": "Security Requirements"
      }
    }
  ],
  "total_tasks": 15,
  "stats": {
    "type_distribution": {"functional": 8, "security": 5, "interface": 2},
    "avg_confidence": 0.82
  },
  "processing_time": 2.3
}
```

### 2. Generate Tasks + Estimate Effort

**POST** `/generate-tasks-estimate`

Tá»± Ä‘á»™ng tÃ­nh story points vÃ  effort cho tá»«ng task.

### 3. Upload Document and Generate Tasks

**POST** `/upload-requirements-generate-tasks`

- Upload file: `.txt`, `.pdf`, `.docx`, `.md`
- Tá»± Ä‘á»™ng parse + generate tasks

### 4. Submit Feedback (Learning Loop)

**POST** `/tasks/feedback`

```json
{
  "task_id": "uuid",
  "accepted": true,
  "edited_task": {...},
  "comment": "Need more specific AC"
}
```

---

## ðŸ§ª Testing

### Test thá»­ vá»›i sample text:

```python
import requests

text = """
The system must support user registration with email verification.
Users shall be able to login using email and password.
The application should display a dashboard after successful login.
All user data must be encrypted at rest and in transit.
The system needs to support role-based access control.
"""

response = requests.post(
    "http://localhost:8000/generate-tasks",
    json={
        "text": text,
        "max_tasks": 10,
        "domain_hint": "general"
    }
)

tasks = response.json()
print(f"Generated {tasks['total_tasks']} tasks")
for task in tasks['tasks']:
    print(f"- [{task['type']}] {task['title']} (Priority: {task['priority']})")
```

---

## ðŸ“Š Model Performance

Sau khi training, check metrics:

```bash
cat models/task_gen/requirement_detector_metrics.json
cat models/task_gen/enrichers_summary.json
```

Expected performance (medium dataset):
- **Requirement Detector**: F1 > 0.85, PR-AUC > 0.90
- **Type Classifier**: Macro-F1 > 0.75
- **Priority Classifier**: Macro-F1 > 0.70
- **Domain Classifier**: Macro-F1 > 0.80

---

## ðŸ”§ Customization

### ThÃªm Type Má»›i

Edit `requirement_analyzer/task_gen/generator_templates.py`:

```python
self.templates['custom_type'] = {
    'title_template': 'Your template here',
    'description_template': '...',
    'ac_templates': ['...']
}
```

### ThÃªm Role Mapping Rules

Edit `requirement_analyzer/task_gen/enrichers.py` â†’ `RoleAssigner`:

```python
self.role_patterns['NewRole'] = [
    r'\bnew_keyword\b',
    r'\banother_pattern\b'
]
```

### Tune Deduplication Threshold

```python
from requirement_analyzer.task_gen import get_postprocessor

postprocessor = get_postprocessor(
    similarity_threshold=0.90,  # higher = less aggressive
    min_task_length=15
)
```

---

## ðŸŽ¯ Use Cases

### 1. Jira/Trello Integration

```python
from requirement_analyzer.task_gen import get_pipeline

pipeline = get_pipeline()
response = pipeline.generate_tasks(requirement_doc)

# Export to Jira format
for task in response.tasks:
    jira_issue = {
        "summary": task.title,
        "description": task.description,
        "issuetype": {"name": "Story"},
        "priority": {"name": task.priority},
        "labels": [task.type, task.domain],
        "customfield_storypoints": task.story_points
    }
    # POST to Jira API...
```

### 2. Sprint Planning

```python
# Generate tasks
tasks = pipeline.generate_tasks(sprint_requirements)

# Group by priority and story points
high_priority = [t for t in tasks if t.priority == 'High']
total_points = sum(t.story_points for t in high_priority)

print(f"High priority: {len(high_priority)} tasks, {total_points} points")
```

### 3. Requirements Quality Check

```python
# Check requirement coverage
response = pipeline.generate_tasks(requirements_doc)

if response.total_tasks < 5:
    print("âš ï¸  Too few requirements detected. Document may be incomplete.")

low_confidence = [t for t in response.tasks if t.confidence < 0.5]
if low_confidence:
    print(f"âš ï¸  {len(low_confidence)} tasks have low confidence")
```

---

## ðŸ› Troubleshooting

### Models not loading
```
âš ï¸  Task generation pipeline not available
```
**Fix**: Train models first using scripts 04 and 05.

### spaCy model error
```
OSError: Can't find model 'en_core_web_sm'
```
**Fix**: 
```bash
python -m spacy download en_core_web_sm
```

### Memory issues with large dataset
```
MemoryError during training
```
**Fix**: Use smaller chunksize or switch to `dataset_medium_100k`.

### Low task generation quality
**Fix**: 
1. Check data quality report
2. Increase training data size
3. Adjust confidence threshold in detection

---

## ðŸ“ˆ Roadmap

### Current (v1.0 - Template Mode)
- âœ… ML-based requirement detection
- âœ… Multi-class enrichment (type/priority/domain)
- âœ… Template-based generation
- âœ… Rule-based role assignment
- âœ… Deduplication & filtering

### Next (v1.1)
- [ ] RAG mode with LLM (optional)
- [ ] Fine-tuned role classifier
- [ ] Feedback loop with retraining
- [ ] Dependency detection between tasks

### Future (v2.0)
- [ ] Fine-tune T5 for textâ†’JSON generation
- [ ] Multi-language support (Vietnamese)
- [ ] Integration with GitHub Issues, Azure DevOps
- [ ] Automated sprint planning

---

## ðŸ“š Architecture Details

### Why Template-based First?

1. **á»”n Ä‘á»‹nh**: JSON luÃ´n valid, khÃ´ng hallucinate
2. **Giáº£i thÃ­ch Ä‘Æ°á»£c**: Rule-based, dá»… debug
3. **Nhanh**: KhÃ´ng cáº§n LLM API calls
4. **Production-ready**: Cháº¡y offline, khÃ´ng phá»¥ thuá»™c external services

### Dataset Schema

```
text,is_requirement,type,priority,domain
"System shall...",1,functional,High,ecommerce
"Figure 3 shows...",0,non_requirement,none,general
```

- `is_requirement`: Binary (0/1)
- `type`: functional, security, interface, data, performance, etc.
- `priority`: Low, Medium, High, none
- `domain`: ecommerce, iot, healthcare, education, finance, general

---

## ðŸ¤ Contributing

Cáº¥u trÃºc code:

```
requirement_analyzer/
  task_gen/
    __init__.py           # Public API
    schemas.py            # Pydantic models
    segmenter.py          # Document â†’ sentences
    req_detector.py       # Binary classifier
    enrichers.py          # Multi-class classifiers + role
    generator_templates.py # Template-based generation
    postprocess.py        # Dedupe + filter
    pipeline.py           # Orchestrator
```

Äá»ƒ thÃªm feature má»›i:
1. Táº¡o module trong `task_gen/`
2. Update `pipeline.py` Ä‘á»ƒ integrate
3. ThÃªm endpoint trong `api.py`
4. Viáº¿t test

---

## ðŸ“ž Support

Náº¿u gáº·p issue:
1. Check logs: `requirement_analyzer.api` logger
2. Verify models loaded: `GET /health`
3. Test vá»›i small document trÆ°á»›c

---

**Version**: 1.0.0  
**Last Updated**: 2026-01-20  
**Maintained by**: AI-Project Team
