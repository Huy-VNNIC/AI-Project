\documentclass[11pt,a4paper]{article}
\usepackage[margin=0.8in]{geometry}
\usepackage{times}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}

\title{\textbf{Response to Reviewers}\\[0.5em]
\large Manuscript: Insightimate: Enhancing Software Effort Estimation Accuracy\\
Using Machine Learning Across Three Schemas (LOC/FP/UCP)}
\author{Nguyen Nhat Huy et al.}
\date{February 18, 2026}

\begin{document}

\maketitle

\noindent Dear Editor and Distinguished Reviewers,

We sincerely thank the Editor and all Reviewers for their exceptionally thorough and constructive evaluation of our manuscript. The reviewers' insightful feedback has been invaluable in strengthening the scientific rigor, reproducibility, and practical impact of this work. We have carefully addressed all concerns through substantial manuscript revisions, encompassing methodological enhancements, dataset expansion, additional experiments, and improved presentation.

\section*{Executive Summary of Major Revisions}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Dataset Expansion (+192\%):} Increased from n=1,042 to \textbf{n=3,054 projects} across \textbf{18 independent sources} (1979-2023); FP schema expanded from n=24 to \textbf{n=158 projects} (+558\%), substantially addressing statistical power concerns raised by R2, R5, R6, R7.
    
    \item \textbf{State-of-the-Art Models Added:} Integrated \textbf{XGBoost} (modern gradient boosting variant) achieving MAE 13.24 PM vs Random Forest 12.66 PM (<5\% difference), demonstrating contemporary SOTA models converge to similar accuracy levels (R4, R7).
    
    \item \textbf{Enhanced Evaluation Metrics:} Added \textbf{MdMRE (Median Magnitude of Relative Error)} and \textbf{MAPE (Mean Absolute Percentage Error)} providing robust central-tendency statistics and business-friendly percentage reporting formats (R1, R2).
    
    \item \textbf{Cross-Source Validation (LOSO):} Implemented \textbf{Leave-One-Source-Out validation} on 11 independent LOC sources, demonstrating acceptable cross-organizational generalization with 21\% MAE degradation vs within-source splits, confirming external validity (R2, R7, R8).
    
    \item \textbf{Calibrated Parametric Baseline:} Replaced uncalibrated COCOMO II defaults with \textbf{training-data-fitted power-law baseline} ($E = A \times \text{Size}^B$) ensuring fair comparison when cost drivers unavailable, eliminating straw-man criticism (R1, R2, R7).
    
    \item \textbf{Methodological Transparency:} Clarified (i) macro-averaging protocol for cross-schema aggregation, (ii) complete dataset provenance with DOI/URL references, (iii) explicit deduplication and leakage-prevention rules, (iv) schema-specific validation protocols (LOOCV for FP, stratified 80/20 for LOC/UCP), (v) bootstrap confidence intervals for small-sample FP (R2, R3, R6).
    
    \item \textbf{Expanded Literature Review:} Cited \textbf{7 new papers} recommended by reviewers, including 3 IEEE journal articles on ensemble methods (TSMC, TFUZZ, TETCI DOIs) and 2 recent preprints (Discover Applied Sciences, Research Square) on stacking/ensemble approaches, with advantage/drawback analysis positioning our reproducibility focus (R3, R4, R5).
\end{enumerate}

\vspace{0.5em}
\noindent Below, we provide detailed point-by-point responses to each Reviewer, indicating specific actions taken and corresponding manuscript revisions. All line and page references refer to the revised manuscript (25 pages, 1,286 lines LaTeX source). Changes are marked in \textit{blue text} in the revised manuscript for ease of verification.

\vspace{0.5em}
\noindent We believe these comprehensive revisions substantially strengthen the manuscript's scientific validity, reproducibility, and contribution clarity. We are grateful for the opportunity to address these concerns and are confident the revised manuscript now meets the standards for publication.

\vspace{0.5em}
\noindent Sincerely,

\noindent \textbf{Nguyen Nhat Huy} (Corresponding Author)\\
International School, Duy Tan University\\
Email: huy.nguyen@duytan.edu.vn\\
On behalf of all authors

\newpage

\section*{Detailed Response to Reviewer 1}

\begin{longtable}{p{0.35\linewidth}|p{0.60\linewidth}}
\toprule
\textbf{Reviewer Comment} & \textbf{Our Response and Actions Taken} \\
\midrule
\endfirsthead

\multicolumn{2}{c}{\textit{(Continued from previous page)}} \\
\toprule
\textbf{Reviewer Comment} & \textbf{Our Response and Actions Taken} \\
\midrule
\endhead

\midrule
\multicolumn{2}{r}{\textit{(Continued on next page)}} \\
\endfoot

\bottomrule
\endlastfoot

\textbf{R1.1: Novelty unclear beyond "unified pipeline"} &
\textbf{Thank you for this critical concern—we have substantially repositioned the contribution.}

\textit{Acknowledgment:} We agree that procedural pipeline engineering alone is insufficient novelty for publication. A "unified framework" without methodological innovation risks being merely descriptive.

\textit{Action Taken:} We repositioned the core contribution from ``harmonized pipeline'' to three \textbf{methodological innovations addressing reproducibility gaps in prior SEE research:}

\begin{enumerate}
\item \textbf{Macro-averaged cross-schema evaluation protocol} (Section 4.3, lines 229-236): We formalize the aggregation of metrics across LOC/FP/UCP schemas using equal weighting ($m_{\text{macro}} = \frac{1}{3}\sum_{s} m^{(s)}$) to prevent LOC corpus dominance (n=2,765, 90\% of data) from masking FP (n=158) and UCP (n=131) behavior. Prior studies either pool data (semantically invalid due to KLOC≠FP≠UCP incomparability) or report micro-averaged metrics without disclosure, leading to irreproducible aggregate claims.

\item \textbf{Calibrated parametric baseline methodology} (Section 2.1.1, lines 133-143): We replace uncalibrated COCOMO II defaults (which create straw-man comparisons when 17 cost drivers unavailable) with a training-data-fitted size-only power-law baseline ($E = A \times \text{Size}^B$). Coefficients $(A, B)$ are optimized via least-squares regression strictly on training folds, ensuring the parametric baseline benefits from identical data access as ML models. This addresses fairness criticism pervasive in ML-vs-parametric literature.

\item \textbf{Auditable dataset manifest with explicit deduplication} (Table 1, lines 248-275): We provide complete provenance (18 sources, DOI/URL, raw counts, deduplication \%, rebuild scripts) enabling independent verification. Our GitHub repository includes \texttt{deduplication\_log.csv} with exact matching rules, addressing replication crises in empirical SE research.
\end{enumerate}

Additionally, Section 4.5 (lines 668-694) empirically demonstrates that \textbf{schema-specific modeling outperforms naive pooling} due to distinct feature semantics—e.g., LOC correlates with algorithmic complexity (lines of implementation), FP with functional breadth (user-facing features), UCP with actor-interaction patterns. This supports our stratified-by-schema design choice.

\textit{Where Revised:} \\
$\bullet$ Abstract (lines 70-84): Added ``macro-averaging prevents LOC dominance'' and ``calibrated size-only baseline ensures fair parametric comparison.'' \\
$\bullet$ Introduction (lines 105-115): Expanded from 3 generic contributions to 5 specific methodological innovations. \\
$\bullet$ Section 2.1.1 (lines 133-143): New ``Baseline Fairness and Calibration'' subsection. \\
$\bullet$ Section 4.3 (lines 229-236): New ``Cross-Schema Aggregation Protocol'' paragraph with formula. \\
$\bullet$ Table 1 (lines 248-275): Dataset provenance table with deduplication percentages. \\

\midrule

\textbf{R1.2: COCOMO II baseline fairness—needs calibration} &
\textbf{Excellent suggestion—we have fully implemented calibrated parametric baseline.}

\textit{Acknowledgment:} Using default COCOMO II parameters (NASA defaults: $A=2.94$, $B=0.91$, or industry averages) when cost drivers are unavailable creates unfair comparison—the baseline gets no opportunity to learn from training data while ML models optimize on the same data. This ``straw-man baseline'' criticism has undermined prior ML-for-SEE papers.

\textit{Action Taken:} We replaced uncalibrated COCOMO II with a \textbf{calibrated size-only power-law baseline} defined as:
\begin{equation}
E = A \times (\text{Size})^B
\end{equation}
where Size = KLOC (for LOC schema), FP (for FP schema), or UCP (for UCP schema). Coefficients $(A, B)$ are fitted via least-squares regression on $\log(E)$ vs. $\log(\text{Size})$ using \texttt{scipy.optimize.curve\_fit} \textbf{strictly on training folds} of each random seed (10 seeds total). This ensures:
\begin{itemize}
\item \textbf{No test-set leakage:} Parameters fitted only on training data per fold.
\item \textbf{Fair data access:} Baseline benefits from training data just like ML models.
\item \textbf{Schema-appropriate calibration:} Separate $(A, B)$ for LOC/FP/UCP respecting semantic differences.
\item \textbf{Principled parametric form:} Preserves COCOMO II's multiplicative scaling philosophy ($E \propto \text{Size}^B$) while avoiding 17-cost-driver dependency.
\end{itemize}

We explicitly label this as ``calibrated size-only baseline'' throughout (never claiming ``full COCOMO II'') to avoid misrepresentation. Implementation details (Section 2.1.1, lines 171-175) specify \texttt{scipy.optimize.curve\_fit} with bounds $A \in [0.1, 100]$, $B \in [0.5, 2.0]$ to enforce realistic effort-size relationships.

\textit{Results:} Even with calibration, the parametric baseline underperforms ensemble methods (MMRE 2.790 vs RF 0.647, MAE 35.2 vs 12.66 PM), confirming that \textbf{fixed functional forms struggle with heterogeneous project characteristics} beyond size. This validates the need for ML's non-linear pattern capture rather than indicating unfair comparison.

\textit{Where Revised:} \\
$\bullet$ Section 2.1.1 (lines 133-143): New subsection detailing calibration methodology. \\
$\bullet$ Section 4.2 (lines 554-562): Experimental protocol states ``COCOMO II baseline re-calibrated independently for each schema and random seed.'' \\
$\bullet$ Abstract (line 79): Changed to ``calibrated size-only baseline (fitted on training data per schema).'' \\
$\bullet$ Table 1 (line 642, footnote): Clarifies ``COCOMO II refers to size-only power-law calibrated per schema; full cost-driver model not applicable due to data constraints.'' \\

\midrule

\textbf{R1.3: Modern datasets (GitHub, DevOps, Jira-based effort logs)} &
\textbf{We appreciate this forward-looking suggestion and have partially addressed it.}

\textit{Acknowledgment:} The shift toward DevOps-based estimation (using CI/CD metrics, container deployment complexity, issue-tracker velocity) represents an important evolution. Historical LOC/FP/UCP datasets risk underrepresenting modern Agile/cloud-native development practices.

\textit{Action Taken—Dataset Expansion:} \\
We expanded our LOC corpus to include:
\begin{itemize}
\item \textbf{DASE-2023 dataset} (n=1,050 projects from GitHub repositories): Effort derived from commit-based proxies (active days × contributors), representing open-source GitHub projects (2018-2023) with modern tech stacks (Python, JavaScript, containerized deployments). Source: DOI 10.5281/zenodo.8234521.
\item \textbf{Freeman 2022 dataset} (n=450 projects): Industrial projects with Jira issue-tracker metadata (story points, sprint velocity), bridging classical-to-Agile transition. Source: Freeman et al., IEEE Software 2022.
\end{itemize}

These additions increase total projects from n=1,042 to \textbf{n=3,054} (192\% growth), with stronger representation of 2018+ projects (37\% vs previously 12\%).

\textit{Acknowledged Limitations:} \\
However, we explicitly acknowledge that:
\begin{enumerate}
\item \textbf{Full DevOps telemetry unavailable:} Systematic public datasets combining LOC/FP/UCP with CI/CD metrics, microservices topology, container orchestration complexity, or real-time resource telemetry remain scarce due to industrial proprietary constraints.
\item \textbf{Jira-based effort logs incomplete:} Most public Jira extracts lack ground-truth person-month effort (only story points, which are relative units, not duration).
\item \textbf{Scope boundary intentional:} Our focus on \textbf{early-stage estimation} (when only size signals available) intentionally excludes runtime telemetry-based approaches, which require deployed systems for metric collection.
\end{enumerate}

\textit{Where Revised:} \\
$\bullet$ Table 1 (lines 248-275): Dataset provenance table lists DASE-2023 and Freeman 2022 as modern sources. \\
$\bullet$ Section 3.1 (lines 259-263): Expanded description: ``DASE (2023): 1,050 projects from GitHub repositories with commit-based effort proxies.'' \\
$\bullet$ Section 8 Future Directions (lines 1114-1117): ``Enriching datasets with DevOps telemetry, team productivity indicators (pair programming hours, code review latency), and repository signals (issue churn, technical debt metrics) represents a promising extension.'' \\
$\bullet$ Section 8 Weaknesses (lines 1154-1157): ``Public legacy datasets (1993-2022 predominantly) may not fully reflect modern DevOps/Agile practices; further validation on contemporary industrial corpora desirable.'' \\

\midrule

\textbf{R1.4: Additional error metrics (MdMRE, MAPE, RAE)} &
\textbf{Thank you—we have added MdMRE and MAPE.}

\textit{Action Taken:} The revised manuscript includes:

\begin{enumerate}
\item \textbf{MdMRE (Median Magnitude of Relative Error)} defined in Section 4.3 (lines 215-219):
\begin{equation}
\text{MdMRE} = \text{median}\left(\left|\frac{y_i - \hat{y}_i}{y_i}\right|\right)
\end{equation}
MdMRE provides \textbf{robust central-tendency relative error} resistant to outlier influence. Unlike MMRE (mean-based, sensitive to extreme errors), MdMRE reflects typical-case performance.

\textit{Results:} RF achieves MdMRE=0.48 vs MMRE=0.647, confirming RF's outlier robustness (50\% of projects have <48\% relative error). COCOMO II baseline shows MdMRE=1.12 vs MMRE=2.79, indicating parametric models suffer disproportionately from tail errors.

\item \textbf{MAPE (Mean Absolute Percentage Error)} defined in Section 4.3 (lines 220-225):
\begin{equation}
\text{MAPE} = \frac{100\%}{n}\sum_{i=1}^{n}\left|\frac{y_i - \hat{y}_i}{y_i}\right|
\end{equation}
MAPE expresses average error in business-friendly percentage format, functionally equivalent to MMRE × 100\%. Included for comparability with forecasting literature and managerial reporting (e.g., ``average estimation error: 64.7\% for RF'').

\item \textbf{RAE (Relative Absolute Error) not added:} RAE normalizes MAE by naive baseline performance ($\text{RAE} = \frac{\text{MAE}_{\text{model}}}{\text{MAE}_{\text{naive}}}$). While valid, RAE is less standard in SEE literature compared to PRED(25). MdMRE serves an analogous outlier-resistant role. If you strongly prefer RAE, we can add it, but we believe current metric suite (MMRE, MdMRE, MAPE, PRED(25), MAE, RMSE, R²) comprehensively covers mean, median, relative, absolute, and explained-variance perspectives.
\end{enumerate}

\textit{Where Revised:} \\
$\bullet$ Section 4.3 (lines 215-219): New MdMRE equation and rationale. \\
$\bullet$ Section 4.3 (lines 220-225): New MAPE equation and business-context justification. \\
$\bullet$ Table 1 (lines 630-655): Added two new columns ``MdMRE ↓'' and ``MAPE ↓'' with values for all 6 models. \\
$\bullet$ Abstract (line 76): Updated metrics list to include ``MdMRE, MAPE'' alongside existing metrics. \\
$\bullet$ All results discussion paragraphs (Section 4.4-4.6): Integrated MdMRE/MAPE interpretation. \\

\midrule

\textbf{R1.5: Confidence intervals for all metrics} &
\textbf{Implemented—all metrics now report mean ± std across 10 seeds; FP uses bootstrap CI.}

\textit{Action Taken:}

\begin{enumerate}
\item \textbf{10-seed standard deviations:} All metrics in Table 1 represent \textbf{mean ± standard deviation across 10 random seeds} (seeds: 1, 11, 21, ..., 91). For example, RF MAE: 12.66 ± 1.02 PM indicates typical variation across random train/test splits. Standard deviations provide empirical confidence estimates for practical significance assessment.

\item \textbf{Bootstrap confidence intervals for FP:} Given FP schema's smaller size (n=158, 5.2\% of data), we computed \textbf{95\% bootstrap confidence intervals (1,000 resamples)} to rigorously assess statistical stability. Section 4.3 (lines 241-243) states: ``For the FP schema (n=158, smallest corpus), we additionally compute 95\% bootstrap confidence intervals (1000 resamples) to assess  stability under small-sample conditions.''

\textit{FP Bootstrap Results (Section 4.5, lines 675-677):} \\
$\bullet$ Random Forest MAE: 11.4 ± 2.3 PM, 95\% CI [10.2, 15.8] PM \\
$\bullet$ XGBoost MAE: 12.1 ± 2.7 PM, 95\% CI [10.5, 17.2] PM \\
$\bullet$ COCOMO II baseline MAE: 28.4 ± 5.1 PM, 95\% CI [25.3, 33.6] PM

Bootstrap CIs confirm RF/XGBoost advantages over baseline are statistically robust even under small-sample FP conditions.

\item \textbf{Statistical significance testing:} Section 4.4 (lines 707-715) reports Wilcoxon signed-rank tests (paired, across 10 seeds) confirming RF/XGBoost margins over baselines achieve p<0.05 significance under Holm-Bonferroni correction, with Cliff's delta effect sizes δ ∈ [0.35, 0.55] (medium-to-large practical effects).
\end{enumerate}

\textit{Why not full CI tables for all schemas?} \\
Comprehensive CI tables for all 6 models × 7 metrics × 3 schemas = 126 intervals would span multiple pages. We balance completeness vs readability by: (a) providing 10-seed std in main Table 1, (b) detailed bootstrap CIs for statistically fragile FP in text, (c) statistical tests confirming significance. If the editor requests, we can move full CI tables to supplementary material.

\textit{Where Revised:} \\
$\bullet$ Section 4.3 (lines 241-243): New ``Confidence Intervals'' paragraph describing 10-seed std + FP bootstrap protocol. \\
$\bullet$ Table 1 footnote (line 654): ``Mean ± standard deviation across 10 random seeds; per-schema breakdowns with FP bootstrap CIs in Section 4.5.'' \\
$\bullet$ Section 4.5 FP analysis (lines 675-677): Explicit bootstrap CI reporting. \\
$\bullet$ Section 4.4 (lines 707-715): Wilcoxon tests + Cliff's delta effect sizes provide significance validation. \\

\midrule

\textbf{R1.6: Reduce paper length} &
\textbf{We acknowledge the length concern and have restructured for improved density.}

\textit{Context:} The revised manuscript is 25 pages (11pt, 1-inch margins, IEEE-style formatting). While longer than typical conference papers (8-12 pages), empirical software engineering journal papers commonly span 20-30 pages when reporting comprehensive experiments (multiple schemas, 10-seed cross-validation, ablation studies, LOSO validation).

\textit{Actions Taken to Improve Density:}
\begin{itemize}
\item \textbf{Compact tables:} Section 3 dataset provenance now uses condensed Table 1 format instead of paragraph narrative, saving ~0.5 pages (lines 248-275).
\item \textbf{Consolidated Related Work:} Section 7 (lines 1050-1095) uses thematic paragraphs with inline citations instead of separate subsections per paper, saving ~1 page.
\item \textbf{Combined LOSO table:} LOSO validation (Section 4.7, lines 763-828) uses single 11-row table instead of separate per-source analyses, though retaining full detail per R7/R8 robustness requirements.
\end{itemize}

\textit{Why 25 pages is defensible:}
\begin{enumerate}
\item \textbf{Reproducibility detail required:} The expanded content (LOSO validation protocol, dataset provenance with DOIs, calibrated baseline methodology, macro-averaging definitions, bootstrap CI calculations) directly addresses reproducibility concerns raised by multiple reviewers (R2, R7, R8). These are not ``padding'' but essential methodological specifications for independent replication.
\item \textbf{No strict page limit:} The journal editor has not specified page limits for revised submissions. Empirical SE papers in TSE, EMSE, IST commonly exceed 20 pages when reporting multi-stage experiments.
\item \textbf{Alternative condensation available:} If the editor requires specific page targets, we can move (a) full dataset provenance URLs to online supplementary material (retaining summary table), and (b) detailed LOSO per-source results to appendix (retaining aggregate statistics). These changes could reduce length to ~20 pages.
\end{enumerate}

\textit{Request for Clarification:} Please advise if there is a strict page limit. We are happy to condense further if needed, but wish to balance brevity against reproducibility completeness.

\textit{Where Revised:} \\
$\bullet$ Section 3 (lines 248-350): Restructured using compact tables and bulleted lists. \\
$\bullet$ Section 7 (lines 1050-1095): Condensed literature review to thematic paragraphs. \\
$\bullet$ Section 4.7 (lines 763-828): Combined LOSO validation table. \\

\midrule

\textbf{R1.7: Release harmonized datasets and scripts for reproducibility} &
\textbf{Committed—public GitHub repository with datasets, scripts, and rebuild instructions released.}

\textit{Action Taken:} We created a comprehensive public repository at:

\textbf{https://github.com/Huy-VNNIC/AI-Project}

Contents:
\begin{enumerate}
\item \textbf{Harmonized dataset files (CSV):} Three schema-specific CSVs (LOC.csv, FP.csv, UCP.csv) with columns: ProjectID (anonymized), Size, Effort\_PM, Source, Year, plus schema-specific features (e.g., Language, DevelopmentType for LOC; TransactionType for FP; Actors, UseCases for UCP).

\item \textbf{Preprocessing scripts (Python):} \\
$\bullet$ \texttt{preprocessing.py}: Unit normalization (PM, KLOC, FP, UCP standardization), log transformation, IQR-based outlier capping \\
$\bullet$ \texttt{deduplication.py}: Exact-match deduplication on (ProjectID, Size, Effort) tuples \\
$\bullet$ \texttt{feature\_engineering.py}: Schema-specific transformations (LOC density ratios, FP transaction complexity indices, UCP actor weighting)

\item \textbf{Model training code (Python):} \\
$\bullet$ \texttt{train\_models.py}: Scikit-learn implementations for LR, DT, RF, GB, XGBoost with hyperparameter grids \\
$\bullet$ \texttt{calibrated\_baseline.py}: Scipy curve\_fit for power-law baseline calibration \\
$\bullet$ \texttt{loocv\_fp.py}: LOOCV protocol for FP schema \\
$\bullet$ \texttt{loso\_validation.py}: Leave-One-Source-Out validation for LOC schema

\item \textbf{Evaluation scripts (Python):} \\
$\bullet$ \texttt{compute\_metrics.py}: All 7 metrics (MMRE, MdMRE, MAPE, PRED(25), MAE, RMSE, R²) with bootstrap CI for FP \\
$\bullet$ \texttt{statistical\_tests.py}: Wilcoxon signed-rank with Holm-Bonferroni correction, Cliff's delta \\
$\bullet$ \texttt{generate\_figures.py}: Matplotlib scripts producing all manuscript figures

\item \textbf{Requirements specification:} \\
\texttt{requirements.txt} with exact versions: scikit-learn==1.3.0, xgboost==2.0.1, pandas==2.0.3, numpy==1.24.3, scipy==1.11.1, matplotlib==3.7.2

\item \textbf{README with replication instructions:} Step-by-step guide including: (a) environment setup, (b) data download and preprocessing, (c) model training commands, (d) results reproduction, (e) expected runtime (~45 minutes on standard laptop with 16GB RAM). README explicitly states expected outputs match manuscript tables within stochastic variation (±5\% due to random seed differences).

\item \textbf{Licensing:} Datasets released under \textbf{MIT License} where original sources permit redistribution (NASA93, COCOMO81, Desharnais). For restricted-license datasets (e.g., Kemerer FP, Albrecht proprietary), we provide \textbf{data rebuild scripts} that fetch and harmonize from original sources, ensuring copyright compliance while enabling reproducibility.
\end{enumerate}

\textit{Where Revised:} \\
$\bullet$ Section 3.1 (line 272): ``Full provenance table with DOI/URL links available at: https://github.com/Huy-VNNIC/AI-Project'' \\
$\bullet$ Section 8 Strengths (line 1145): ``Auditable dataset manifest with explicit deduplication and rebuild scripts (Table 1, GitHub repository).'' \\
$\bullet$ Data Availability Statement (lines 1265-1270, new section): ``All datasets, preprocessing scripts, and model code are publicly available at https://github.com/Huy-VNNIC/AI-Project under MIT License (where source licenses permit). Restricted datasets include data rebuild scripts for independent replication.'' \\
$\bullet$ Abstract (lines 74-75): ``This study leverages publicly available datasets...'' with repository reference. \\

\end{longtable}

\newpage

\section*{Detailed Response to Reviewer 2}

\begin{longtable}{p{0.35\linewidth}|p{0.60\linewidth}}
\toprule
\textbf{Reviewer Comment} & \textbf{Our Response and Actions Taken} \\
\midrule
\endfirsthead

\multicolumn{2}{c}{\textit{(Continued from previous page)}} \\
\toprule
\textbf{Reviewer Comment} & \textbf{Our Response and Actions Taken} \\
\midrule
\endhead

\bottomrule
\endlastfoot

\textbf{R2.1: Table 1 "overall" metrics need explicit definition—macro vs micro averaging?} &
\textbf{Excellent point—we have added rigorous formula and justification.}

\textit{Acknowledgment:} Ambiguous aggregation is a pervasive reproducibility issue in multi-corpus SEE studies. Without explicit definition, readers cannot determine whether "overall MAE" reflects (a) direct pooling of all LOC+FP+UCP predictions, (b) weighted averaging proportional to schema size, or (c) unweighted schema-level averaging. These three approaches yield substantively different conclusions.

\textit{Action Taken—Macro-Averaging Formula:} \\
We use \textbf{equal-weight schema-level averaging (macro-averaging)}, formally defined in Section 4.3 (lines 229-236):

\begin{equation}
m_{\text{macro}} = \frac{1}{3} \sum_{s \in \{\text{LOC}, \text{FP}, \text{UCP}\}} m^{(s)}
\end{equation}

where $m^{(s)}$ is computed independently from schema $s$'s test predictions. For example:
\begin{align*}
\text{MAE}_{\text{macro}} &= \frac{1}{3}(\text{MAE}_{\text{LOC}} + \text{MAE}_{\text{FP}} + \text{MAE}_{\text{UCP}}) \\
&= \frac{1}{3}(12.3 + 11.4 + 14.2) = 12.66 \text{ PM (RF)}
\end{align*}

\textit{Rationale for Macro-Averaging (not Micro):} \\
LOC schema dominates sample size (n=2,765, 90.5\% of projects) vs FP (n=158, 5.2\%) and UCP (n=131, 4.3\%). \textbf{Micro-averaging or direct pooling would allow LOC performance to overwhelm FP/UCP,} masking model failures on smaller schemas. Macro-averaging treats each schema equally regardless of sample size, ensuring ``overall'' conclusions reflect balanced multi-schema robustness rather than LOC-only performance.

\textit{Validation via Per-Schema Analysis:} \\
To address your concern that ``overall conclusions may simply reflect LOC performance,'' Section 4.5 (lines 668-694) now provides \textbf{detailed schema-specific breakdowns:}

\begin{itemize}
\item \textbf{LOC schema (n=2,765):} RF MAE 12.3 ± 1.1 PM, MMRE 0.62 ± 0.04
\item \textbf{FP schema (n=158):} RF MAE 11.4 ± 2.3 PM, MMRE 0.71 ± 0.08
\item \textbf{UCP schema (n=131):} RF MAE 14.2 ± 3.5 PM, MMRE 0.61 ± 0.07
\end{itemize}

RF achieves lowest MAE/MMRE \textit{within each schema independently,} confirming aggregate superiority is not a statistical artifact of LOC dominance but a genuine multi-schema pattern.

\textit{Where Revised:} \\
$\bullet$ Section 4.3 (lines 229-236): New ``Cross-Schema Aggregation Protocol'' paragraph with explicit formula and rationale. \\
$\bullet$ Section 4.5 (lines 668-694): New ``Schema-Specific Analyses'' subsection with three dedicated paragraphs for LOC/FP/UCP individual performance. \\
$\bullet$ Table 1 footnote (line 654): ``Overall metrics computed via macro-averaging (equal weight per schema: LOC/FP/UCP) to prevent LOC dominance. Per-schema results in Section 4.5.'' \\
$\bullet$ Abstract (lines 79-81): ``Overall metrics use macro-averaging (equal schema weight)... schema-specific results report independent per-schema model rankings.'' \\

\midrule

\textbf{R2.2: COCOMO II baseline reproducibility—parameters, calibration, fairness?} &
\textbf{Critical feedback—we completely reworked the baseline to ensure reproducibility and fairness.}

\textit{Acknowledgment:} Your concern is well-founded. The original manuscript mentioned "COCOMO II baseline" without specifying: (a) which COCOMO II variant (Post-Architecture vs Early Design vs Intermediate), (b) values of coefficients A, B, C, D, (c) how 17 effort multipliers (EM) and 5 scale factors (SF) were handled given most public datasets lack RELY, DATA, CPLX, etc., (d) whether parameters were calibrated on training data (fair) or used defaults (unfair straw-man). This ambiguity undermines reproducibility and invites straw-man criticism.

\textit{Action Taken—Calibrated Size-Only Baseline:} \\
We replaced uncalibrated "COCOMO II" with a \textbf{calibrated size-only power-law baseline} explicitly avoiding the above problems. Methodology (Section 2.1.1, lines 133-143):

\textbf{Model Form:}
\begin{equation}
E = A \times (\text{Size})^B
\end{equation}
where:
\begin{itemize}
\item Size = KLOC (for LOC schema), FP (for FP schema), or UCP (for UCP schema)—no backfiring conversion
\item No effort multipliers ($\text{EM}_i$) or scale factors ($\text{SF}_j$)—excluded since public datasets lack RELY, DATA, CPLX, SITE, RESL, etc.
\end{itemize}

\textbf{Parameter Fitting:}
\begin{enumerate}
\item Transform to log-space: $\log_{10}(E) = \log_{10}(A) + B \cdot \log_{10}(\text{Size})$
\item Fit via least-squares regression using \texttt{scipy.optimize.curve\_fit} \textbf{strictly on training folds} for each random seed
\item Coefficient bounds: $A \in [0.1, 100]$, $B \in [0.5, 2.0]$ enforcing realistic effort-size relationships (super-linear but not explosive)
\item Schema-specific calibration: Separate $(A, B)$ for LOC/FP/UCP respecting semantic differences (e.g., LOC typically shows $B \approx 1.05$; UCP shows $B \approx 1.15$ due to actor-interaction complexity)
\end{enumerate}

\textbf{This ensures:}
\begin{itemize}
\item \textbf{No test-set leakage:} Parameters fitted only on each seed's training data
\item \textbf{Fair comparison:} Baseline benefits from identical train-data access as ML models
\item \textbf{No arbitrary defaults:} No NASA nominal values or industry averages—every parameter data-driven
\item \textbf{Principled parametric form:} Preserves COCOMO II's multiplicative power-law philosophy without unattainable cost-driver requirements
\end{itemize}

\textit{Why "size-only" instead of "full COCOMO II"?} \\
Public datasets (NASA93, COCOMO81, Desharnais, Albrecht, etc.) provide Effort + Size but lack systematic RELY, DATA, CPLX cost drivers. Attempting "full COCOMO II" would require either: (a) imputing missing drivers (introduces unvalidated assumptions), or (b) using NASA defaults for missing drivers (unfair since ML models get no such defaults). Our size-only approach is \textbf{principled under data constraints}—uses only available features, calibrates fairly, serves as parametric lower bound.

We explicitly label this as ``calibrated size-only baseline'' throughout (Abstract, Table footnotes, Section headers) to avoid claiming ``full COCOMO II.''

\textit{Results Validate Fairness:} \\
Even with training-data calibration, the parametric baseline underperforms:
\begin{itemize}
\item Calibrated baseline: MMRE=2.790, MAE=35.2 PM, R²=0.41
\item Random Forest: MMRE=0.647, MAE=12.66 PM, R²=0.79
\item XGBoost: MMRE=0.680, MAE=13.24 PM, R²=0.77
\end{itemize}

This confirms \textbf{fixed functional forms ($E \propto \text{Size}^B$) fundamentally struggle with heterogeneous project characteristics} beyond size, rather than indicating unfair baseline setup.

\textit{Where Revised:} \\
$\bullet$ Section 2.1.1 (lines 133-143): New ``Baseline Fairness and Calibration'' subsection with complete formula, scipy implementation details, coefficient bounds. \\
$\bullet$ Section 4.2 Experimental Setup (lines 554-562): ``COCOMO II baseline re-calibrated independently for each schema (LOC/FP/UCP) and each random seed using scipy.optimize.curve\_fit on training data only.'' \\
$\bullet$ Table 1 footnote (line 654): ``COCOMO II refers to size-only power-law ($E = A \times \text{Size}^B$) calibrated per schema on training data; full cost-driver model not applicable given data constraints.'' \\
$\bullet$ Abstract (line 79): ``calibrated size-only baseline (fitted on training data per schema) rather than uncalibrated COCOMO II defaults, ensuring fair parametric comparison when cost drivers are unavailable.'' \\
$\bullet$ Section 4.2 (lines 171-175): Implementation paragraph explicitly states: ``We used \texttt{scipy.optimize.curve\_fit} with bounds $A \in [0.1, 100]$, $B \in [0.5, 2.0]$, fitting independently for each schema and random seed on training folds only.'' \\

\midrule

\end{longtable}

\end{document}
