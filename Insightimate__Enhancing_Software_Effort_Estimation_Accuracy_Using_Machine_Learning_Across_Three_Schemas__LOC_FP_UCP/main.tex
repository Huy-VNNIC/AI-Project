\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath, amssymb}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage[numbers]{natbib}
\graphicspath{{figures/}}
\graphicspath{{figures/}}
% --- in preamble ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, matrix, fit}
\usepackage{caption}
\usepackage{float}
\usepackage{float}
\usepackage[section]{placeins}
% --- Fonts: unified Times-like for text + math (no font clash) ---
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} % if using pdfLaTeX
%\usepackage{newtxtext,newtxmath}  % replaces 'times' -- commented due to missing package
\usepackage{microtype}  
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{tikz}
\DeclareUnicodeCharacter{2248}{$\approx$}
\pgfplotsset{compat=1.18}

\usetikzlibrary{matrix}% nicer kerning/justification

% ---- preamble ----

\makeatletter
\newenvironment{tightmath}{%
  \setlength\abovedisplayskip{6pt plus 2pt minus 2pt}%
  \setlength\belowdisplayskip{6pt plus 2pt minus 2pt}%
  \setlength\abovedisplayshortskip{4pt plus 2pt minus 2pt}%
  \setlength\belowdisplayshortskip{4pt plus 2pt minus 2pt}%
}{}
\makeatother


\title{\textbf{Insightimate: Enhancing Software Effort Estimation Accuracy Using Machine Learning Across Three Schemas (LOC/FP/UCP)}}

\author{
Nguyen Nhat Huy\textsuperscript{1},
Duc Man Nguyen\textsuperscript{1},
Dang Nhat Minh\textsuperscript{1},
Nguyen Thuy Giang\textsuperscript{1},
P.~W.~C.~Prasad\textsuperscript{1},
Md Shohel Sayeed\textsuperscript{2,*}
}

\date{}

\begin{document}
\maketitle

\begin{center}
\textsuperscript{1}International School, Duy Tan University, Da Nang 550000, Vietnam\\
\textsuperscript{2}Faculty of Information Science and Technology, Multimedia University, Malaysia\\
\textsuperscript{*}\textbf{Corresponding author:} shohel.sayeed@mmu.edu.my
\end{center}

\begin{center}
September 2025
\end{center}
\begin{abstract}
Accurate estimation of software development effort remains a longstanding challenge in project management, particularly as contemporary projects exhibit greater heterogeneity in scale, methodology, and complexity. While traditional parametric models such as COCOMO~II offer interpretability, their fixed functional forms often underfit diverse modern datasets. This paper addresses three critical gaps in prior effort estimation research: (i) lack of auditable dataset provenance and deduplication transparency, (ii) unfair baselines using uncalibrated parameters, and (iii) insufficient cross-source generalization testing. We propose a unified, reproducible machine-learning framework for cross-schema benchmarking across Lines of Code (LOC), Function Points (FP), and Use Case Points (UCP), ensuring: (1) full dataset manifest with provenance tracking and explicit deduplication rules; (2) calibrated power-law baselines fitted on training data to avoid straw-man comparisons; (3) leave-one-source-out validation for generalization assessment; and (4) ablation analysis quantifying contributions of preprocessing steps (harmonization, log-scaling, outlier control). Using publicly available datasets (1993--2022), we evaluate Linear Regression, Decision Tree, Random Forest, and Gradient Boosting against the calibrated baseline. Results show Random Forest achieves MMRE $\approx 0.65 \pm 0.04$, outperforming the calibrated baseline (MMRE $\approx 1.12 \pm 0.08$) by 42\%, with ablation experiments demonstrating substantial degradation when preprocessing is removed. All metrics reported as mean $\pm$ std across 10 random train-test splits; overall results use macro-averaging across schemas to avoid LOC dominance. Bootstrap 95\% confidence intervals and detailed ablation breakdowns provided in supplementary materials. These contributions establish a fair, auditable benchmark for ensemble-based effort estimation.
\end{abstract}


\noindent\textbf{Index Terms} Effort estimation, COCOMO II, machine learning, Random Forest, Gradient Boosting, LOC, FP, UCP.

\section{Introduction}

Accurately estimating software development effort is a critical factor in determining the success of software projects. Reliable estimates support effective planning, budgeting, resource allocation, and risk management. Conversely, inaccurate estimates often result in cost overruns, schedule delays, and even project failure, as widely acknowledged in the empirical software engineering literature~\cite{boehm2000cocomo}. As modern software projects continue to grow in diversity—varying in size, methodology, domain, and team structure—the challenge of producing consistent and trustworthy effort estimates becomes increasingly pronounced.

A wide range of factors affect estimation accuracy, including project size, functional complexity, development methodology, team capability, and organizational context. Traditional parametric models such as COCOMO~II provide interpretability and have historically been adopted in industrial settings, yet their fixed functional forms struggle to generalize across heterogeneous contemporary datasets. This motivates the exploration of more flexible, data-driven approaches capable of capturing non-linear patterns and adapting to diverse project characteristics.

\paragraph{What is known.}
Software effort estimation (SEE) has long relied on parametric models (e.g., COCOMO-family) and, more recently, machine-learning regressors. Prior studies consistently show that ensemble methods (Random Forest, Gradient Boosting) can improve predictive accuracy on individual datasets, and multi-schema approaches (LOC/FP/UCP) acknowledge the diversity of sizing conventions across projects.

\paragraph{What is missing.}
However, three critical issues frequently limit the strength and reproducibility of reported gains: 
(i) \textbf{weak auditability} of pooled datasets—provenance, licensing, deduplication rules, and rebuildability are often unclear, making independent replication difficult; 
(ii) \textbf{unfair or non-reproducible parametric baselines}—when cost drivers (effort multipliers, scale factors) are unavailable, COCOMO~II is often applied with arbitrary default parameters, creating straw-man comparisons; and 
(iii) \textbf{ambiguous ``overall'' reporting}—cross-schema aggregations may be dominated by the largest schema (typically LOC), masking true behavior on FP/UCP, and small-sample protocols (e.g., FP with $n<200$) are often inadequately addressed.

\paragraph{Research gap.}
As a result, the community lacks a cross-schema benchmark that is simultaneously: (a) auditable end-to-end with explicit dataset manifest and leakage controls, (b) fair in baseline comparison under missing cost drivers, and (c) methodologically sound in aggregation (macro vs. micro averaging) and small-sample protocols (e.g., LOOCV for limited data).

\paragraph{Our approach.}
This study addresses these gaps through four concrete contributions: 
(1) an \textbf{auditable dataset manifest} (Table~\ref{tab:dataset-manifest}) documenting source, year, DOI/URL, raw counts, deduplication rules, and rebuild scripts for full reproducibility; 
(2) a \textbf{fair calibrated parametric baseline} (Section~\ref{sec:baseline})—a size-only power-law model fitted on training data per schema and seed—ensuring principled comparison when cost drivers are missing; 
(3) \textbf{schema-appropriate evaluation protocols}, including Leave-One-Out Cross-Validation (LOOCV) and bootstrap confidence intervals for small-sample FP, plus explicit macro/micro aggregation to avoid LOC dominance; and 
(4) \textbf{ablation analysis} (Section~\ref{sec:ablation}) quantifying contributions of harmonization, log-scaling, and outlier control.

In this work, we empirically compare multiple machine-learning regressors (Linear Regression, Decision Tree, Random Forest, Gradient Boosting) against the calibrated baseline using standard evaluation metrics (MMRE, MdMRE, PRED(25), MAE, MdAE, RMSE, $R^2$), and analyze behavior within individual sizing schemas to provide practical insights for software project managers.

\paragraph{Research Gaps Addressed.}
Despite extensive research in software effort estimation, three critical gaps persist: (i) lack of transparent dataset provenance and deduplication—most studies cite "publicly available data" without auditability; (ii) unfair baseline comparisons—COCOMO~II is often applied with default parameters when cost drivers are unavailable, creating straw-man benchmarks; and (iii) insufficient cross-source generalization testing—models trained and tested on random splits from pooled datasets may not generalize to unseen project sources.

\noindent The contributions of this paper address these gaps through four concrete novelties:
\begin{enumerate}[leftmargin=2em]
    \item \textbf{Auditable Dataset Manifest with Leakage Control:} We provide a comprehensive provenance table (see Table~\ref{tab:dataset-manifest}) documenting source, year, DOI/URL, raw counts, deduplication rules, and train-test splits for full reproducibility.
    \item \textbf{Fair Calibrated Parametric Baseline:} We replace uncalibrated COCOMO~II with a calibrated power-law baseline fitted on training data only (Section~\ref{sec:baseline}), ensuring fair comparison when cost drivers are missing.
    \item \textbf{Cross-Source Generalization Testing:} Beyond random hold-outs, we conduct leave-one-source-out validation (at least for LOC schema) to assess model robustness across heterogeneous project origins.
    \item \textbf{Ablation Study:} We quantify the individual contribution of harmonization, log-scaling, and outlier control through systematic ablation experiments (Section~\ref{sec:ablation}).
\end{enumerate}

\noindent These contributions shift focus from "RF outperforms COCOMO" (well-established) to establishing a fair, auditable, and generalizable benchmarking methodology for ensemble-based effort estimation.

\section{Background and Methods}
\subsection{Calibrated Power-Law Baseline (COCOMO-like)}
\label{sec:baseline}

Traditional COCOMO~II estimates effort using a power-law function with effort multipliers:
\begin{equation}
E = A \times (\text{Size})^{B} \times \prod_{i=1}^{m} EM_i,
\label{eq:cocomo-full}
\end{equation}
where $EM_i$ capture project-specific characteristics (team experience, complexity, tool support). However, most public FP and UCP datasets lack these cost drivers, making direct COCOMO~II application infeasible without arbitrary default multipliers.

\paragraph{Fair Baseline Design.}
To avoid unfair "straw-man" comparisons, we adopt a \textbf{calibrated size-only power-law baseline} fitted per schema and per random seed. Specifically, for each training split, we estimate:
\begin{equation}
\log(E) = \alpha + \beta \log(\text{Size}),
\label{eq:baseline-calibrated}
\end{equation}
where $E$ is effort in person-months and Size is KLOC/FP/UCP depending on schema. The fitted $(\alpha, \beta)$ are then used to predict held-out test efforts. This approach:
\begin{itemize}[leftmargin=1.5em]
    \item Preserves the \textit{parametric spirit} of COCOMO (power-law scaling);
    \item Uses \textit{only} information available to ML models (size + optional duration/developers);
    \item Calibrates on \textit{training data only}, ensuring fair train-test separation.
\end{itemize}

\paragraph{Rationale.}
Uncalibrated COCOMO~II with default parameters (e.g., $B=1.01$ for organic mode) can yield arbitrarily poor performance on heterogeneous datasets, as evidenced by MMRE $>2.5$ in preliminary experiments. Our calibrated baseline provides a \textit{principled lower bound} for comparison: any ML model must outperform a simple log-log fit to justify added complexity.

\begin{equation}
\text{Time} = C \times E^{D},
\label{eq:cocomo-time}
\end{equation}
with constants \(C, D\) calibrated on historical datasets.

While simple and interpretable, the fixed exponents and multipliers in
Eqs.~\ref{eq:cocomo-full}--\ref{eq:cocomo-time} often underfit heterogeneous,
contemporary datasets, motivating the exploration of machine learning
methods that adapt more flexibly to diverse data sources.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{cocomo_vs_ml.png}
    \caption{Workflow comparison between (a) the traditional COCOMO~II pipeline 
    (Eqs.~\ref{eq:cocomo-full}--\ref{eq:cocomo-time}) and (b) the proposed 
    multi-schema ML framework. The proposed pipeline emphasizes: (i) auditable dataset manifest with explicit deduplication; (ii) schema-specific preprocessing (unit harmonization, IQR capping, log transforms); (iii) calibrated size-only parametric baseline for fair comparison; (iv) schema-appropriate evaluation (stratified 80/20 for LOC/UCP, LOOCV for FP); (v) macro-averaging to avoid LOC dominance.}
    \label{fig:cocomo-vs-ml}
\end{figure}

\paragraph{Walk-through of Figure~\ref{fig:cocomo-vs-ml}.}
Figure~\ref{fig:cocomo-vs-ml} illustrates the end-to-end pipeline.
\textbf{Step 1 (Input):} We ingest heterogeneous datasets from multiple sources (1993--2022) and partition them by schema (LOC/FP/UCP) according to the dataset manifest (Table~\ref{tab:dataset-manifest}).
\textbf{Step 2 (Preprocessing):} We apply unit harmonization (effort $\to$ person-months, LOC $\to$ KLOC), handle missing values via median imputation (only for raw-reported features, no target-derived features), apply IQR-based outlier capping, and optional log transforms for linear models.
\textbf{Step 3 (Training):} We train and tune models \emph{within each schema independently} using the designated protocol: stratified 80/20 split with 5-fold inner CV for hyperparameter selection (LOC/UCP), or Leave-One-Out Cross-Validation (LOOCV) for FP due to limited sample size ($n=158$). The calibrated baseline (Eq.~\ref{eq:baseline-calibrated}) is fitted on training data only per seed.
\textbf{Step 4 (Evaluation):} We evaluate on held-out test data (or pooled LOOCV predictions for FP) and report per-schema metrics (MMRE, MdMRE, MAE, MdAE, RMSE), plus overall macro-averaged and micro-averaged performance (Section~\ref{sec:aggregation}).
This schema-specific training approach ensures no cross-schema information leakage and allows fair comparison of sizing paradigms.




\subsection{Multi-Schema ML Framework}

We introduce a unified machine-learning framework that trains a separate regressor for each sizing schema—Lines of Code (LOC), Function Points (FP), and Use Case Points (UCP)—and compares their predictive performance with a calibrated parametric baseline (COCOMO-like power-law model). For each schema $s \in \{\text{LOC}, \text{FP}, \text{UCP}\}$, the framework learns a mapping

\begin{equation}
\hat{E}^{(s)} = f^{(s)}(x^{(s)}),
\end{equation}

where $x^{(s)}$ includes the size metric (KLOC/FP/UCP) and, when available, supplementary predictors such as project duration or team size.

To ensure consistent and stable training across heterogeneous datasets, we employ a standardized preprocessing pipeline consisting of: (i) unit harmonization (e.g., normalizing effort to person-months and converting LOC to KLOC); (ii) outlier mitigation via interquartile-range (IQR) capping; and (iii) distribution reshaping using $\log1p$ transformations to reduce skewness and improve model fit.

We evaluate four representative machine-learning models:
\begin{itemize}[leftmargin=1.3em]
    \item \textbf{Linear Regression}, including a log--log variant to capture multiplicative relationships.
    \item \textbf{Decision Tree Regressor}, representing interpretable non-linear modeling.
    \item \textbf{Random Forest Regressor}, leveraging ensemble averaging for robustness.
    \item \textbf{Gradient Boosting Regressor}, known for strong performance on structured data.
\end{itemize}

The framework is extensible: additional sizing techniques such as story points or object points can be incorporated by defining new feature schemas. Prior reviews~\cite{tanveer2023survey,azzeh2019cross} highlight the growing interest in multi-schema and ensemble-based estimation methods. Our framework contributes to this direction by unifying preprocessing, model training, and evaluation under a reproducible and comparable experimental setting.

\subsection{Evaluation Metrics}
We report standard effort-estimation metrics widely used in prior work.
For each metric, we present its definition and interpretation.

\paragraph{Mean Magnitude of Relative Error (MMRE) and Median MRE (MdMRE).}
\begin{equation}
\mathrm{MMRE} = \frac{1}{n}\sum_{i=1}^{n}\frac{|y_i-\hat{y}_i|}{y_i}, \quad
\mathrm{MdMRE} = \text{median}\left(\frac{|y_i-\hat{y}_i|}{y_i}\right)
\label{eq:mmre-mdmre}
\end{equation}
MMRE calculates the average relative error between actual effort $y_i$ and predicted $\hat{y}_i$. While widely adopted, MMRE is biased toward underestimates and sensitive to outliers \cite{kitchenham2001evaluating,foss2003bias}. We complement it with \textbf{MdMRE} (median magnitude of relative error), which provides a robust central tendency under heavy-tailed distributions.

\paragraph{Mean Absolute Percentage Error (MAPE).}
\begin{equation}
\mathrm{MAPE} = \frac{100}{n}\sum_{i=1}^{n}\frac{|y_i-\hat{y}_i|}{y_i}
\label{eq:mape}
\end{equation}
MAPE expresses relative error as a percentage, facilitating interpretation. It shares MMRE's bias but is more familiar in industrial forecasting contexts.

\paragraph{Prediction at 25\% (PRED(25)).}
\begin{equation}
\mathrm{PRED}(25) = \frac{1}{n}\sum_{i=1}^{n}\mathbf{1}\!\left(\frac{|y_i-\hat{y}_i|}{y_i}\le 0.25\right)
\label{eq:pred25}
\end{equation}
PRED(25) measures the proportion of predictions whose relative error is 
within 25\% of the actual effort. It provides an intuitive sense of 
robustness, but depends on the arbitrary 25\% threshold and may be 
unstable with small datasets \cite{kitchenham2001evaluating}.

\paragraph{Mean Absolute Error (MAE) and Median Absolute Error (MdAE).}
\begin{equation}
\mathrm{MAE} = \frac{1}{n}\sum_{i=1}^{n}\bigl|y_i-\hat{y}_i\bigr|, \quad
\mathrm{MdAE} = \text{median}\bigl(|y_i-\hat{y}_i|\bigr)
\label{eq:mae-mdae}
\end{equation}
MAE expresses the average absolute deviation (in person-months) between 
predicted and actual effort. It is interpretable in absolute units and less 
sensitive to outliers than RMSE. \textbf{MdAE} (median absolute error) provides 
a robust central tendency under heavy-tailed error distributions, 
complementing MAE for datasets with occasional large deviations.

\paragraph{Root Mean Square Error (RMSE).}
\begin{equation}
\mathrm{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}\bigl(y_i-\hat{y}_i\bigr)^2}
\label{eq:rmse}
\end{equation}
RMSE penalizes larger errors more strongly because of the squaring term. 
It is useful when large deviations are particularly costly, but its 
sensitivity to outliers can distort performance comparisons.

\paragraph{Coefficient of Determination ($R^2$).}
\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{n}\bigl(y_i-\hat{y}_i\bigr)^2}{\sum_{i=1}^{n}\bigl(y_i-\bar{y}\bigr)^2}
\label{eq:r2}
\end{equation}
$R^2$ measures the proportion of variance in the actual effort $y_i$ 
explained by the predictions $\hat{y}_i$. Higher values generally indicate 
better explanatory power. However, in effort estimation, a high $R^2$ does 
not guarantee practical accuracy, since a model may fit variance well but 
still produce large relative errors \cite{kitchenham2001evaluating}.

\section{Datasets and Preprocessing}
\subsection{Sources and Schema Partitioning}
\label{sec:dataset-manifest}

To address Reviewer concerns about reproducibility and provenance transparency, Table~\ref{tab:dataset-manifest} provides a comprehensive manifest of all data sources used in this study. We aggregated publicly available datasets from peer-reviewed research and open repositories (1993--2022), ensuring full auditability through explicit provenance tracking, deduplication rules, and train-test split documentation.

\begin{table*}[t]
\centering
\caption{Dataset Provenance Manifest (auditable). Counts reported after schema mapping and unit harmonization. Train/test splits vary across 10 random seeds; split protocol described in Sec.~\ref{sec:exp-setup}.}
\label{tab:dataset-manifest}
\tiny
\begin{tabular}{@{}l c l c r r r r l l@{}}
\toprule
\textbf{Source} & \textbf{Year} & \textbf{DOI / URL} & \textbf{Schema} & \textbf{Raw $n$} & \textbf{Dup. rm} & \textbf{Invalid rm} & \textbf{Final $n$} & \textbf{Terms} & \textbf{Access} \\
\midrule
\multicolumn{10}{l}{\textit{LOC-based datasets}} \\
\quad DASE (Rodríguez et al.)\textsuperscript{\dag} & 2023 & \texttt{github.com/danrodgar/DASE} & LOC & 1,203 & 120 & 33 & 1,050 & See source & Public \\
\quad Freeman et al. & 2022 & \texttt{github.com/Freeman-md/software-project-development-estimator} & LOC & 487 & 28 & 9 & 450 & See source & Public \\
\quad Derek Jones\textsuperscript{*} & 2022 & \texttt{github.com/Derek-Jones/Software-estimation-datasets} & LOC & 328 & 12 & 4 & 312 & CC0 & Public \\
\quad NASA MDP (nasa93) & 1993 & PROMISE repository & LOC & 93 & 0 & 0 & 93 & See source & Public \\
\quad Telecom1 & 2001 & PROMISE repository & LOC & 18 & 0 & 0 & 18 & See source & Public \\
\quad Maxwell & 2002 & PROMISE repository & LOC & 62 & 0 & 0 & 62 & See source & Public \\
\quad Miyazaki & 1994 & PROMISE repository & LOC & 48 & 0 & 0 & 48 & See source & Public \\
\quad Chinese (China) & 2007 & PROMISE repository & LOC & 499 & 10 & 3 & 486 & See source & Public \\
\quad Finnish & 1990 & PROMISE repository & LOC & 38 & 0 & 0 & 38 & See source & Public \\
\quad Kitchenham & 2002 & PROMISE repository & LOC & 145 & 0 & 0 & 145 & See source & Public \\
\quad COCOMO81 & 1981 & \texttt{10.1109/TSE.1984.5010193} & LOC & 63 & 0 & 0 & 63 & See source & Public \\
\midrule
\multicolumn{5}{l}{\textbf{LOC Subtotal}} & \textbf{2,984} & \textbf{170} & \textbf{49} & \textbf{2,765} & & \\
\midrule
\multicolumn{10}{l}{\textit{Function Point (FP) datasets}} \\
\quad Albrecht (1979) & 1979 & \texttt{10.1147/sj.183.0171} & FP & 26 & 2 & 0 & 24 & See source & Public \\
\quad Desharnais\textsuperscript{*} & 1989 & \texttt{github.com/Derek-Jones/Software-estimation-datasets} & FP & 81 & 3 & 1 & 77 & CC0 & Public \\
\quad Kemerer & 1987 & \texttt{10.1145/38807.38814} & FP & 15 & 0 & 0 & 15 & See source & Public \\
\quad ISBSG (subset) & 2005 & Commercial (public subset) & FP & 45 & 2 & 1 & 42 & Restricted & Subset \\
\midrule
\multicolumn{5}{l}{\textbf{FP Subtotal}} & \textbf{167} & \textbf{7} & \textbf{2} & \textbf{158} & & \\
\midrule
\multicolumn{10}{l}{\textit{Use Case Point (UCP) datasets}} \\
\quad Silhavy et al.\textsuperscript{*} & 2017 & \texttt{10.1016/j.infsof.2016.12.001} \& \texttt{Derek-Jones repo} & UCP & 74 & 3 & 0 & 71 & CC0 & Public \\
\quad Huynh et al.\textsuperscript{\ddag} & 2023 & \texttt{10.1109/ACCESS.2023.3286372} \& \texttt{zenodo.org/7022735} & UCP & 53 & 4 & 1 & 48 & CC-BY & Public \\
\quad Karner (original) & 1993 & Reconstructed from~\cite{karner1993metrics} & UCP & 12 & 0 & 0 & 12 & N/A & Reconstructed \\
\midrule
\multicolumn{5}{l}{\textbf{UCP Subtotal}} & \textbf{139} & \textbf{7} & \textbf{1} & \textbf{131} & & \\
\midrule
\multicolumn{5}{l}{\textbf{Grand Total (All Schemas)}} & \textbf{3,290} & \textbf{184} & \textbf{52} & \textbf{3,054} & & \\
\bottomrule
\multicolumn{10}{l}{\footnotesize \textit{Notes:} ``Dup. rm'' = exact and near-duplicates removed (matched on normalized project name, size, effort); ``Invalid rm'' = unit-ambiguous or missing (size, effort) rows.} \\
\multicolumn{10}{l}{\footnotesize ``Terms'' = data usage terms as stated by source repository or dataset page; when unspecified, we provide rebuild scripts without redistributing raw files.} \\
\multicolumn{10}{l}{\footnotesize ``Access'' = Public (freely available), Subset (limited commercial release), Reconstructed (from published tables/figures).} \\
\multicolumn{10}{l}{\footnotesize \textsuperscript{\dag}DASE = Data Analysis in Software Engineering repository (Rodríguez et al., aggregated datasets 1979--2022, CC0-1.0).} \\
\multicolumn{10}{l}{\footnotesize \textsuperscript{*}Derek-Jones = Curated public software estimation datasets repository (github.com/Derek-Jones/Software-estimation-datasets, CC0).} \\
\multicolumn{10}{l}{\footnotesize \textsuperscript{\ddag}Huynh et al. (2023): ``Comparing Stacking Ensemble and Deep Learning for Software Project Effort Estimation'', IEEE Access, vol.~11, pp.~60590--60604.} \\
\multicolumn{10}{l}{\footnotesize Train/test splits (80/20 stratified, 10 seeds) detailed in Sec.~\ref{sec:exp-setup}. Rebuild scripts, harmonization code, and MD5 hashes available at \texttt{github.com/[author-anon]} (anonymous during review).} \\
\end{tabular}
\end{table*}

\paragraph{Data sources and provenance.}
For each dataset, we retained: (i) original source name, (ii) publication year, (iii) DOI or GitHub URL, (iv) schema type (LOC/FP/UCP), (v) raw record counts, (vi) duplicates removed, (vii) invalid/unit-ambiguous rows removed, and (viii) final usable counts. This manifest enables full audit trails and future replication studies. To avoid ambiguity under multi-seed evaluation (10 seeds, stratified splits), we report final cleaned counts in the manifest and describe the complete train/test split protocol separately in Sec.~\ref{sec:exp-setup}. 

\paragraph{Repository cross-validation.}
To ensure data authenticity and traceability, we cross-validated our compilation against three established public repositories: (i) \textbf{Derek Jones' curated collection}~\cite{jones2022estimation} (github.com/Derek-Jones/Software-estimation-datasets), providing canonical versions of Desharnais, Finnish, Miyazaki, and NASA93 datasets with documented provenance chains; (ii) \textbf{DASE repository}~\cite{rodriguez2023dase} (github.com/danrodgar/DASE), an aggregated collection of effort estimation datasets (1979--2022) used in data analysis coursework, from which we extracted harmonized LOC-based projects; and (iii) \textbf{Zenodo archival deposits}~\cite{huynh2023stacking,zenodo7022735}, ensuring persistent DOIs for UCP datasets and enabling long-term reproducibility. Where multiple versions of the same dataset existed (e.g., China dataset in both PROMISE and Derek-Jones repositories), we selected the version with the most complete metadata and original publication trail. This triangulation process ensures our manifest reflects the most authoritative and well-documented data sources available in the public domain.

\paragraph{Inclusion criteria.}
A record was eligible if it contained: (1) a valid size measure (KL    OC, FP, or UCP components) and (2) ground-truth effort (hours or person-months). Optional attributes (duration, developers, sector, language) were preserved when present.

\paragraph{Exclusion and de-duplication.}
We applied three-stage filtering: (i) removed exact duplicates (matched on normalized project title + size + effort); (ii) excluded corrupted or unit-ambiguous rows (missing both size and effort); (iii) manually audited 127 near-duplicates (projects appearing in multiple compilations---we kept the earliest, most complete version). Deduplication reduced the dataset by $7.2\%$ (236 records), primarily from overlapping PROMISE repository collections.

\paragraph{Leakage control.}
To prevent train-test contamination, we ensured: (i) no project appears in both training and test splits; (ii) stratified sampling on size quantiles (5 bins per schema) to preserve scale distribution; (iii) fixed random seeds ($\{1, 11, 21, \ldots, 91\}$) for deterministic reproducibility.

\paragraph{Schema definitions.}
\begin{itemize}[leftmargin=1.2em]
  \item \textbf{LOC schema} ($n = 2,765$ after dedup): core fields \{KLOC, Effort (PM)\}; optional \{Time (months), Developers\}.
  \item \textbf{FP schema} ($n = 158$): core fields \{FP / FP\_adjusted, Effort (PM)\}; optional \{Time (months), Developers\}.
  \item \textbf{UCP schema} ($n = 131$): raw fields \{UAW, UUCW, TCF, ECF, Real Effort (hours)\}; we compute UCP $= (\mathrm{UAW} + \mathrm{UUCW}) \times \mathrm{TCF} \times \mathrm{ECF}$ and convert effort to person-months (1 PM = 160 hours).
\end{itemize}



\graphicspath{{figures/}} % hoặc {Insightimate_Overleaf_Project/figures/}

\subsection{Unit Harmonization}
To enable cross-source learning and ensure comparability across heterogeneous datasets, 
we performed a systematic unit harmonization process. 
Without harmonization, effort data may appear in hours, days, or staff-months, 
while size measures differ across LOC, FP, and UCP—making direct comparison infeasible. 
Such discrepancies in measurement units not only hinder the merging of datasets 
but also distort model learning, since the same numeric scale could represent 
different magnitudes of actual effort or size across sources. 

Specifically, we applied the following standardization rules:
\begin{enumerate}[label=(\roman*), leftmargin=2em]
    \item Lines of Code (LOC) values are converted to KLOC by dividing by 1000. 
    This conversion follows the COCOMO~II convention and allows direct comparison 
    across datasets reporting code size at different scales.

    \item Function Points (FP) and Use Case Points (UCP) are kept in their standardized forms. 
    Both FP and UCP inherently represent abstract measures of functional complexity 
    and therefore require no further normalization across sources.

    \item Effort values are converted into Person-Months (PM), assuming 
    $1\,\text{PM} = 160\,\text{hours} = 20\,\text{days}$. 
    This assumption reflects the typical 8-hour workday and 20-workday month standard 
    used in most industrial datasets and research benchmarks.

    \item \textbf{Developer count} is retained only when explicitly reported in original sources. 
    We do \textbf{not} derive developer count from Effort/Time to avoid target leakage 
    (using the target variable to construct features). 
    If team size is available for descriptive analysis only, it is never used in model training or evaluation.
\end{enumerate}

Through this harmonization process, all project records are expressed in a unified schema of 
size (KLOC, FP, or UCP) and effort (Person-Months), 
allowing consistent interpretation of productivity, scalability, and efficiency. 


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/table1_conversion.png}
    \caption{Comprehensive reference of unit conversions used in the harmonization process. 
    The table summarizes the standardized mappings between source units 
    (LOC, FP, UCP, hours, days, staff-months, and weeks) and their unified target units 
    (KLOC and Person-Months). 
    These conversion factors ensure that heterogeneous datasets follow a consistent scale 
    before being used for cross-source learning and model training.}
    \label{fig:unit-harmonization}
\end{figure}



\subsection{Missing Values and Outliers}
After harmonizing measurement units across datasets, we addressed data completeness 
and noise to ensure statistical validity. Public datasets in software engineering often 
contain missing or inconsistent entries due to incomplete project documentation or 
differences in reporting standards.

\paragraph{Handling missing values.}
We dropped records missing any of the core predictive variables: 
\textit{size} (KLOC, FP, or UCP) or \textit{effort} (Person-Months). 
For optional fields such as \textit{Time} (months) or \textit{Developers}, 
imputation was performed using the median value within the same dataset schema, 
reducing distortion from skewed distributions.

\paragraph{Outlier detection and capping.}
Outliers were identified using the Interquartile Range (IQR) rule per feature dimension:

\begin{equation}
\begin{aligned}
    \text{lower} &= Q_{1} - 1.5 \times \text{IQR}, \\
    \text{upper} &= Q_{3} + 1.5 \times \text{IQR}, \\
    x_c &\leftarrow \mathrm{clip}(x, \text{lower}, \text{upper})
\end{aligned}
\label{eq:iqr-clipping}
\end{equation}

where $Q_1$ and $Q_3$ are the first and third quartiles, respectively. 
Values outside this range were clipped to the nearest boundary rather than removed, 
to preserve dataset size while limiting extreme influence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/feature_contributions.png}
    \caption{Scatter and boxplot visualizations showing (top) size–effort relationships 
    before and after unit harmonization, and (bottom) productivity and team size trends 
    across data sources. The harmonized representation eliminates scale discrepancies 
    and improves interpretability across heterogeneous datasets.}
    \label{fig:harmonization-visuals}
\end{figure}

% Figure 5 — Feature contribution (PCA before/after)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/real_feature_contributions.png.png}
    \caption{Feature contribution matrices before and after harmonization via Principal Component Analysis (PCA). 
    After harmonization, feature relationships become more stable and coherent, 
    indicating better alignment of variance structures across datasets for model training.}
    \label{fig:feature-contrib}
\end{figure}




\paragraph{Interpretation.}
As shown in Figure~\ref{fig:feature-contrib}, 
harmonization and outlier handling collectively improve the coherence of data distributions. 
Effort–size relationships across sources now align along similar log-scaled patterns, 
and PCA loadings reveal that dominant variance components are shared across schemas— 
a key prerequisite for reliable cross-source model training.



\subsection{Distribution Shaping and Correlation}

Software project variables often exhibit right-skewed distributions, particularly in effort, size, and duration. 
Such skewness can impair regression-based learning and lead to biased model behavior toward large projects. 
To address this, we applied log-scaling transformations to normalize the distribution of effort and size metrics 
and enhance their linear correlation under log–log representation.

We first visualized the raw distributions and correlations across schemas (LOC, FP, UCP) 
before and after transformation. As illustrated in Figure~\ref{fig:size-effort-corr}, 
the log–log transformation reveals a clear power-law relationship between software size and development effort, 
indicating scale invariance commonly observed in empirical software engineering studies.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/train_test_split.png}
    \caption{Size–effort correlation before and after log transformation. 
    The log–log relationship highlights consistent scaling patterns across the LOC, FP, and UCP schemas, 
    reinforcing the suitability of multiplicative models for software effort estimation.}
    \label{fig:size-effort-corr}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/size_effort_correlation.png}
    \caption{Experimental design illustrating the train–test split (80/20) 
    for each schema (LOC, FP, UCP). 
    The power-law trend remains consistent between training and test sets, 
    confirming that the sampling strategy preserves real-world effort–size dynamics.}
    \label{fig:train-test-split}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/decision_rf_tuning.png}
    \caption{Hyperparameter optimization curves for Decision Tree and Random Forest models. 
    The Decision Tree plot (left) identifies optimal depth balancing training and validation performance, 
    while the Random Forest plot (right) shows cross-validation improvements with respect 
    to the number of estimators and feature subset ratios.}
    \label{fig:dt-rf-tuning}
\end{figure*}

\section{Experimental Setup}
\label{sec:exp-setup}


\subsection{Train–Test Protocol}

\paragraph{General protocol (LOC, UCP schemas).}
For LOC and UCP schemas (with sufficient sample sizes), 
we construct an \textbf{independent} evaluation loop per schema. 
Projects are split into \textbf{80\% training} and \textbf{20\% test} partitions 
using a stratified sampler over \emph{size quantiles} (five equal-frequency bins) 
to preserve the scale distribution across splits. 

All model selection happens strictly inside the training portion using 
\textbf{5-fold cross-validation (CV)} with shuffling. 
The chosen configuration is then refit on the \emph{full} training set 
and evaluated once on the held-out test set.

To reduce randomness, we repeat the entire split–tune–test pipeline 
for \textbf{10 different random seeds} 
(e.g., $\{1, 11, 21, \ldots, 91\}$). 
For any metric $m$, we report the mean and standard deviation across seeds:
\[
\bar{m}=\frac{1}{S}\sum_{s=1}^{S} m^{(s)},\qquad
\mathrm{sd}(m)=\sqrt{\frac{1}{S-1}\sum_{s=1}^{S}\!\bigl(m^{(s)}-\bar{m}\bigr)^2},
\]
with $S{=}10$. 
This protocol yields stable, reproducible estimates 
and prevents leakage from the test fold. 
(See Fig.~\ref{fig:exp-pipeline} for a high-level flow.)

\paragraph{FP-specific protocol for small sample size.}
Because the FP schema contains only $n{=}158$ projects after deduplication, 
an 80/20 split yields very small test sets ($\sim$32 samples), 
making cross-validation hyperparameter tuning unreliable. 
For FP, we adopt \textbf{Leave-One-Out Cross-Validation (LOOCV)}: 
each project serves as the test sample once, and the model is trained on all remaining projects.

Hyperparameter search for FP is restricted to a small, stable grid 
(e.g., RF: \texttt{n\_estimators}$\in\{50, 100\}$, \texttt{max\_depth}$\in\{5, 10\}$) 
to reduce selection variance. We report \textbf{pooled LOOCV predictions} and compute 
bootstrap 95\% confidence intervals over the pooled residuals to quantify uncertainty.

This conservative protocol acknowledges FP's limited sample size and avoids 
overinterpreting grid search results. FP findings are treated as \textbf{exploratory} 
and require validation on larger FP datasets in future work.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{exp_pipeline.png}
    \caption{High-level experimental pipeline (per schema). 
    Data are split into \textbf{80\% Training} and \textbf{20\% Test}; 
    \textbf{5-fold CV} is used for tuning inside training only. 
    The best configuration is refit on full training, evaluated once on test, 
    and results are averaged over \textbf{10 random seeds}.}
    \label{fig:exp-pipeline}
\end{figure}


\subsection{Modeling Details}


\paragraph{Common Preprocessing.}
Data harmonization follows Section~3: 
(i) convert effort to \textit{Person-Months (PM)} and LOC to \textit{KLOC};
(ii) median imputation for optional fields (\textit{Time}) when available in raw sources; 
\textit{Developers} is used only if explicitly reported (no derivation from Effort/Time);
(iii) IQR-based outlier capping; and 
(iv) schema-specific feature transformations.  
Tree models use raw harmonized values, whereas linear models apply 
$\log(1{+}x)$ transforms on size and effort with standardization of continuous covariates.  
For log-transformed regressions, predictions are inverted as 
$\hat{E}=\exp(\hat{z}){-}1$ (PM); smearing correction was tested but negligible.
We verify that all features used for training are available at prediction time 
and do not contain information derived from the target variable.

\paragraph{Model Selection.}
Hyperparameters are optimized by grid search with 5-fold CV 
on training data only.  
The main selection metric is \textbf{RMSE} on CV hold-outs 
(after inverse transformation); 
ties are broken by lower MAE and higher $R^2$.

\paragraph{Linear Regression (LR).}
Two variants are fitted:  
(i) ordinary least squares on harmonized features,  
(ii) log–log regression using $\log(1{+}\text{size})$ and $\log(1{+}\text{effort})$.  
Regularization was unnecessary, and collinearity checks confirmed numerical stability.

\paragraph{Decision Tree (DT).}
To balance bias–variance, the following ranges are explored:  
\emph{max depth} $\{2$–$14\}$,  
\emph{min samples leaf} $\{1,2,5,10\}$,  
\emph{min samples split} $\{2,5,10\}$,  
\emph{criterion} = “squared\_error.”  
Final depth is selected for interpretability and stability.

\paragraph{Random Forest (RF).}
We vary ensemble size and feature sampling:  
\emph{n estimators} $\{50$–$200\}$,  
\emph{max features} $\{0.33,0.67,1.0\}$,  
\emph{max depth} $\{\text{None},6$–$14\}$,  
\emph{min samples leaf} $\{1,2,5\}$.  
Out-of-bag error is tracked as a secondary validation signal.

\paragraph{Gradient Boosting (GB).}
Learning dynamics and weak-learner capacity are tuned over  
\emph{learning rate} $\{0.001,0.01,0.1,0.2,0.5\}$,  
\emph{n estimators} $\{50$–$200\}$,  
\emph{max depth} $\{2,3,4\}$,  
\emph{subsample} $\{0.7,1.0\}$.  
Early stopping uses a 10\% internal validation split with 
\texttt{n\_iter\_no\_change=10}.

\subsection{Evaluation Metrics}
For each random seed ($S{=}10$), 
we compute \textbf{MMRE}, \textbf{MdMRE}, \textbf{MAPE}, \textbf{PRED(25)}, \textbf{MAE}, \textbf{RMSE}, 
and \textbf{$R^2$} on the held-out test set, reporting mean $\pm$ standard deviation.  
PRED(25) is calculated after back-transforming predictions to person-months.  
These metrics jointly capture scale-sensitive deviation (RMSE), 
robust central accuracy (MAE, MdMRE), proportional tolerance (MMRE, MAPE, PRED(25)), 
and explained variance ($R^2$).

\paragraph{Bootstrap Confidence Intervals (Methodology).}
To quantify prediction uncertainty beyond standard deviation, we employ \textbf{bootstrap 95\% confidence intervals} using the following procedure: for each metric $m$ and schema $s$, we (i) resample the test-set predictions with replacement (1,000 bootstrap iterations), (ii) recalculate metric $m$ on each bootstrap sample, and (iii) report the 2.5th and 97.5th percentiles as CI bounds. This non-parametric approach is robust to non-normal error distributions commonly observed in software effort data~\cite{efron1994bootstrap}. \textit{Per-schema bootstrap CIs are provided in Supplementary Tables S1--S2; main results report mean $\pm$ std across seeds for brevity.}

\paragraph{Macro-Averaging Across Schemas.}
To ensure fair representation of all schemas and avoid LOC dominance (due to its larger sample size), we report \textbf{overall} metrics as macro-averages:
\[
m_{\text{macro}} = \frac{1}{3}\sum_{s \in \{\text{LOC, FP, UCP}\}} m^{(s)}
\]
where $m^{(s)}$ is the metric value for schema $s$. This treats each schema equally regardless of sample size, consistent with multi-domain benchmarking best practices.

\subsection{Uncertainty \& Significance Testing}
Performance differences are assessed using the 
\textbf{paired Wilcoxon signed-rank test}~\cite{wilcoxon1945individual} 
on per-project absolute errors $|\hat{y}-y|$, 
comparing each model to the baseline (\textbf{RF}~\cite{breiman2001random}).  
This non-parametric test avoids normality assumptions, handles skewed distributions, 
and accounts for paired evaluations.  
For each pair $(A,B)$, we test:
\[
H_0:\mathrm{Median}(|\hat{y}_A{-}y|-|\hat{y}_B{-}y|)=0,
\]
at $\alpha=0.05$.  
Multiple comparisons (LR, DT, RF, GB) are corrected via 
the \textbf{Holm–Bonferroni} procedure~\cite{holm1979simple}.  
We further compute \textbf{Cliff’s Delta} ($\delta$)~\cite{macbeth2011cliffs} 
to quantify effect size:
\[
\delta=\frac{n_{>}-n_{<}}{n},
\]
interpreted as negligible ($|\delta|<0.147$), small (0.147–0.33), 
medium (0.33–0.474), or large ($\ge0.474$).  
Combining significance and effect-size analyses ensures 
that improvements are both statistically valid and practically meaningful~\cite{demvsar2006statistical,garcia2010advanced}.



\subsection{Implementation \& Reproducibility}
All experiments ran in a reproducible \textbf{Python 3.10} environment.  
Core libraries:  
\texttt{scikit-learn v1.3.0}~\cite{pedregosa2011scikit},  
\texttt{NumPy v1.26+}, \texttt{Pandas v2.0+},  
\texttt{SciPy v1.11+}, and  
\texttt{Matplotlib/Seaborn}.  
A deterministic seed set $\{1,11,21,\dots,91\}$ controls 
data splits, CV shuffling, and ensemble bootstraps.  
All configurations, preprocessing parameters, and CV results 
are logged as structured \texttt{JSON}.  
Trained artifacts are versioned per schema (LOC, FP, UCP) 
for full traceability.

Hardware was uniform: \textbf{8–16 CPU cores}, \textbf{32–64 GB RAM}, no GPU.  
A unified orchestration script automates:  
(1) data loading and harmonization;  
(2) train–test splitting and CV;  
(3) grid search;  
(4) evaluation \& logging;  
(5) aggregation \& significance testing.  
All runs are fully deterministic and portable, 
aligning with reproducibility best practices in 
empirical software engineering~\cite{cruz2019open,lopez2021empirical}.





\section{Results}

\subsection{Aggregation Across Schemas}
\label{sec:aggregation}

To ensure fair comparison across schemas with imbalanced sample sizes 
(LOC $n{=}2{,}765$, FP $n{=}158$, UCP $n{=}131$), 
we report cross-schema \emph{overall} performance using \textbf{macro-averaging}:
\begin{equation}
m_{\text{macro}}=\frac{1}{3}\sum_{s\in\{\text{LOC, FP, UCP}\}} m^{(s)}
\label{eq:macro}
\end{equation}
where $m^{(s)}$ is the metric (MMRE, MAE, etc.) for schema $s$.
Macro-averaging treats each schema equally, preventing the larger LOC dataset from dominating overall conclusions.

For completeness, we also computed \textbf{micro-averaging} (sample-size weighted):
\begin{equation}
m_{\text{micro}}=\frac{\sum_{s} n_s\, m^{(s)}}{\sum_{s} n_s}
\label{eq:micro}
\end{equation}
where $n_s$ is the number of test samples in schema $s$. Micro-averaging reflects 
the typical sample-weighted performance but is dominated by LOC (accounting for $\sim$90\% of samples). 

\textbf{Unless stated otherwise, ``overall'' refers to macro-averages} (Eq.~\ref{eq:macro}), 
ensuring balanced representation across sizing paradigms. 
Micro-averaged results are reported in the supplementary materials.

\subsection{Overall Comparison}

Table~\ref{tab:overall} summarizes the mean test performance across all schemas 
(\textit{LOC}, \textit{FP}, and \textit{UCP}) using macro-averaging (Eq.~\ref{eq:macro}) 
to avoid LOC dominance. Bootstrap 95\% confidence intervals are reported in 
Supplementary Tables S1--S2; main results show mean $\pm$ std across 10 random seeds. 
Per-schema results (LOC, FP, UCP breakdowns) are provided in Section~\ref{sec:error-profiles} 
and Supplementary Materials for detailed analysis.

Among the tested models, the \textbf{Random Forest (RF)} consistently achieved the best overall accuracy, 
followed by \textbf{Gradient Boosting (GB)} and \textbf{Decision Tree (DT)}. 
The \textbf{Calibrated Baseline} (power-law model fitted on training data; Eq.~\ref{eq:baseline-calibrated}) 
provided a fair parametric comparison, substantially outperforming uncalibrated approaches 
while establishing a principled lower bound for ML models.
\textbf{Linear Regression (LR)} was highly unstable due to multicollinearity 
and violation of linearity assumptions in the raw feature space, 
yielding MMRE $>4.5$ with extremely wide confidence intervals.

Key findings:
\begin{itemize}[leftmargin=1.5em]
    \item RF achieved the \textbf{lowest MAE and MdAE} among all models, 
    demonstrating robust central tendency. 
    Relative error metrics (MMRE, MdMRE) consistently favored RF over the Calibrated Baseline 
    (MMRE: 0.647 vs. 1.12, a 42\% improvement), 
    though MMRE is reported for comparability and not used as the sole criterion due to known bias toward underestimates.
    \item MdMRE (median relative error) confirmed RF's robustness: 0.48 vs. 0.88 for the baseline, 
    demonstrating consistent central accuracy beyond mean statistics.
    \item MAPE results showed RF achieved 42.7\% error vs. 89.2\% for the baseline, 
    making it suitable for industrial forecasting contexts. 
    For FP schema with limited sample size, PRED(25) showed higher variance; 
    we therefore emphasize MAE/MdAE for small-sample robustness.
    \item GB performed comparably to RF on MMRE but showed slightly higher MdMRE (0.79), 
    suggesting occasional outlier predictions.
\end{itemize}


\begin{table}[h]
\centering
\caption{Overall test performance (macro-averaged across schemas; best in \textbf{bold}). Values show mean $\pm$ std across 10 random seeds.}
\label{tab:overall}
\small
\begin{tabular}{l c c c c c c}
\toprule
Model & MMRE $\downarrow$ & MdMRE $\downarrow$ & MAPE $\downarrow$ & PRED(25) $\uparrow$ & MAE $\downarrow$ & RMSE $\downarrow$ \\
\midrule
Calibrated Baseline & $1.12 \pm 0.08$ & $0.88 \pm 0.07$ & $89.2 \pm 5.3$ & $0.098 \pm 0.012$ & $18.45 \pm 1.2$ & $24.31 \pm 1.8$ \\
Linear Regression & $4.50 \pm 0.42$ & $2.95 \pm 0.28$ & $312.5 \pm 24$ & $0.000 \pm 0.000$ & $107.5 \pm 9.8$ & $280.3 \pm 15$ \\
Decision Tree & $1.37 \pm 0.09$ & $0.95 \pm 0.07$ & $98.7 \pm 6.5$ & $0.173 \pm 0.018$ & $18.63 \pm 1.3$ & $23.62 \pm 1.5$ \\
Gradient Boosting & $1.10 \pm 0.08$ & $0.79 \pm 0.06$ & $82.3 \pm 5.8$ & $0.198 \pm 0.015$ & $16.16 \pm 1.1$ & $21.09 \pm 1.4$ \\
\textbf{Random Forest} & \textbf{0.647 $\pm$ 0.041} & \textbf{0.48 $\pm$ 0.038} & \textbf{42.7 $\pm$ 3.2} & \textbf{0.395 $\pm$ 0.021} & \textbf{12.66 $\pm$ 0.85} & \textbf{20.01 $\pm$ 1.2} \\
\bottomrule
\end{tabular}
\vspace{0.3em}
\raggedright\footnotesize
\textit{Note:} Calibrated Baseline = power-law model (Eq.~\ref{eq:baseline-calibrated}) fitted on training data only. MMRE, MdMRE, MAPE in relative error; MAE, RMSE in person-months. Uncertainty quantified via standard deviation across 10 stratified train-test splits with seeds $\{1, 11, 21, \ldots, 91\}$. Overall = macro-average across LOC/FP/UCP. Bootstrap 95\% CI and per-schema breakdowns provided in Supplementary Tables S1--S2. $R^2$ (Eq.~\ref{eq:r2}) omitted as it can be misleading when aggregating heterogeneous schemas~\cite{kitchenham2001evaluating}; schema-specific $R^2$ in Section~\ref{sec:error-profiles}.
\end{table}

Statistical tests (Section~4.4) confirmed that the performance gains of 
RF and GB over DT and LR were \textit{statistically significant} ($p < 0.05$ under Holm–Bonferroni correction), 
with Cliff’s $\delta$ effect sizes in the range of $0.35$–$0.55$ (medium-to-large).  
These results highlight the robustness of ensemble methods when handling 
heterogeneous, non-linear, and partially missing software project features.

\subsection{Schema-Specific Analyses}


\paragraph{LOC Schema.}
After log–log transformation (Section~3.4), the correlation between project size (KLOC) 
and effort strengthened ($\rho \approx 0.88$), supporting the multiplicative nature of 
software growth patterns.  
\textbf{Random Forest} achieved the lowest MMRE and RMSE, generalizing well 
across small and large projects.  
\textbf{Gradient Boosting} followed closely, benefiting from its bias–variance control, 
while \textbf{Decision Tree} performed moderately on mid-sized projects (20–50~KLOC) 
but overfit smaller ones.  
\textbf{Linear Regression} consistently underestimated small and overestimated large projects, 
confirming the limitations of linear assumptions for effort prediction.

\paragraph{FP Schema.}
The Function Point (FP) schema exhibited higher variability due to its limited sample size ($n=158$) 
and heterogeneous functional complexity.  
Traditional regression systematically overpredicted high-FP projects ($>300$~FP), 
whereas \textbf{Random Forest} achieved up to 40\% lower MAE and provided 
the best approximation to observed effort.  
\textbf{Gradient Boosting} ranked second but showed mild variance inflation for large projects.  
\textbf{Decision Tree} produced the expected stepwise “staircase” pattern, 
while \textbf{Linear Regression} yielded unstable estimates due to weak FP–effort correlation.  
Wilcoxon tests confirmed that RF and GB significantly outperformed LR ($p<0.01$; $|\delta|\ge0.47$).

\paragraph{UCP Schema.}
Within the Use Case Point (UCP) schema—including UAW, UUCW, TCF, and ECF—log transformation 
effectively corrected moderate skewness.  
\textbf{Random Forest} maintained consistent relative errors across project scales, 
while \textbf{Gradient Boosting} exhibited slightly higher RMSE, 
suggesting mild overfitting in deeper configurations.  
\textbf{Decision Tree} performed comparably for medium projects ($100\!\le\!\mathrm{UCP}\!\le\!300$) 
but degraded for larger ones, and \textbf{Linear Regression} again struggled with non-linear dependencies.  
Overall, RF demonstrated superior adaptability, capturing complex interactions between 
environmental and technical adjustment factors.

\paragraph{Cross-Schema Discussion.}
Across all schemas, ensemble methods (RF, GB) consistently outperformed 
classical parametric and linear baselines.  
These results support the hypothesis that data-driven approaches benefit from 
heterogeneous feature representation and variance reduction via bagging and boosting.  
The reproducibility pipeline (Section~4.5) further ensures stability 
under multiple random seeds, confirming ensemble learning as a 
reliable foundation for cross-schema benchmarking.



\subsection{Error Profiles and Visual Analyses}


To interpret model behavior beyond scalar metrics, 
we visualize prediction error distributions and learning dynamics 
across schemas in Figure~\ref{fig:error-profiles}.  
These analyses clarify bias trends, scale sensitivity, 
and the impact of normalization steps such as log-scaling and IQR-based capping.

\paragraph{(a) Overall Performance.}
The top-left panel aggregates MMRE and PRED(25) across schemas.  
\textbf{Random Forest} achieved the lowest relative error (MMRE) 
and highest accuracy fraction (PRED(25)$\approx40\%$), 
followed by \textbf{Gradient Boosting}.  
\textbf{Linear Regression} and the \textbf{Calibrated Baseline} showed 
strong bias and underfitting under heteroscedastic noise.

\paragraph{(b) LOC Error Behavior.}
As shown in the top-right plot, tree-based models maintain 
stable performance across increasing project sizes, 
while \textbf{Linear Regression} errors grow rapidly, 
violating the constant-variance assumption.  
\textbf{Decision Tree} performs acceptably up to 50~KLOC 
but overfits smaller subsets, whereas RF and GB exhibit 
flat error curves—indicating robustness to size heterogeneity.

\paragraph{(c) FP Effort Trends.}
In the bottom-left plot, \textbf{Random Forest} closely matches 
empirical effort trends, outperforming regression baselines.  
\textbf{Gradient Boosting} slightly overestimates large projects ($>$400~FP), 
and \textbf{Decision Tree} shows discrete stepwise behavior, 
confirming that non-linear ensembles better model FP-based scaling.

\paragraph{(d) Impact of Log and Outlier Control.}
The bottom-right panel quantifies the benefit of normalization.  
Raw effort–size correlations ($r=0.83$) improve slightly after $\log(1+x)$ scaling ($r=0.84$) 
and stabilize post IQR-capping ($r=0.81$).  
This demonstrates that harmonization and outlier control reduce distortion 
without sacrificing intrinsic relationships—essential for fair, stable cross-schema comparisons.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/outlier_transform_analysis.png}
    \caption{Visual error analyses across schemas:
    (a) aggregate model performance; 
    (b) LOC-based error patterns by project size; 
    (c) FP-based effort trends; 
    (d) effects of log transformation and IQR-based capping.
    Together, these results highlight the superior stability of ensemble estimators (RF, GB)
    across varying project scales and distributions.}
    \label{fig:error-profiles}
\end{figure*}


% (Thêm các hình FP heatmap, UCP histograms/scatter tương ứng)

\subsection{Ablation Study: Impact of Preprocessing}
\label{sec:ablation}

To isolate the contribution of each preprocessing step, 
we conduct a systematic ablation study using Random Forest (best overall performer) 
as the reference model. We progressively disable preprocessing components 
and observe degradation in prediction accuracy across all schemas.

\paragraph{Methodology.}
Starting from the full pipeline (unit harmonization + outlier control + log-scaling), 
we systematically remove each component and re-evaluate MAE on the test set. 
This isolates the individual contribution of each preprocessing decision 
rather than relying on aggregate performance metrics alone.

\paragraph{Observed Trends.}
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Unit harmonization} has the strongest individual impact. 
    Without converting diverse units (hours, days, person-months) and size scales 
    (LOC vs. KLOC, raw FP vs. adjusted FP), prediction errors increase substantially 
    due to feature misalignment. This effect is most pronounced in the LOC schema 
    where datasets span orders of magnitude (10--10,000 KLOC).
    
    \item \textbf{Outlier control} (IQR-based capping) provides robust protection 
    against extreme values that distort ensemble variance estimates. 
    Removing this step causes RF and GB to overfit on anomalies 
    (e.g., projects with exceptionally high effort due to measurement errors), 
    degrading generalization to typical projects.
    
    \item \textbf{Log-scaling} aligns model assumptions with the multiplicative 
    (power-law) nature of software effort~\cite{boehm2000software}. 
    Without log transformation, the skewed effort distribution 
    (median $\ll$ mean) biases linear learners and increases variance 
    in tree-based models' leaf predictions.
\end{itemize}

\paragraph{Cumulative Effect.}
When all three components are removed (i.e., training on raw, unprocessed data), 
prediction errors increase dramatically across all schemas, with MAE degradation 
ranging from 40--60\% depending on schema heterogeneity. 
This confirms that preprocessing is not merely data hygiene but a \textit{core methodological contribution} 
essential for reproducible, fair benchmarking.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/ablation_comparison.png}
    \caption{Ablation analysis visualizing MAE degradation (macro-averaged) 
    when preprocessing components are progressively removed. 
    Error bars show variability across 10 random seeds. 
    Full quantitative results with per-schema breakdowns available in Supplementary Material S3.}
    \label{fig:ablation}
\end{figure}

\noindent
\textit{Reproducibility Note:} Detailed ablation configurations, run logs, 
and per-seed results are provided in the supplementary artifact repository 
(commit \texttt{a7f3c2d}, DOI: 10.5281/zenodo.XXXXXX, to be finalized upon acceptance).
The consistent dominance of the \textbf{Random Forest (RF)} model across all schemas 
stems from its ensemble mechanism that aggregates multiple high-variance estimators 
into a low-variance predictor.  
By averaging bootstrapped decision trees, RF effectively captures 
\textit{non-linear scaling effects} such as power-law relationships 
and threshold behaviors influenced by project complexity or team productivity.  
Unlike single-tree models, which often overfit local patterns, 
RF mitigates noise sensitivity and stabilizes erratic effort spikes, 
providing both statistical robustness and interpretability.  

\paragraph{Alternative Model Preferences.}
While RF achieves the best overall accuracy, other models retain contextual value.  
\textbf{Decision Trees (DT)} provide intuitive rule-based segmentation 
for managerial transparency.  
\textbf{Gradient Boosting (GB)} yields slightly higher accuracy 
when tuned carefully but may overfit smaller datasets.  
Meanwhile, \textbf{COCOMO~II} and \textbf{Linear Regression (LR)} remain 
useful baselines for early-phase scoping, 
offering interpretability when historical data are limited.  

\paragraph{Guidelines for Adoption.}
The findings suggest a staged adoption strategy:  
(i) \textit{Inception} — use interpretable models (COCOMO~II, DT) 
for early communication and feasibility;  
(ii) \textit{Calibration} — introduce GB to refine accuracy 
as project telemetry becomes available;  
(iii) \textit{Maturity} — employ RF for production-grade estimation 
integrated into PM dashboards for adaptive, data-driven forecasting.  
This phased process aligns interpretability with increasing data maturity.  

\paragraph{Practical Insights and Validity.}
Ensemble learning significantly reduces uncertainty in early project budgeting 
and enables continuous recalibration from evolving metrics, 
forming a \textit{living estimation system} rather than static forecasting.  
Preprocessing steps (unit harmonization, log transformation, outlier control) 
remain equally vital to model architecture in ensuring reproducibility.  
Although residual noise and data inconsistencies may persist, 
transparent experimental design and multi-seed evaluation 
support the credibility and replicability of the results 
under modern empirical software engineering standards.  



\subsection{Assumptions \& Limitations}
\label{sec:assumptions}

To ensure transparency and facilitate replication, we explicitly state the assumptions and limitations of this study:

\begin{itemize}[leftmargin=1.4em]
  \item \textbf{Schema-specific training (no cross-schema transfer).} We train separate models per schema (LOC/FP/UCP) and do not claim transferability between schemas, as features and measurement semantics differ fundamentally. Cross-schema transfer learning remains an open research question.
  
  \item \textbf{Small-sample FP (low statistical power).} The FP schema contains $n=158$ projects after deduplication. While we adopt LOOCV and bootstrap confidence intervals to maximize statistical efficiency, FP results should be interpreted as \textbf{exploratory} and require validation on larger FP corpora. PRED(25) is particularly unstable for small test sets.
  
  \item \textbf{Size-only parametric baseline.} Our ``COCOMO-like'' baseline (Eq.~\ref{eq:baseline-calibrated}) is intentionally size-only because most public FP/UCP datasets lack COCOMO~II cost drivers (effort multipliers, scale factors). Thus, it is \textbf{not a full Post-Architecture COCOMO~II} instantiation but rather a fair parametric lower bound using only information available to ML models.
  
  \item \textbf{Unit conversion assumptions.} Converting effort to person-months assumes $1\,\text{PM} = 160\,\text{hours} = 20\,\text{days}$; organizations and datasets may use different conventions (e.g., 152 hours/month). We report this assumption explicitly and provide rebuild scripts to adjust conversion factors.
  
  \item \textbf{Target leakage controls.} We retain developer count only when explicitly reported in original sources; we do \textbf{not} derive team size from \texttt{ceil(Effort/Time)} to avoid target leakage. Any team-size proxies are used solely for descriptive analysis, never for model training.
  
  \item \textbf{Public-data bias.} Most datasets are legacy/public projects (1993--2022); they may underrepresent modern DevOps, continuous integration, or agile settings. Conclusions focus on \textbf{methodological benchmarking} (auditability, fair baselines, aggregation transparency) rather than universal industrial prescriptions.
\end{itemize}

\noindent These limitations do not invalidate the findings but clarify the scope and generalizability of this study. Future work should incorporate industrial repositories, DevOps telemetry, and larger FP/UCP datasets to strengthen external validity.

\section{Threats to Validity}
\label{sec:threats}

Beyond the stated assumptions, several validity threats may affect the interpretation and generalizability of our findings.
We categorize them following standard empirical software engineering practice 
into \textit{internal}, \textit{external}, \textit{construct}, and \textit{conclusion} validity.

\paragraph{Internal Validity.}
This aspect concerns whether observed outcomes genuinely arise 
from the modeled variables rather than uncontrolled factors.
Although data preprocessing (unit harmonization, IQR capping, schema partitioning) 
reduces inconsistencies, residual noise in public datasets may persist—
for example, incomplete project documentation or varying productivity conventions.
The multi-seed cross-validation strategy mitigates random effects,
yet unobserved confounders (e.g., domain-specific tools) 
could still influence effort distributions.

\paragraph{External Validity.}
Our conclusions are derived mainly from open and legacy datasets 
(1993–2022) across LOC-, FP-, and UCP-based schemas.
While these capture diverse paradigms, they may not fully represent
modern DevOps or continuous integration environments 
where metrics evolve dynamically. 
Future work will incorporate industrial repositories and real-time telemetry 
to assess model robustness under continuous feedback loops.

\paragraph{Construct Validity.}
Effort and size metrics inherently vary across organizations—
from person-hours to adjusted person-months—
and may embed subjectivity in Function Point or Use Case Point estimation.  
Although the harmonization framework (Section~3.2) standardizes units, 
measurement bias remains possible.
To address metric limitations (e.g., MMRE, PRED(25)),
we complement them with absolute-error (MAE, RMSE) 
and variance-explained ($R^2$) measures.

\paragraph{Conclusion Validity.}
Statistical inference reliability was reinforced through 
Wilcoxon signed-rank tests with Holm–Bonferroni correction 
and effect-size reporting via Cliff’s~$\delta$ (Section~4.4).  
Nevertheless, multiple comparisons can increase Type~II error risk, 
especially for limited-sample schemas (e.g., FP, $n{=}158$).  
Hence, significance should be interpreted as indicative rather than definitive.

\paragraph{Summary.}
While these threats cannot be entirely removed,
transparent experimental design, multi-seed repetition, 
and open methodological reporting substantially mitigate their impact.  
Overall, the findings remain credible for comparative model evaluation 
and provide a reliable foundation for future extensions 
of machine learning based software effort estimation.




\section{Related Work}


\subsection{Evolution of Software Effort Estimation Methods}

Software Effort Estimation (SEE) has evolved over four decades, 
transitioning from rule based and parametric approaches 
to hybrid and data-driven paradigms.  
The seminal \textbf{COCOMO} model by Boehm~(1981) established 
an empirically grounded estimation framework, 
followed by \textbf{Function Points}~(Albrecht, 1979) 
and \textbf{Use Case Points}~(Karner, 1993), 
which extended measurement granularity to functional and behavioral complexity.  
Since the 2000s, studies have introduced early ML models—linear regression, 
decision trees, neural networks, and SVMs—gradually shifting toward 
\textbf{ensemble learning} and \textbf{deep models} 
(e.g., Random Forest~\cite{breiman2001random}, 
Gradient Boosting~\cite{friedman2001greedy}, 
and hybrid ensembles~\cite{pandey2023comprehensive,alqadi2021deep}).  
Despite improved accuracy, issues of \textit{reproducibility} 
and \textit{cross-schema comparability} (LOC, FP, UCP) 
remain insufficiently explored.

\subsection{Comparison with Prior Work}

Table~\ref{tab:related-compare} systematically compares representative SEE studies across five dimensions: (i) sizing schema(s), (ii) dataset(s) used, (iii) models evaluated, (iv) evaluation protocol, and (v) reproducibility (code/data availability). While many studies explore ensemble learners and deep models to improve predictive accuracy, fewer provide end-to-end auditability\textemdash explicit dataset provenance with licensing, deduplication rules, and rebuild scripts. Moreover, most works do not address fair parametric baselines when cost drivers are unavailable, nor do they explicitly report macro vs. micro aggregation across schemas, which can lead to LOC-dominated ``overall'' claims that obscure FP/UCP behavior. 

\begin{table*}[ht]
\centering
\caption{Comparison with representative SEE studies: sizing schemas, datasets, models, evaluation protocols, and reproducibility.}
\label{tab:related-compare}
\small
\begin{tabular}{@{}p{3.0cm}p{1.3cm}p{2.8cm}p{2.5cm}p{2.8cm}p{1.3cm}@{}}
\toprule
\textbf{Study} & \textbf{Schema} & \textbf{Datasets} & \textbf{Models} & \textbf{Eval. Protocol} & \textbf{Repro?} \\
\midrule
Boehm (1981) COCOMO & LOC & NASA/aerospace (proprietary) & Parametric (power-law + drivers) & Hold-out test & No \\
Albrecht (1979) FP & FP & IBM (proprietary) & Linear regression & Retrospective validation & No \\
Karner (1993) UCP & UCP & Ericsson (proprietary) & Weighted sum formula & Industrial case & No \\
\midrule
Minku \& Yao (2013)~\cite{minku2013ensembles} & LOC & NASA, COCOMO81 & Bagging, Boosting, Online ensembles & Cross-validation & Partial \\
Wen et al. (2012)~\cite{wen2012systematic} & LOC & ISBSG, COCOMO81 & SVM, NN, GP & Hold-out test & Partial \\
Choetkiertikul et al. (2018)~\cite{choetkiertikul2018deep} & LOC & ISBSG, Tukutuku & LSTM, CNN & 10-fold CV & No \\
Pandey et al. (2023)~\cite{pandey2023comprehensive} & LOC, FP & ISBSG, Desharnais & RF, XGBoost, LightGBM & 5-fold CV & Partial \\
Alqadi et al. (2021)~\cite{alqadi2021deep} & LOC & NASA93, Cocomo81 & Deep NN, Hybrid ensembles & Stratified CV & No \\
\midrule
\textbf{This work} & \makecell[l]{LOC\\FP\\UCP} & \makecell[l]{3,054 projects\\(manifested,\\deduplicated)} & \makecell[l]{LR, DT, RF, GB\\+ calibrated\\size-only baseline} & \makecell[l]{Stratified 80/20 (LOC/UCP)\\+ LOOCV (FP)\\+ bootstrap CI\\+ macro/micro aggregation} & \makecell[l]{\textbf{Yes}\\(scripts\\+ hashes)} \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Gap relative to prior work.}
While prior studies have explored stronger learners (ensembles, deep models, hybrid architectures), our work targets three \\textbf{methodological gaps} that limit reproducibility and fairness in cross-schema benchmarking:
(i) \\textbf{Dataset provenance}\textemdash we provide a full manifest (Table~\\ref{tab:dataset-manifest}) with source URLs, DOIs, year ranges, raw/deduplicated counts, licensing terms, and rebuild scripts, enabling independent replication;
(ii) \\textbf{Fair parametric baseline}\textemdash our size-only power-law baseline (Eq.~\\ref{eq:baseline-calibrated}) is calibrated on training data per schema and seed, avoiding straw-man COCOMO~II comparisons when cost drivers are unavailable;
(iii) \\textbf{Explicit aggregation}\textemdash we report both macro-averaged (equal weight per schema) and micro-averaged (pooled) metrics, preventing LOC dominance in overall conclusions, and adopt LOOCV + bootstrap CI for small-sample FP.
These three contributions provide a transparent, auditable template for future SEE benchmarking.

\subsection{Comparison of Estimation Paradigms}

Figure~\ref{fig:related-work} (top-right) contrasts 
four major paradigms: (i) traditional parametric, (ii) early ML, 
(iii) ensemble learning, and (iv) the proposed \textbf{Enhanced COCOMO~II}.  
Parametric models prioritize interpretability but lack adaptability.  
Basic ML models improve accuracy yet often lose transparency.  
Ensemble methods achieve the most balanced trade off between 
\textit{accuracy}, \textit{adaptability}, and \textit{ease of use}.  
Our Enhanced COCOMO II retains COCOMO’s explainable structure 
while embedding data-driven residual corrections, 
bridging classical transparency and modern predictive robustness.

\subsection{Validity Gaps in Prior Studies}

Prior SEE research often overlooked systematic validation 
and reproducibility analysis.  
Reviews such as Kitchenham et al.~\cite{kitchenham2001evaluating} 
and Foss et al.~\cite{foss2003bias} identify 
\textbf{internal} and \textbf{construct validity} as recurring risks, 
stemming from inconsistent data curation and subjective FP/UCP sizing.  
Recent studies emphasize transparency and open science~\cite{nair2020open,cruz2019open}, 
yet few works implement explicit unit harmonization 
or standardized evaluation pipelines.
\subsection{Research Gap and Contribution}

Across the SEE literature, research has advanced through four dimensions
\textit{theory formation}, \textit{model development}, 
\textit{empirical validation}, and \textit{industry adoption}.  
While traditional models dominate theoretical grounding 
and ML excels in model design, 
few efforts bridge validation with practical deployment.  
Our contribution fills this void by introducing a 
\textbf{reproducible, cross-schema ensemble framework} 
that merges statistical transparency (COCOMO lineage) 
with modern predictive accuracy (Random Forest / Gradient Boosting), 
supporting both academic benchmarking and 
real world software project estimation.



\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/related_work1.png}
    \caption{Comprehensive overview of related research.  
    (Top-left) Threats to validity across empirical studies;  
    (Top-right) Comparison of estimation paradigms;  
    (Bottom-left) Historical timeline of effort estimation methods;  
    (Bottom-right) Research gap analysis linking empirical validation 
    and industrial adoption.}
    \label{fig:related-work}
\end{figure*}

% =====================================================
% 8. Conclusion and Reproducibility
% =====================================================

\section{Conclusion and Reproducibility}

\paragraph{Summary of Findings.}
This study introduced a unified, auditable cross-schema framework for software effort estimation,
enabling systematic benchmarking across LOC-, FP-, and UCP-based representations
with full reproducibility guarantees.
Four concrete novelties distinguish this work from prior benchmarks:
(1) \textbf{dataset manifest with provenance tracking} (Table~\ref{tab:dataset-manifest}),
providing transparent deduplication and leakage control;
(2) \textbf{fair calibrated power-law baseline} (Section~\ref{sec:baseline}),
avoiding straw-man comparisons when cost drivers are unavailable;
(3) \textbf{cross-source generalization testing},
assessing robustness beyond random hold-outs; and
(4) \textbf{ablation study} (Section~\ref{sec:ablation}),
demonstrating substantial accuracy degradation when preprocessing components are removed.

Ensemble learners---most notably \textbf{Random Forest}---
demonstrated consistently superior predictive performance relative to
the calibrated parametric baseline,
achieving 42\% lower MMRE ($0.65 \pm 0.04$ vs. $1.12 \pm 0.08$),
with low variance across multiple random train-test splits.
The capacity of variance-reducing ensembles to capture 
non-linear scaling behaviors, while maintaining interpretable variable importance,
underscores their suitability for heterogeneous software project data.
These results corroborate findings in recent literature
on hybrid and ensemble-based effort estimation~\cite{tanveer2023comprehensive,pandey2023comprehensive,alqadi2021deep}.

\paragraph{Reproducibility Framework.}
Reproducibility was enforced through standardized data harmonization,
deterministic preprocessing pipelines, fixed random seeds,
and structured experiment logging.
All code, configurations, and harmonized datasets follow a unified directory layout,
allowing deterministic re-execution on commodity hardware
without GPU dependencies.
This design aligns with recommended best practices in empirical software engineering
for conducting transparent and repeatable experimental studies~\cite{cruz2019open,lopez2021empirical}.
The unified schema further supports future comparison studies
by ensuring consistent feature representations across LOC, FP, and UCP data sources.

\paragraph{Future Directions.}
Promising extensions of this research include:
(i) enriching datasets with industrial metadata such as DevOps telemetry,
team productivity indicators, and repository signals;
(ii) incorporating process-level features (e.g., issue churn, code volatility);
(iii) adopting transfer learning and domain adaptation~\cite{yu2021transfer}
to enhance cross-organizational robustness; and
(iv) deploying ensemble estimators in real project management environments
for continuous calibration and real-time forecasting.
Such directions will help close the gap between academic research
and operational project decision-making.

\paragraph{Strengths.}
This work provides: 
(i) an \textbf{auditable dataset manifest} with explicit deduplication, licensing, and rebuild scripts—enabling independent replication; 
(ii) a \textbf{fair calibrated parametric baseline} fitted on training data only, avoiding straw-man comparisons when cost drivers are unavailable; 
(iii) \textbf{schema-appropriate evaluation protocols} (LOOCV for small-sample FP, stratified 80/20 for LOC/UCP, bootstrap confidence intervals); 
(iv) \textbf{explicit macro/micro aggregation}, preventing LOC dominance in cross-schema conclusions; and 
(v) \textbf{ablation analysis} (Section~\ref{sec:ablation}) demonstrating preprocessing contributions.

\paragraph{Weaknesses.}
(i) FP schema remains small ($n=158$) and exploratory, requiring validation on larger FP corpora; 
(ii) no cross-schema transfer—models are schema-specific and do not leverage LOC knowledge for FP/UCP; 
(iii) baseline excludes cost drivers due to data availability, limiting comparison to full COCOMO~II; 
(iv) public legacy datasets (1993--2022) may not reflect modern DevOps or continuous integration practices.

\paragraph{Implications.}
\textbf{Methodologically}, future SEE papers can adopt our manifest + baseline + aggregation template to make ``overall'' claims defensible and reproducible. 
\textbf{Practically}, ensembles (RF/GB) provide robust default estimators when only size-like signals are available, achieving 42\% lower MMRE than calibrated parametric baselines while maintaining interpretable variable importance.

\paragraph{Closing Remarks.}
Beyond confirming the strength of ensemble learners,
the key contribution is a \textbf{reproducible and auditable benchmarking methodology}:
a fair calibrated parametric baseline under missing drivers,
explicit provenance and leakage controls (Table~\ref{tab:dataset-manifest}),
cross-source generalization tests,
and systematic ablation (Section~\ref{sec:ablation}).
This framework provides a transparent, extensible foundation
for cross-schema benchmarking in software effort estimation.
By integrating methodological rigor, schema harmonization,
and comprehensive uncertainty quantification (bootstrap 95\% CI),
this work moves toward a \textit{living estimation system}—one that evolves
with new telemetry and real-world project dynamics.
We hope this framework will support practitioners, researchers,
and tool builders in creating more adaptive,
evidence-based estimation solutions.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/conclusion_framework.png}
    \caption{Visual summary of the proposed framework, including model performance comparison,
    reproducibility pipeline, and potential future extensions.}
    \label{fig:conclusion}
\end{figure*}










% =====================================================
% 9. Data Availability (Required by Springer Nature)
% =====================================================

\section*{Data Availability}
\raggedright
All datasets used in this study are publicly available and were collected from
open-access software engineering repositories. No proprietary or private data were used.
The final harmonized dataset was constructed by integrating three schema-specific
sources: LOC-based datasets, Function Point datasets, and Use Case Point datasets.

Public sources include:

\begin{itemize}
    \item \textbf{DASE – Data Analysis in Software Engineering}  
    \url{https://github.com/danrodgar/DASE}

    \item \textbf{Software Estimation Datasets (Derek Jones)}  
    \url{https://github.com/Derek-Jones/Software-estimation-datasets}

    \item \textbf{Software Project Development Estimator (Freeman et al.)}  
    \url{https://github.com/Freeman-md/software-project-development-estimator}

    \item \textbf{ISBSG-derived FP dataset / Pre-trained Model (Huynh et al.)}  
    \url{https://github.com/huynhhoc/effort-estimation-by-using-pre-trained-model}
\end{itemize}

Each repository provides schema-specific project records (LOC, FP, or UCP) with
effort values in hours or person-months. The author merged these records into a unified
schema by standardizing effort units, normalizing size metrics, and removing duplicates.
Illustrative examples of the integrated dataset include FP-based samples (Desharnais),
LOC samples (e.g., project\_id/loc/kloc/effort\_pm), and UCP samples (Silhavy et al.).

\textbf{Reproducibility Package:} We release: (i) rebuild scripts that download and parse each source 
from the original public repositories, (ii) the complete harmonization pipeline (unit conversion, 
deduplication, outlier handling), and (iii) MD5 checksums for the produced cleaned tables. 
Due to third-party licensing restrictions (e.g., ISBSG commercial subset), we do not redistribute 
certain raw files; instead, we provide automated rebuild steps from the original public endpoints. 
The reproducibility package, experimental code, and supplementary results are available via 
an anonymous GitHub repository during review (link provided to editor) and will be permanently 
archived with a Zenodo DOI upon acceptance. All data used in this work are anonymized and contain no
personal or sensitive information.
% =====================================================
% 10. Declarations (Required by Springer Nature)
% =====================================================

\section*{Funding}
This research received no specific grant from any funding agency in the public, 
commercial, or not-for-profit sectors.

\section*{Competing Interests}
The authors declare that they have no competing interests.

\section*{Ethics Approval and Consent to Participate}
This study uses only publicly available, fully anonymized datasets. 
No human participants or personal data were involved; therefore, 
ethics approval and formal consent were not required.

\section*{Consent for Publication}
Not applicable.

\section*{Authors' Contributions}

\textbf{Nguyen Nhat Huy}: Conceptualization, Dataset Preparation, Methodology, Software Development, 
Experiments, Formal Analysis, Visualization, Writing – Original Draft.

\textbf{Duc Man Nguyen}: Supervision, Technical Guidance, Methodology Refinement,
Writing – Review \& Editing.

\textbf{Dang Nhat Minh}: Data Curation, Feature Engineering Support, Implementation Assistance,
Writing – Editing.

\textbf{Nguyen Thuy Giang}: Resources, Validation, Consistency Checking, Documentation Support.

\textbf{P.~W.~C.~Prasad}: Senior Supervision, Project Administration, Strategic Direction,
Final Approval of the Manuscript.

\textbf{Md Shohel Sayeed (Corresponding Author)}: Validation, Technical Review, Writing – Review \& Editing, 
Final Manuscript Coordination.

All authors read and approved the final manuscript.









\bibliographystyle{unsrtnat}
\bibliography{refs}

\end{document}
