\documentclass[11pt,a4paper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{times}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

\title{\textbf{Response to Reviewers}\\[0.5em]
\large Manuscript: Insightimate: Enhancing Software Effort Estimation Accuracy\\
Using Machine Learning Across Three Schemas (LOC/FP/UCP)}
\author{Nguyen Nhat Huy et al.}
\date{February 19, 2026}

\begin{document}

\maketitle

\noindent Dear Editor and Distinguished Reviewers,

We sincerely thank the Editor and all Reviewers for their exceptionally thorough and constructive evaluation of our manuscript entitled \textit{``Insightimate: Enhancing Software Effort Estimation Accuracy Using Machine Learning Across Three Schemas (LOC/FP/UCP)''}. The reviewers' insightful feedback has been invaluable in strengthening the scientific rigor, reproducibility, and practical impact of this work. We have carefully addressed all concerns through substantial manuscript revisions, encompassing methodological enhancements, dataset expansion, additional experiments, and improved presentation quality.

\section*{Executive Summary of Major Revisions}

The revised manuscript incorporates the following major improvements addressing concerns from multiple reviewers:

\begin{enumerate}[leftmargin=*,label=\textbf{\arabic*.}]
    \item \textbf{Dataset Expansion (+192\%):} Increased from n=1,042 to \textbf{n=3,054 projects} across \textbf{18 independent sources} spanning 1979-2023. The FP schema expanded from n=24 to \textbf{n=158 projects} (+558\%), substantially addressing statistical power concerns raised by Reviewers 2, 5, 6, and 7.
    
    \item \textbf{State-of-the-Art Models:} Integrated \textbf{XGBoost} (modern gradient boosting variant with regularization) achieving MAE 13.24 PM compared to Random Forest's 12.66 PM (<5\% difference), demonstrating that contemporary SOTA models converge to similar accuracy levels on this task (Reviewers 4, 7).
    
    \item \textbf{Enhanced Evaluation Metrics:} Added \textbf{MdMRE} (Median Magnitude of Relative Error) and \textbf{MAPE} (Mean Absolute Percentage Error) providing robust central-tendency statistics and business-friendly percentage reporting formats (Reviewers 1, 2).
    
    \item \textbf{Cross-Source Validation (LOSO):} Implemented \textbf{Leave-One-Source-Out (LOSO)} validation on 11 independent LOC sources, demonstrating acceptable cross-organizational generalization with 21\% MAE degradation compared to within-source splits, confirming external validity (Reviewers 2, 7, 8).
    
    \item \textbf{Calibrated Parametric Baseline:} Replaced uncalibrated COCOMO II defaults with \textbf{training-data-fitted power-law baseline} ($E = A \times \text{Size}^B$ where coefficients optimized via least-squares on training data only) eliminating the straw-man criticism and ensuring fair comparison (Reviewers 1, 2, 7).
    
    \item \textbf{Methodological Transparency:} Comprehensively clarified (i) macro-averaging protocol for cross-schema aggregation (equal weight per schema), (ii) complete dataset provenance with DOI/URL references, (iii) explicit deduplication and leakage-prevention rules, (iv) schema-specific validation protocols (LOOCV for FP, stratified 80/20 for LOC/UCP), (v) bootstrap confidence intervals for small-sample FP schema (Reviewers 2, 3, 6).
    
    \item \textbf{Expanded Literature Review:} Cited \textbf{7 new papers} recommended by reviewers, including 3 IEEE journal articles on ensemble methods (IEEE TSMC DOI: 10.1109/TSMC.2025.3580086, IEEE TFUZZ DOI: 10.1109/TFUZZ.2025.3569741, IEEE TETCI DOI: 10.1109/TETCI.2025.3647653) and 2 recent preprints (Discover Applied Sciences DOI: 10.1007/s44248-024-00016-0, Research Square DOI: 10.21203/rs.3.rs-7556543/v1) with comparative advantage/drawback analysis positioning our reproducibility focus (Reviewers 3, 4, 5).
    
    \item \textbf{Improved Presentation Quality:} Enhanced all figures to 300 DPI resolution, added proper captions and numbering, implemented line numbering, restructured introduction with explicit gaps/contributions, added comprehensive limitations discussion, and conducted three-pass linguistic revision (Reviewers 4, 5, 6, 7).
\end{enumerate}

\vspace{0.3em}
\noindent \textbf{Document Structure:} Below, we provide detailed point-by-point responses to each Reviewer in three-column table format: (1) Reviewer Comment (verbatim quote), (2) Response (detailed explanation with evidence), (3) Where Revised (specific line numbers and sections in manuscript). All line and page references refer to the revised manuscript (25 pages, 1,286 lines LaTeX source, compiled PDF 2.1 MB).

\vspace{0.3em}
\noindent We believe these comprehensive revisions substantially strengthen the manuscript's scientific validity, reproducibility, and contribution clarity. We are grateful for the opportunity to address these concerns and are confident the revised manuscript now meets the standards for publication in your esteemed journal.

\vspace{0.5em}
\noindent Sincerely,

\noindent \textbf{Nguyen Nhat Huy} (Corresponding Author)\\
International School, Duy Tan University, Da Nang, Vietnam\\
Email: huy.nguyen@duytan.edu.vn\\
On behalf of all co-authors

\newpage

\section*{Detailed Response to Reviewer 1}

\begin{longtable}{p{0.28\linewidth}|p{0.42\linewidth}|p{0.25\linewidth}}
\toprule
\textbf{Reviewer Comment} & \textbf{Response (and text added/updated)} & \textbf{Where revised in manuscript} \\
\midrule
\endfirsthead

\multicolumn{3}{c}{\textit{(Continued from previous page)}} \\
\toprule
\textbf{Reviewer Comment} & \textbf{Response (and text added/updated)} & \textbf{Where revised in manuscript} \\
\midrule
\endhead

\midrule
\multicolumn{3}{r}{\textit{(Continued on next page)}} \\
\endfoot

\bottomrule
\endlastfoot

``Provide a clearer positioning of what is novel beyond `a unified evaluation pipeline'.'' &

\textbf{Thank you for this critical concern—we have substantially repositioned the contribution.}

\textit{Acknowledgment:} We fully agree that procedural pipeline engineering alone represents insufficient novelty for journal publication. A ``unified framework'' without methodological innovation risks being merely descriptive work rather than advancing scientific knowledge.

\textit{Action Taken—Three Methodological Innovations:}

We repositioned the core contribution from ``harmonized pipeline'' to \textbf{three methodological innovations addressing reproducibility gaps} documented in prior SEE meta-analyses~\cite{jorgensen2007systematic,kitchenham2007guidelines}:

\textbf{(1) Macro-averaged cross-schema evaluation protocol} (Section 4.3, lines 229-236):

We formalize metric aggregation across LOC/FP/UCP schemas using equal weighting:
$$m_{\text{macro}} = \frac{1}{3}\sum_{s \in \{\text{LOC}, \text{FP}, \text{UCP}\}} m^{(s)}$$

This prevents LOC corpus dominance (n=2,765, 90.5\% of projects) from masking FP (n=158, 5.2\%) and UCP (n=131, 4.3\%) performance. Prior studies either pool data---semantically invalid due to KLOC$\neq$FP$\neq$UCP incomparability---or report micro-averaged metrics without disclosure, leading to irreproducible aggregate claims. Our protocol ensures ``overall'' conclusions reflect balanced multi-schema robustness.

\textbf{(2) Calibrated parametric baseline methodology} (Section 2.1.1, lines 133-143):

We replace uncalibrated COCOMO II defaults with a \textit{training-data-fitted size-only power-law baseline} $E = A \times \text{Size}^B$. Coefficients $(A, B)$ are optimized via least-squares regression (\texttt{scipy.optimize.curve\_fit}) strictly on training folds, ensuring the parametric baseline benefits from identical data access as ML models. This addresses fairness criticism pervasive in ML-vs-parametric literature.

Results: Even with calibration, parametric baseline underperforms (MMRE 2.790 vs RF 0.647, MAE 35.2 vs 12.66 PM), confirming that \textbf{fixed functional forms struggle with heterogeneous project characteristics beyond size alone}.

\textbf{(3) Auditable dataset manifest with explicit deduplication} (Table 1, lines 248-275):

We provide complete provenance (18 sources, DOI/URL, raw vs final counts, deduplication \%, rebuild scripts) enabling independent verification. GitHub repository includes \texttt{deduplication\_log.csv} with exact matching rules, addressing replication crisis in empirical SE.

\textit{Empirical Validation:} Section 4.5 (lines 668-694) demonstrates \textbf{schema-specific modeling outperforms naive pooling} due to distinct feature semantics: LOC $\leftrightarrow$ algorithmic complexity, FP $\leftrightarrow$ functional breadth, UCP $\leftrightarrow$ actor-interaction patterns.

\textit{Positioning vs Prior Work:} We explicitly contrast methodological focus against algorithmic novelty in Related Work (Section 7, lines 1082-1089): ``Our contribution lies in establishing \textit{reproducible evaluation protocols}---analogous to ImageNet providing standardized benchmarks for computer vision---rather than proposing novel models.'' &

\textbf{Where Revised:}

$\bullet$ \textbf{Abstract} (lines 70-84): Added ``macro-averaging prevents LOC dominance'' and ``calibrated size-only baseline ensures fair parametric comparison.''

$\bullet$ \textbf{Introduction} (lines 105-115): Expanded from 3 generic contributions to 5 specific methodological innovations with quantitative details.

$\bullet$ \textbf{Section 2.1.1} (lines 133-143): New ``Baseline Fairness and Calibration'' subsection with scipy implementation.

$\bullet$ \textbf{Section 4.3} (lines 229-236): New ``Cross-Schema Aggregation Protocol'' paragraph with formula.

$\bullet$ \textbf{Section 4.5} (lines 668-694): Per-schema analysis validating stratified modeling rationale.

$\bullet$ \textbf{Table 1} (lines 248-275): Complete dataset provenance table.

$\bullet$ \textbf{Section 7} (lines 1082-1089): Methodological vs algorithmic positioning. \\

\midrule

``Add experiments with recalibrated COCOMO II for a fairer comparison.'' &

\textbf{Excellent suggestion—we have fully implemented calibrated parametric baseline.}

\textit{Acknowledgment:} Using default COCOMO II parameters (NASA defaults: $A=2.94$, $B=0.91$) when cost drivers unavailable creates unfair comparison—the baseline gets no opportunity to learn from training data while ML models optimize on the same data. This ``straw-man baseline'' criticism has undermined prior ML-for-SEE papers.

\textit{Action Taken—Calibrated Size-Only Baseline:}

We replaced uncalibrated COCOMO II with a \textbf{calibrated size-only power-law baseline}:
$$E = A \times (\text{Size})^B$$

where Size = KLOC (LOC schema), FP (FP schema), or UCP (UCP schema)—no backfiring conversion.

\textbf{Parameter Fitting Protocol:}
\begin{enumerate}[leftmargin=1.5em]
\item Transform to log-space: $\log_{10}(E) = \log_{10}(A) + B \cdot \log_{10}(\text{Size})$
\item Fit via least-squares using \texttt{scipy.optimize.curve\_fit} \textbf{strictly on training folds} for each random seed
\item Coefficient bounds: $A \in [0.1, 100]$, $B \in [0.5, 2.0]$ enforcing realistic effort-size relationships
\item Schema-specific calibration: Separate $(A, B)$ for LOC/FP/UCP respecting semantic differences
\end{enumerate}

\textbf{Implementation Details} (Section 2.1.1, lines 171-175):
\begin{verbatim}
from scipy.optimize import curve_fit
def power_law(size, A, B):
    return A * (size ** B)
popt, _ = curve_fit(power_law, 
    X_train['Size'], y_train,
    bounds=([0.1, 0.5], [100, 2.0]))
\end{verbatim}

This ensures: (a) no test-set leakage, (b) fair data access, (c) schema-appropriate calibration, (d) principled parametric form preserving COCOMO's multiplicative scaling philosophy.

\textbf{Results Validate Fairness:} Even with training-data calibration, parametric baseline still underperforms:
\begin{itemize}
\item Calibrated baseline: MMRE=2.790, MAE=35.2 PM, R$^2$=0.41
\item Random Forest: MMRE=0.647, MAE=12.66 PM, R$^2$=0.79
\item XGBoost: MMRE=0.680, MAE=13.24 PM, R$^2$=0.77
\end{itemize}

The 64.1\%  MMRE reduction (2.790 $\rightarrow$ 0.647) confirms that \textbf{fixed functional forms fundamentally struggle with heterogeneous project characteristics} beyond size, validating ML's necessity rather than indicating unfair comparison. &

\textbf{Where Revised:}

$\bullet$ \textbf{Section 2.1.1} (lines 133-143): New subsection detailing calibration methodology.

$\bullet$ \textbf{Section 4.2} (lines 554-562): Experimental protocol states ``COCOMO II baseline re-calibrated independently for each schema and random seed.''

$\bullet$ \textbf{Abstract} (line 79): Changed to ``calibrated size-only baseline (fitted on training data per schema).''

$\bullet$ \textbf{Table 1 footnote} (line 654): Clarifies ``COCOMO II refers to size-only power-law calibrated per schema; full cost-driver model not applicable.''

$\bullet$ \textbf{Section 2.1.1} (lines 171-175): Implementation code snippet with scipy details. \\

\midrule

\end{longtable}

\end{document}
