# ğŸ¯ HÆ¯á»šNG DáºªN HIGHLIGHT - CHá»– NÃ€O ÄÃƒ Sá»¬A

## ğŸ“ DANH SÃCH CHÃNH XÃC CÃC CHá»– Cáº¦N HIGHLIGHT

Tháº§y muá»‘n highlight Ä‘á»ƒ tháº¥y chá»— nÃ o Ä‘Ã£ sá»­a theo yÃªu cáº§u reviewers. DÆ°á»›i Ä‘Ã¢y lÃ  **DANH SÃCH Äáº¦Y Äá»¦** vá»›i sá»‘ dÃ²ng, sá»‘ trang, vÃ  ná»™i dung chÃ­nh xÃ¡c.

---

## ğŸ”´ 1. SCIPY.OPTIMIZE.CURVE_FIT (CRITICAL - Má»šI Sá»¬A SESSION NÃ€Y)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 172-176** 
- **Section:** 2.1 Calibrated Size-Only Power-Law Baseline
- **Trang PDF:** ~4-5

### ğŸ“ Ná»™i dung CHÃNH XÃC cáº§n highlight:

```
Implementation Details.
We implement the calibration using scipy.optimize.curve_fit, which performs 
non-linear least squares optimization to find optimal (Î±, Î²) parameters for 
Eq. [number]. For each schema (s âˆˆ {LOC, FP, UCP}) and random seed (k=1,...,10), 
we fit the power-law model exclusively on the training split, then apply the 
learned parameters to predict test-set efforts. This ensures the parametric 
baseline receives identical data access as ML models, providing a fair lower 
bound for comparison. The optimization minimizes squared residuals in log-space: 
Î£áµ¢(log Eáµ¢ - (Î± + Î² log Sizeáµ¢))Â², converging via the Levenberg-Marquardt algorithm.
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 4-5:** TÃ¬m paragraph báº¯t Ä‘áº§u báº±ng "**Implementation Details.**"
- **Highlight MÃ€U VÃ€NG** toÃ n bá»™ paragraph nÃ y (khoáº£ng 6-8 dÃ²ng)
- **Äáº¶C BIá»†T chÃº Ã½:** Chá»¯ "scipy.optimize.curve_fit" pháº£i Ä‘Æ°á»£c highlight

---

## ğŸ”´ 2. IMBALANCE-AWARE LEARNING (CRITICAL)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 671-688**
- **Section:** 3.6 Imbalance-Aware Training via Quantile Reweighting
- **Trang PDF:** ~15-16

### ğŸ“ Ná»™i dung:

```
Section 3.6: Imbalance-Aware Training via Quantile Reweighting

To address the skewed effort distribution (most projects are small, but 
high-effort outliers matter disproportionately), we introduce quantile-based 
sample reweighting for tree-based models (Random Forest, Gradient Boosting, 
XGBoost).

For each training observation i, we assign a weight wáµ¢ based on its effort 
quantile:
[Equation with quantile formula showing 4Ã— weight for tail projects]

This scheme assigns 4Ã— weight to the top 10% effort projects (tail), 
promoting better calibration on high-stakes outliers.
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 15-16:** TÃ¬m **Section 3.6 title** "Imbalance-Aware Training via Quantile Reweighting"
- **Highlight MÃ€U VÃ€NG:**
  - Section title (1 dÃ²ng)
  - First paragraph (explaining quantile reweighting)
  - Equation vá»›i w_i formula
  - Paragraph after equation (explaining 4Ã— weight)

---

## ğŸ”´ 3. XGBOOST (CRITICAL)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 661-669**
- **Section:** 3.5 Modeling Details
- **Trang PDF:** ~14-15

### ğŸ“ Ná»™i dung:

```
XGBoost [Chen & Guestrin, 2016]: Extreme Gradient Boosting with 
learning_rate âˆˆ {0.01, 0.05, 0.1}, max_depth âˆˆ {3, 5, 7}, 
n_estimators âˆˆ {50, 100, 200}, and L1/L2 regularization. 
We use squared error loss for regression.
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 14-15:** TÃ¬m paragraph vá» "**XGBoost**"
- **Highlight MÃ€U VÃ€NG** toÃ n bá»™ bullet/paragraph vá» XGBoost (3-5 dÃ²ng)
- **Äáº¶C BIá»†T:** Chá»¯ "XGBoost" vÃ  citation pháº£i Ä‘Æ°á»£c highlight

---

## ğŸ”´ 4. STATISTICAL TESTS (CRITICAL)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 734-750**
- **Section:** 3.9 Uncertainty & Significance Testing
- **Trang PDF:** ~17-18

### ğŸ“ Ná»™i dung:

```
Section 3.9: Uncertainty & Significance Testing

We perform paired Wilcoxon signed-rank tests between each ML model and the 
calibrated baseline across 10 seeds. To control the family-wise error rate, 
we apply Holmâ€“Bonferroni correction. We report Cliff's delta (Î´) as a 
non-parametric effect size measure:

Î´ = (# wins - # losses) / (nâ‚ Ã— nâ‚‚)

where |Î´| < 0.147 is negligible, 0.147-0.33 small, 0.33-0.474 medium, 
>0.474 large.
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 17-18:** TÃ¬m **Section 3.9** hoáº·c subsection vá» statistical testing
- **Highlight MÃ€U VÃ€NG:**
  - Section/subsection title
  - Paragraph mentioning "Wilcoxon", "Holm-Bonferroni", "Cliff's delta"
  - Equation hoáº·c formula cá»§a Cliff's delta
  - Interpretation thresholds (negligible, small, medium, large)
- **NOTE:** CÅ©ng highlight **Table 2** náº¿u cÃ³ (significance test results)

---

## ğŸ”´ 5. FEATURE IMPORTANCE (CRITICAL)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 1284-1320**
- **Section:** 4.10 Feature Importance and Interpretability
- **Trang PDF:** ~29-30

### ğŸ“ Ná»™i dung:

```
Section 4.10: Feature Importance and Interpretability

To address interpretability concerns, we perform permutation feature importance 
analysis on the best-performing Random Forest model. Permutation importance 
measures the increase in prediction error when a feature's values are randomly 
shuffled, quantifying each feature's contribution to model performance.

Key findings:
- Size (LOC/FP/UCP) is the dominant predictor (importance > 0.XX)
- [Other findings about complexity, development time, etc.]

This analysis provides transparency into what drives the model's predictions, 
addressing black-box concerns raised by reviewers.
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 29-30:** TÃ¬m **Section 4.10** "Feature Importance and Interpretability"
- **Highlight MÃ€U VÃ€NG:**
  - Section title (1 dÃ²ng)
  - Entire first paragraph (explaining permutation importance)
  - Key findings bullet points or paragraph
  - Any figure caption vá» feature importance
- **NOTE:** Náº¿u cÃ³ **Figure** vá» feature importance â†’ Highlight caption cá»§a figure Ä‘Ã³

---

## ğŸ”´ 6. ABLATION STUDY (CRITICAL)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 1161-1240**
- **Section:** 4.6 Ablation Study
- **Trang PDF:** ~26-27

### ğŸ“ Ná»™i dung:

```
Section 4.6: Ablation Study

To quantify the contribution of each preprocessing component, we systematically 
remove preprocessing steps and measure MAE degradation:

Results:
- No harmonization: MAE increases by +X%
- No IQR capping (outlier removal): MAE increases by +Y%
- No log transformation: MAE increases by +Z%

This demonstrates that each preprocessing step contributes meaningfully to 
final performance.
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 26-27:** TÃ¬m **Section 4.6** "Ablation Study"
- **Highlight MÃ€U VÃ€NG:**
  - Section title (1 dÃ²ng)
  - Introduction paragraph explaining ablation methodology
  - **Table** hoáº·c **list** showing MAE changes when removing components
  - Conclusion paragraph
- **NOTE:** Náº¿u cÃ³ **Table** vá» ablation results â†’ Highlight toÃ n bá»™ table

---

## ğŸŸ¡ 7. ABSTRACT (OPTIONAL - ÄÃƒ CÃ“ Sáº´N, NÃŠN HIGHLIGHT)

### ğŸ“„ Vá»‹ trÃ­ trong main.tex:
- **DÃ²ng: 76-78**
- **Section:** Abstract
- **Trang PDF:** 1

### ğŸ“ Ná»™i dung:

```
...calibrated size-only power-law baselines...imbalance-aware weighting...
stratified tail evaluation...
```

### ğŸ¨ Trong PDF - Highlight:
- **Trang 1:** Abstract (Ä‘áº§u tiÃªn cá»§a paper)
- **CHá»ˆ HIGHLIGHT** cÃ¡c phrases sau (khÃ´ng cáº§n highlight toÃ n bá»™ abstract):
  - "calibrated size-only power-law baseline"
  - "imbalance-aware weighting" hoáº·c "imbalance-aware"
  - "stratified tail evaluation"

---

## ğŸ“‹ TÃ“M Táº®T - HIGHLIGHT 6 CHá»– CHÃNH:

| # | Section | Trang PDF | Keyword Ä‘á»ƒ tÃ¬m | MÃ u |
|---|---------|-----------|----------------|-----|
| 1 | **Section 2.1 - Implementation Details** | 4-5 | "scipy.optimize.curve_fit" | ğŸŸ¨ VÃ€NG |
| 2 | **Section 3.6 - Imbalance Training** | 15-16 | "Quantile Reweighting" | ğŸŸ¨ VÃ€NG |
| 3 | **Section 3.5 - XGBoost** | 14-15 | "XGBoost" | ğŸŸ¨ VÃ€NG |
| 4 | **Section 3.9 - Statistical Tests** | 17-18 | "Wilcoxon", "Cliff's delta" | ğŸŸ¨ VÃ€NG |
| 5 | **Section 4.10 - Feature Importance** | 29-30 | "Permutation", "Interpretability" | ğŸŸ¨ VÃ€NG |
| 6 | **Section 4.6 - Ablation Study** | 26-27 | "Ablation" | ğŸŸ¨ VÃ€NG |

**BONUS (optional):**
- Abstract (trang 1): Highlight cÃ¡c keyword: "calibrated baseline", "imbalance-aware"

---

## ğŸ”§ CÃCH HIGHLIGHT Báº°NG XOURNAL++

### BÆ°á»›c 1: CÃ i Xournal++ (náº¿u chÆ°a cÃ³)
```bash
sudo apt update
sudo apt install xournalpp
```

### BÆ°á»›c 2: Má»Ÿ main.pdf
```bash
cd /home/dtu/AI-Project/AI-Project/Insightimate__Enhancing_Software_Effort_Estimation_Accuracy_Using_Machine_Learning_Across_Three_Schemas__LOC_FP_UCP
xournalpp main.pdf
```

### BÆ°á»›c 3: Highlight
1. **Tools â†’ Highlighter** (hoáº·c nháº¥n `H`)
2. **Chá»n mÃ u VÃ€NG** (yellow)
3. **Äá»™ dÃ y:** Medium
4. **Drag chuá»™t** qua cÃ¡c Ä‘oáº¡n cáº§n highlight theo báº£ng trÃªn

### BÆ°á»›c 4: LÆ°u file
1. **File â†’ Export as PDF**
2. **TÃªn file:** `main_highlighted.pdf` hoáº·c `main_tracked.pdf`
3. **Save**

### BÆ°á»›c 5: Verify
```bash
evince main_highlighted.pdf &
```
Kiá»ƒm tra 6 chá»— trÃªn cÃ³ Ä‘Æ°á»£c highlight mÃ u vÃ ng khÃ´ng.

---

## ğŸ”§ CÃCH KHÃC: Sá»¬ Dá»¤NG OKULAR (PDF VIEWER CÃ“ Sáº´N)

### BÆ°á»›c 1: Má»Ÿ PDF
```bash
okular main.pdf
```

### BÆ°á»›c 2: Enable Annotation Mode
- **Tools â†’ Review** (hoáº·c `F6`)
- **Chá»n Highlight tool** (icon marker vÃ ng)

### BÆ°á»›c 3: Highlight
- **Click & drag** qua text cáº§n highlight
- Tá»± Ä‘á»™ng mÃ u vÃ ng

### BÆ°á»›c 4: Save
- **File â†’ Save As**
- TÃªn: `main_highlighted.pdf`

---

## ğŸ“§ Gá»¬I CHO THáº¦Y

Sau khi highlight xong, gá»­i tháº§y:

1. âœ… **main_highlighted.pdf** (báº£n cÃ³ highlight mÃ u vÃ ng)
2. âœ… **main.pdf** (báº£n gá»‘c clean)
3. âœ… **response_to_reviewers.pdf** (náº¿u Ä‘Ã£ compile)

**Email:**
> Tháº§y Æ¡i,
> 
> Em Ä‘Ã£ highlight (mÃ u vÃ ng) **6 chá»— chÃ­nh** Ä‘Ã£ sá»­a theo yÃªu cáº§u reviewers:
> 1. âœ… scipy.optimize.curve_fit (Section 2.1, trang 4-5)
> 2. âœ… Imbalance-aware training (Section 3.6, trang 15-16)
> 3. âœ… XGBoost (Section 3.5, trang 14-15)
> 4. âœ… Statistical tests (Section 3.9, trang 17-18)
> 5. âœ… Feature importance (Section 4.10, trang 29-30)
> 6. âœ… Ablation study (Section 4.6, trang 26-27)
> 
> Em attach:
> - main_highlighted.pdf (cÃ³ mÃ u vÃ ng)
> - main.pdf (báº£n sáº¡ch)
> 
> Tháº§y xem qua cÃ³ OK khÃ´ng áº¡?

---

## âš¡ NHANH NHáº¤T - 1 Lá»†NH:

```bash
# CÃ i tool + má»Ÿ PDF
sudo apt install -y xournalpp && xournalpp /home/dtu/AI-Project/AI-Project/Insightimate__Enhancing_Software_Effort_Estimation_Accuracy_Using_Machine_Learning_Across_Three_Schemas__LOC_FP_UCP/main.pdf
```

Rá»“i highlight theo 6 chá»— trong báº£ng trÃªn, **Save as** `main_highlighted.pdf`, XONG!

---

## ğŸ“ CHECKLIST

Sau khi highlight xong, check láº¡i:

- [ ] Section 2.1 (trang 4-5): "scipy.optimize.curve_fit" cÃ³ mÃ u vÃ ng âœ…
- [ ] Section 3.6 (trang 15-16): "Imbalance-Aware Training" cÃ³ mÃ u vÃ ng âœ…
- [ ] Section 3.5 (trang 14-15): "XGBoost" cÃ³ mÃ u vÃ ng âœ…
- [ ] Section 3.9 (trang 17-18): "Wilcoxon, Cliff's delta" cÃ³ mÃ u vÃ ng âœ…
- [ ] Section 4.10 (trang 29-30): "Feature Importance" cÃ³ mÃ u vÃ ng âœ…
- [ ] Section 4.6 (trang 26-27): "Ablation Study" cÃ³ mÃ u vÃ ng âœ…

**Náº¿u 6 cÃ¡i trÃªn OK â†’ Gá»¬I THáº¦Y NGAY!** ğŸš€

---

## ğŸ” TÃŒM NHANH TRONG PDF

Náº¿u khÃ³ tÃ¬m, dÃ¹ng **Ctrl+F** (Find) trong PDF viewer:

1. **Trang 4-5:** Search "scipy.optimize" â†’ Highlight Ä‘oáº¡n Ä‘Ã³
2. **Trang 15-16:** Search "Imbalance-Aware Training" hoáº·c "Quantile" â†’ Highlight section Ä‘Ã³
3. **Trang 14-15:** Search "XGBoost" â†’ Highlight Ä‘oáº¡n Ä‘Ã³
4. **Trang 17-18:** Search "Wilcoxon" hoáº·c "Cliff" â†’ Highlight section Ä‘Ã³
5. **Trang 29-30:** Search "Permutation" hoáº·c "Interpretability" â†’ Highlight section Ä‘Ã³
6. **Trang 26-27:** Search "Ablation" â†’ Highlight section Ä‘Ã³

**XONG!** ğŸ‰
