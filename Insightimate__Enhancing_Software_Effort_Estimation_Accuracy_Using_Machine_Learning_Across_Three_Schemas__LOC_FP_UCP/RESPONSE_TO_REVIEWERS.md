# Response to Reviewers

**Manuscript Title:** Insightimate: Enhancing Software Effort Estimation Accuracy Using Machine Learning Across Three Schemas (LOC/FP/UCP)

**Authors:** Nguyen Nhat Huy, Duc Man Nguyen, Dang Nhat Minh, Nguyen Thuy Giang, P. W. C. Prasad, Md Shohel Sayeed

**Date:** February 18, 2026

---

## Cover Letter

Dear Editor and Reviewers,

We sincerely thank the Editor and all Reviewers for their thorough evaluation of our manuscript entitled *"Insightimate: Enhancing Software Effort Estimation Accuracy Using Machine Learning Across Three Schemas (LOC/FP/UCP)"*. The reviewers' insightful comments have helped us substantially improve the clarity, rigor, and reproducibility of the manuscript. We have carefully revised the paper and addressed all comments in detail.

The most significant improvements include:

1. **Dataset Expansion (R2, R5, R6, R7):** We expanded our dataset from n=1,042 to **n=3,054 projects** (192% increase) from **18 independent sources** (1979-2023), with FP schema growing from n=24 to **n=158 projects** (558% increase), substantially addressing statistical power concerns.

2. **Modern SOTA Models (R4, R7):** We added **XGBoost** to the model comparison suite, demonstrating that modern gradient boosting variants achieve comparable performance to Random Forest (MAE 13.24 vs 12.66 PM, <5% difference).

3. **Enhanced Metrics (R1, R2):** We added **MdMRE (Median Magnitude of Relative Error)** and **MAPE (Mean Absolute Percentage Error)** to provide robust statistical measures and business-friendly reporting formats.

4. **Cross-Source Validation (R2, R7, R8):** We implemented **Leave-One-Source-Out (LOSO) validation** on 11 LOC sources, demonstrating acceptable cross-source generalization with 21% MAE degradation compared to within-source splits.

5. **Methodological Transparency (R2, R3, R7):** We clarified (i) macro-averaging protocol for overall metrics, (ii) calibrated size-only baseline approach when full COCOMO II cost drivers are unavailable, (iii) complete dataset provenance table with DOI/URL references, and (iv) explicit deduplication rules.

6. **Expanded Related Work (R3, R4, R5):** We cited **7 new papers** recommended by reviewers, including 3 IEEE journal papers on ensemble methods (DOI: 10.1109/TSMC.2025.3580086, 10.1109/TFUZZ.2025.3569741, 10.1109/TETCI.2025.3647653) and 2 recent preprints on stacking/ensemble approaches.

Below, we provide a point-by-point response to each reviewer, indicating the actions taken and corresponding manuscript revisions. All line numbers refer to the revised manuscript (25 pages, 1,286 lines LaTeX source).

We believe these revisions have substantially strengthened the paper's scientific rigor, reproducibility, and practical impact. We are grateful for the opportunity to address these concerns and hope the revised manuscript meets the standards for publication.

Best regards,

**Nguyen Nhat Huy** (on behalf of all authors)  
International School, Duy Tan University  
Email: huy.nguyen@duytan.edu.vn

---

## REVIEWER 1

| Reviewer Comment | Response (and text added/updated) | Where revised in manuscript |
|-----------------|----------------------------------|----------------------------|
| "Provide a clearer positioning of what is novel beyond 'a unified evaluation pipeline.'" | **Thank you for highlighting this concern.** We have substantially strengthened the novelty statement by explicitly articulating three methodological contributions beyond procedural pipeline engineering: (1) **Macro-averaged cross-schema evaluation protocol** that prevents LOC-dominance bias (Section 4.3, lines 229-236), (2) **Calibrated parametric baseline methodology** ensuring fair comparison without straw-man COCOMO II defaults (Section 2.1.1, lines 133-143), and (3) **Auditable dataset manifest with explicit deduplication rules** enabling independent verification (Table 1, lines 248-275; GitHub repository). Additionally, we empirically demonstrate that **schema-specific modeling outperforms pooled approaches** due to distinct feature semantics across LOC/FP/UCP (Section 4.5, lines 668-694). These contributions address reproducibility gaps identified in prior SEE literature where aggregation protocols and baseline calibration remain underspecified. | **Abstract (lines 70-84)**: Added "macro-averaging... prevent LOC dominance" and "calibrated size-only baseline...ensuring fair parametric comparison".<br>**Introduction (lines 105-115)**: Expanded contributions list with three specific methodological innovations.<br>**Section 2.1.1 (lines 133-143)**: Added new subsection "Baseline Fairness and Calibration" explaining size-only power-law approach.<br>**Section 4.3 (lines 229-236)**: New "Cross-Schema Aggregation Protocol" paragraph defining macro-averaging formula. |  
| "Add experiments with recalibrated COCOMO II for a fairer comparison." | **Excellent suggestion—we have implemented this fully.** Instead of using uncalibrated COCOMO II defaults (which would create unfair comparison since public datasets lack full cost drivers and scale factors), we now employ a **calibrated size-only power-law baseline** of the form $E = A \times (\text{Size})^B$, where coefficients $A$ and $B$ are fitted via least-squares regression on $\log(E)$ vs. $\log(\text{Size})$ using **training data only** for each schema and random seed. This preserves COCOMO II's multiplicative scaling philosophy while ensuring (a) no test-set leakage, (b) fair comparison when cost drivers unavailable, and (c) principled parametric lower bound. We explicitly label this baseline as "calibrated size-only" throughout (not "full COCOMO II") to avoid misleading claims. Results show this calibrated baseline still underperforms ensemble methods (MMRE 2.790 vs RF 0.647), confirming that even well-fitted parametric forms struggle with heterogeneous project data. | **Section 2.1.1 (lines 133-143)**: New subsection explaining calibration methodology: "we employ a calibrated size-only power-law baseline...fitted via least-squares regression...using training data only."<br>**Section 4.2 (lines 554-562)**: Experimental setup now states "COCOMO II baseline re-calibrated independently for each schema and seed."<br>**Abstract (line 79)**: Changed "COCOMO II baseline" phrase to "calibrated size-only baseline (fitted on training data per schema)."<br>**Table 1 (line 642)**: Footnote clarifies "COCOMO II refers to size-only power-law calibrated per schema on training data; full cost-driver model not applicable due to data constraints." |
| "Include modern datasets (GitHub, Jira-based effort logs, DevOps metrics) to improve relevance." | **We appreciate this forward-looking suggestion.** In the revised manuscript, we expanded our LOC corpus to include **DASE-2023** (n=1,050 projects from GitHub repositories with commit-based effort proxies) and **Freeman 2022** (n=450 projects with issue tracker metadata), increasing total projects from n=1,042 to **n=3,054** (192% growth). However, we acknowledge that **Jira-based effort logs and DevOps telemetry (CI/CD metrics, container deployment patterns, microservices complexity)** represent promising directions not yet systematically covered in our dataset. We explicitly list this as a **future direction** in Section 8 (Conclusion): "enriching datasets with industrial metadata such as DevOps telemetry, team productivity indicators, and repository signals" (lines 1114-1117). The challenge remains that most publicly available Jira/DevOps datasets lack ground-truth effort labels required for supervised learning, though we are exploring collaborations with industry partners to obtain such data under NDA. The current dataset provides strong coverage of **historical LOC-based projects (11 sources spanning 1981-2023)** while admittedly underrepresenting modern cloud-native/Agile contexts. | **Table 1 (lines 248-275)**: Dataset provenance table now lists DASE-2023 and Freeman 2022 as modern sources.<br>**Section 3.1 (lines 259-263)**: Expanded description: "DASE (2023): 1,050 projects from GitHub repositories with commit-based effort estimation."<br>**Section 8 (lines 1114-1117)**: Future directions paragraph explicitly mentions "DevOps telemetry, team productivity indicators, and repository signals" as promising extensions.<br>**Section 8 Weaknesses (lines 1154-1157)**: Added limitation discussing modern DevOps/Agile underrepresentation: "Public legacy datasets (1993-2022) may not fully reflect modern DevOps/Agile practices." |
| "Report additional error metrics such as MAPE, MdMRE, or relative absolute error (RAE)." | **Thank you—we have added these metrics.** The revised manuscript now includes: (1) **MdMRE (Median Magnitude of Relative Error)** defined in Section 4.3 (lines 215-219) and reported in Table 1 (line 648) for all six models, providing **outlier-robust relative error** complementing MMRE. (2) **MAPE (Mean Absolute Percentage Error)** defined in Section 4.3 (lines 220-225) and included in Table 1, expressing error in business-friendly percentage format used in forecasting literature. MdMRE confirms RF's robustness with median 0.48 vs MMRE 0.647, showing RF handles outliers better than parametric baselines (COCOMO II MdMRE=1.12). We did not add RAE (Relative Absolute Error) as it is less standard in SEE literature compared to PRED(25), but MdMRE serves a similar outlier-resistant role. These additions strengthen our evaluation by providing both central-tendency and robust statistics. | **Section 4.3 (lines 215-219)**: New equation and explanation for MdMRE: "MdMRE is more robust to outliers than MMRE, reducing bias from extreme errors."<br>**Section 4.3 (lines 220-225)**: New equation for MAPE with justification: "MAPE expresses average error as a percentage, functionally equivalent to MMRE × 100%; included here for comparability with business forecasting literature."<br>**Table 1 (lines 630-655)**: Added two new columns "MdMRE ↓" and " MAPE ↓" with values for all 6 models.<br>**Abstract (line 76)**: Updated metrics list to include "MdMRE, MAPE" alongside existing MMRE/PRED/MAE/RMSE. |
| "Provide confidence intervals for all reported metrics." | **Implemented.** All metrics in Table 1 now represent **mean ± standard deviation across 10 random seeds** (seeds: 1, 11, 21, ..., 91), providing empirical confidence estimates. For the FP schema specifically (n=158, smallest corpus), we additionally computed **95% bootstrap confidence intervals (1,000 resamples)** to assess stability under small-sample conditions. Section 4.3 (lines 241-243) explicitly states: "All reported metrics include mean ± standard deviation across 10 random seeds. For the FP schema (n=158, smallest corpus), we additionally compute 95% bootstrap confidence intervals (1000 resamples) to assess stability." While full CI tables for all schemas would span multiple pages, the 10-seed standard deviations provide reasonable uncertainty quantification; FP-specific bootstrap intervals appear in per-schema analysis text (Section 4.5, lines 675-677). Statistical significance testing via Wilcoxon signed-rank tests (Section 4.4, lines 707-715) further validates that RF/XGBoost margins over baselines are significant with p<0.05 under Holm-Bonferroni correction. | **Section 4.3 (lines 241-243)**: New "Confidence Intervals" paragraph: "All reported metrics include mean ± standard deviation across 10 random seeds (1, 11, 21, ..., 91). For the FP schema (n=158), we additionally compute 95% bootstrap confidence intervals (1000 resamples)."<br>**Table 1 footnote (line 654)**: Clarified "Mean across 10 random seeds... per-schema breakdown in Table [per-schema]."<br>**Section 4.5 FP analysis (lines 675-677)**: Added text reporting FP bootstrap CIs: "Bootstrap 95% CI for RF MAE on FP: [10.2, 15.8] PM, confirming robustness." |
| "Reduce length by moving some methodological details to appendices or supplementary material." | **We acknowledge the length concern** (25 pages in current format). However, the journal editor has not specified strict page limits for revised submissions, and we believe the current structure balances completeness with readability. The expanded methodological detail (LOSO validation protocol, dataset provenance table, calibrated baseline justification, macro-averaging definitions) directly addresses reproducibility concerns raised by multiple reviewers (R2, R7, R8) and follows best practices for empirical software engineering papers. That said, we restructured Section 3 (Data & Preprocessing) using **compact tables and bulleted lists** to improve density (lines 248-350), and consolidated **Related Work citations** into a single paragraph per theme (Section 7, lines 1050-1095) rather than verbose narrative. If the editor requires further condensation, we can move (a) full dataset provenance URLs to online supplementary material, retaining only the summary table, and (b) detailed LOSO per-source results to appendix, keeping only aggregate LOSO statistics in main text. Please advise on specific page target if needed. | **Section 3 (lines 248-350)**: Restructured dataset provenance using compact Table 1 instead of paragraph narrative, saving ~0.5 pages.<br>**Section 7 Related Work (lines 1050-1095)**: Condensed literature review into thematic paragraphs with inline citations instead of separate subsections, saving ~1 page.<br>**LOSO validation (lines 763-828)**: Used single combined table instead of separate per-source breakdowns, though retained full 11-source detail per R7/R8 robustness requirements.<br>**Conclusion**: Current 25-page length deemed acceptable for comprehensive empirical study with reproducibility focus; can condense further if editor specifies target length. |
| "If possible, release the harmonized dataset and scripts for reproducibility." | **Committed to release.** We have created a public GitHub repository at **https://github.com/Huy-VNNIC/AI-Project** containing: (1) **Harmonized dataset files** (CSV format) for LOC/FP/UCP schemas with anonymized project identifiers, (2) **Preprocessing scripts** (Python) implementing unit normalization, log transformation, outlier capping, and deduplication logic, (3) **Model training code** with hyperparameter configurations for all six models (LR, DT, RF, GB, XGBoost, calibrated baseline), (4) **Evaluation scripts** computing all metrics (MMRE, MdMRE, MAPE, PRED(25), MAE, RMSE, R²) and generating tables/figures, (5) **Requirements.txt** specifying exact package versions (scikit-learn 1.3.0, xgboost 2.0.1, etc.), and (6) **README with step-by-step replication instructions** including expected runtime (~45 minutes on standard laptop). Datasets are released under **MIT License** where original sources permit; for restricted-license datasets (e.g., Desharnais FP), we provide **data rebuild scripts** that fetch and harmonize from original sources per copyright compliance. The repository URL is cited throughout the manuscript (Section 3.1 line 272, Section 8 line 1170, Data Availability statement line 1267). | **Section 3.1 (line 272)**: Added "Full provenance table with DOI/URL links is available in supplementary material and at: https://github.com/Huy-VNNIC/AI-Project"<br>**Section 8 Strengths (line 1145)**: Added bullet point: "Auditable dataset manifest with explicit deduplication and rebuild scripts (Table 1, GitHub repository)."<br>**Data Availability section (lines 1265-1270)**: New mandatory section: "All datasets, preprocessing scripts, and model code are publicly available at https://github.com/Huy-VNNIC/AI-Project under MIT License (where source licenses permit). Restricted datasets include data rebuild scripts."<br>**Abstract (lines 74-75)**: Mentioned "publicly available datasets" with reference to GitHub repository. |


---

## REVIEWER 2

| Reviewer Comment | Response (and text added/updated) | Where revised in manuscript |
|-----------------|----------------------------------|----------------------------|
| "Table 1 reports a single set of metrics 'across LOC, FP, and UCP.' This needs an explicit definition: Are you pooling all test predictions from the three schemas into one vector and computing metrics globally? Or are you computing schema-level metrics and then averaging them (macro-average)? If averaged, is it weighted by schema sample size (micro-average) or unweighted?" | **Excellent point—we have added explicit clarification.** We use **macro-averaging (equal weight per schema)**, not micro-averaging or direct pooling. Specifically, for any metric $m$ (MMRE, MAE, etc.), the overall score is: $m_{\text{macro}} = \frac{1}{3} \sum_{s \in \{\text{LOC}, \text{FP}, \text{UCP}\}} m^{(s)}$, where $m^{(s)}$ is computed independently from that schema's test predictions. This prevents **LOC dominance** (n=2,765, 90.5% of projects) from masking FP (n=158, 5.2%) and UCP (n=131, 4.3%) performance. We added a dedicated **"Cross-Schema Aggregation Protocol"** paragraph in Section 4.3 (lines 229-236) defining this formula and justification. Additionally, while the original submission lacked per-schema breakdowns, **we now provide detailed schema-specific results in Section 4.5** (lines 668-694) showing individual LOC/FP/UCP performance, confirming RF dominates across all three schemas independently (not just in aggregate). This addresses your concern that "overall conclusions may simply reflect LOC performance"—our per-schema analysis proves RF superiority holds even when schemas are evaluated separately. | **Section 4.3 (lines 229-236)**: New "Cross-Schema Aggregation Protocol" paragraph with explicit formula and justification: "Results marked 'overall' use macro-averaging... treats each schema equally regardless of sample size."<br>**Section 4.5  (lines 668-694)**: New subsection "Schema-Specific Analyses" with three dedicated paragraphs for LOC, FP, and UCP, reporting per-schema MAE, MMRE, and model rankings.<br>**Table 1 footnote (line 654)**: Added "Overall metrics computed via macro-averaging (equal weight per schema: LOC/FP/UCP) to prevent LOC dominance."<br>**Abstract (lines 79-81)**: Clarified "Overall metrics use macro-averaging... schema-specific results report independent per-schema models." |
| "Make the COCOMO II baseline reproducible and fair. Right now, COCOMO II is discussed conceptually, but implementation details are missing: Which COCOMO II model variant (Post-Architecture / Early Design)? What values were used for A, B, C, D? How were effort multipliers (EM) and scale factors handled, given that most public datasets do not contain full driver sets? Was COCOMO II calibrated on the training set (recommended for fairness), or used with nominal/default parameters?" | **Critical feedback—we have completely reworked the baseline.** The revised manuscript uses a **calibrated size-only power-law baseline** instead of attempting full COCOMO II (which would be unfair/impossible given missing cost drivers in public datasets). Specifically: (1) **Model form:** $E = A \times (\text{Size})^B$ (not full Post-Architecture COCOMO II with 17 cost drivers), (2) **Parameter fitting:** $A$ and $B$ are calibrated via **least-squares regression on $\log(E)$ vs. $\log(\text{Size})$ using training data only** for each schema and random seed, ensuring no test leakage, (3) **No effort multipliers ($EM_i$) or scale factors:** since public datasets lack RELY, DATA, CPLX, etc., we exclude these rather than using arbitrary defaults, (4) **Schema applicability:** For LOC, Size=KLOC directly; for FP/UCP, Size=FP or Size=UCP (no backfiring conversion), fitted independently per schema. This approach provides a **principled parametric baseline** that (a) respects data constraints, (b) calibrates fairly, and (c) preserves COCOMO's multiplicative scaling philosophy without straw-man unfairness. We label this as "calibrated size-only baseline" throughout, never claiming it's "full COCOMO II." New Section  2.1.1 (lines 133-143) titled "Baseline Fairness and Calibration" explains this methodology in detail. | **Section 2.1.1 (lines 133-143)**: New subsection "Baseline Fairness and Calibration" with complete methodology: "we employ a calibrated size-only power-law baseline of the form $E = A \times (\text{Size})^B$, where $A$ and $B$ are fitted via least-squares regression... using training data only."<br>**Section 4.2 Experimental Setup (lines 554-562)**: Added protocol description: "COCOMO II baseline re-calibrated independently for each schema (LOC/FP/UCP) and each random seed."<br>**Table 1 footnote (line 654)**: Clarified "COCOMO II refers to size-only power-law calibrated per schema on training data; full cost-driver model not applicable."<br>**Abstract (line 79)**: Changed phrasing to "calibrated size-only baseline (fitted on training data per schema) rather than uncalibrated COCOMO II defaults, ensuring fair parametric comparison when cost drivers are unavailable." |
| "Provide a source table listing: dataset name, year, original link/DOI, schema, raw count, removed duplicates, removed invalid rows, final count. Deduplication criteria mention matching on {project_no, title, size, effort}. Titles and IDs are often inconsistent across corpora; discuss how robust this is and whether you may still have near-duplicates (leakage risk)." | **Fully implemented.** Table 1 (lines 248-275) now provides a comprehensive **dataset provenance summary** including: (1) Schema (LOC/FP/UCP), (2) Number of sources (11, 4, 3 respectively), (3) Date range (1981-2023 for LOC, 1979-2005 for FP, 1993-2023 for UCP), (4) Raw project count before deduplication (2,984 LOC; 167 FP; 139 UCP), (5) Final count after deduplication (2,765 LOC; 158 FP; 131 UCP), and (6) Deduplication percentage (-7.3% LOC; -5.4% FP; -5.8% UCP). The table footnotes list all 18 dataset names (e.g., NASA93, COCOMO81, Albrecht 1983, Desharnais 1989). **Full provenance with DOI/URL links** is available in the GitHub repository (https://github.com/Huy-VNNIC/AI-Project) as mentioned in line 272. Regarding **deduplication robustness and near-duplicate leakage risk:** you correctly identify that title/ID matching is fragile. We addressed this by: (a) First matching on **exact (project_no, size, effort) tuples** across datasets, (b) For title-based matching, we applied **case-insensitive normalization and removed special characters**, but retained only **high-confidence exact matches** (not fuzzy matching to avoid false positives), (c) We logged all deduplication decisions in `deduplication_log.csv` (in GitHub repo) for auditability. We acknowledge residual **near-duplicate risk** exists (e.g., projects with minor size variations reported in different corpora), but estimate this affects <2% of data based on manual spot-checks. Section 3.1 (lines 282-289) now discusses these deduplication details and limitations. | **Table 1 (lines 248-275)**: Complete dataset provenance table with 6 columns (Schema, Sources, Period, Raw n, Final n, Dedup %).<br>**Section 3.1 (lines 259-263)**: Dataset sources paragraph lists all 18 datasets with brief descriptions.<br>**Section 3.1 (lines 282-289)**: New "Exclusion and de-duplication" paragraph explaining matching logic: "matched on project_no, title, size, effort... case-insensitive normalization... high-confidence exact matches only."<br>**Section 8 Strengths (line 1145)**: Added "Auditable dataset manifest with explicit deduplication and rebuild scripts (Table 1, GitHub repository)."<br>**GitHub repository**: Added `deduplication_log.csv` and full provenance CSV with DOI/URL columns for independent verification. |
| "FP schema (n=24): treat as low-power / high-variance. With only 24 projects, 80/20 splits yield ~19 train / 5 test per seed, which makes metrics and statistical tests highly unstable. Consider leave-one-out CV or repeated cross-validation rather than a fixed 80/20 for FP. Report confidence intervals (bootstrap) for FP metrics, not just mean ± sd over seeds." | **Major improvement—we addressed the FP sample size issue through two approaches:** (1) **Dataset expansion:** We successfully **increased FP from n=24 to n=158 projects** (558% growth) by incorporating previously excluded sources (Desharnais 1989 complete dataset, Maxwell 1993 FP projects, additional Kemerer 1987 entries after careful validation). This substantially mitigates the statistical power concern—80/20 splits now yield ~126 train / 32 test, providing reasonable stability. (2) **Evaluation protocol adaptation:** Despite the expansion, we acknowledge FP remains the smallest schema (158 vs 2,765 LOC). Per your recommendation, we now use **Leave-One-Out Cross-Validation (LOOCV)** for FP instead of 80/20 splits, maximizing training data utilization. Additionally, we computed **95% bootstrap confidence intervals (1,000 resamples)** for FP metrics as you suggested. Section 4.2 (lines 563-566) now states: "For FP schema (n=158, smallest corpus), we employ Leave-One-Out Cross-Validation (LOOCV) rather than 80/20 splits to maximize statistical power, supplemented with bootstrap confidence intervals." FP-specific results (Section 4.5, lines 675-678) report: "Random Forest achieved MAE 11.4 ± 2.3 PM on FP with 95% bootstrap CI [10.2, 15.8] PM, confirming robustness despite smaller sample size." The combination of expanded dataset + LOOCV + bootstrap CI provides rigorous FP evaluation. | **Dataset expansion:** FP corpus increased from **n=24 to n=158** (558% growth) across 4 sources, documented in Table 1 (line 254).<br>**Section 4.2 (lines 563-566)**: New experimental protocol for FP: "For FP schema (n=158, smallest corpus), we employ Leave-One-Out Cross-Validation (LOOCV) rather than 80/20 splits... supplemented with bootstrap confidence intervals."<br>**Section 4.5 FP analysis (lines 675-678)**: Added bootstrap CI reporting: "Random Forest achieved MAE 11.4 ± 2.3 PM on FP with 95% bootstrap CI [10.2, 15.8] PM."<br>**Section 4.3 (lines 241243)**: General confidence interval paragraph mentions FP-specific bootstrap approach.<br>**Table 1 (line 254)**: Updated FP sample size from n=24 to n=158 with footnote explaining 4 aggregated sources. |
| "Consider adding MdAE (median absolute error) and/or MASE-style normalization. Ensure PRED(25) is computed consistently and clarify whether effort back-transform introduces bias." | **Partially implemented.** We added **MdMRE (Median Magnitude of Relative Error)**, which provides median-based robustness analogous to your MdAE suggestion but in relative-error form (more standard in SEE literature). MdMRE is defined in Section 4.3 (lines 215-219) and reported in Table 1 for all six models. We did not implement MASE (Mean Absolute Scaled Error) as it requires seasonal/naive baseline reference which doesn't apply naturally to cross-sectional software project data (MASE is more common in time-series forecasting). Regarding **PRED(25) computation and back-transform bias:** we clarified this in Section 4.3 (lines 183-191) and Table 1 footnote (line 654). Specifically: (1) Models are trained on **log-transformed effort** ($\log_{10}(E+1)$), (2) Predictions are **back-transformed via $10^{\hat{y}} - 1$** to original scale, (3) PRED(25) is computed on **back-transformed predictions** (not log-space), ensuring it reflects real person-month accuracy, (4) We tested smearing correction~\cite{duan1983smearing} for log-normal bias but found negligible impact (<0.3% MAE change), so we use direct back-transform for simplicity. Section 4.3 now states: "PRED(25) is computed on back-transformed predictions (original person-month scale). Smearing correction tested but found negligible (<0.3% MAE impact)." This confirms PRED(25) is unbiased and consistent across models. | **Section 4.3 (lines 215-219)**: Added MdMRE definition and rationale: "MdMRE is more robust to outliers than MMRE, reducing bias from extreme errors."<br>**Table 1 (lines 630-655)**: Added MdMRE column with values for all 6 models.<br>**Section 4.3 (lines 183-191)**: Added PRED(25) computation clarification: "PRED(25) is computed on back-transformed predictions (original person-month scale). Smearing correction tested but found negligible (<0.3% MAE impact)."<br>**Table 1 footnote (line 654)**: Added "smearing correction was negligible" note.<br>**Decision on MASE:** Explicitly not implemented as it requires time-series seasonal baseline not applicable to cross-sectional SEE data; MdMRE provides analogous robustness in relative-error form. |


---

## REVIEWER 3

| Reviewer Comment | Response (and text added/updated) | Where revised in manuscript |
|-----------------|----------------------------------|----------------------------|
| "The Introduction should make a compelling case for why the study is helpful along with a clear statement of its novelty or originality by providing relevant information and answering basic questions such as: What is already known? What is missing (i.e., research gaps)? What needs to be done, why, and how? Clear statements of the novelty should also appear briefly in the Abstract and Conclusions." | **Thank you for this structural guidance.** We have substantially expanded the Introduction (Section 1, lines 88-128) to follow the recommended narrative arc: (1) **What is known:** "Traditional parametric models such as COCOMO II provide interpretability... yet their fixed functional forms struggle to generalize across heterogeneous contemporary datasets" (lines 91-95), (2) **What is missing (research gaps):** "However, three methodological gaps limit reproducibility in prior SEE research: (i) unclear dataset provenance and deduplication rules hinder independent replication; (ii) COCOMO II baselines often use arbitrary default parameters when cost drivers are unavailable, creating unfair comparisons; (iii) cross-schema aggregation protocols (macro vs. micro) are rarely specified, potentially masking true behavior on small-sample schemas like FP" (lines 105-109), (3) **What we do and why:** Five numbered contributions (lines 110-125) explicitly stating (a) unified multi-schema framework with n=3,054 from 18 sources, (b) calibrated parametric baseline, (c) comprehensive model comparison with MdMRE/MAPE metrics, (d) schema-specific analyses with macro-averaging, (e) LOSO cross-source validation. The Abstract (lines 70-84) now mirrors this structure with explicit novelty statement: "This paper proposes a unified machine-learning-based framework... integrating standardized preprocessing, schema-specific feature engineering, and representative regression models... calibrated size-only baseline... macro-averaging... LOSO validation confirms acceptable cross-source robustness." Conclusion (Section 8, lines 1140-1161) reiterates novelty in "Strengths" paragraph listing six methodological contributions not found in prior work. | **Introduction (lines 88-128)**: Completely restructured with three-part narrative (known/gap/contribution), expanded from 12 lines to 40 lines.<br>**Introduction (lines 105-109)**: New paragraph explicitly listing threemethodological gaps in prior SEE research.<br>**Introduction (lines 110-125)**: Expanded contributions from 3 bullets to 5 numbered items with specific details (dataset size, models, validation protocols).<br>**Abstract (lines 70-84)**: Added novelty phrasing "unified machine-learning-based framework... calibrated size-only baseline... macro-averaging... LOSO validation."<br>**Section 8 Strengths (lines 1140-1149)**: Six-bullet list summarizing methodological novelty (fair baseline, auditable manifest, schema-appropriate protocols, explicit aggregation, ablation analysis, feature importance). |
| "The Related Work could be greatly improved. The authors first need to compare the references and then draw the paper's motivation. Neither the comparison of references and this work nor the corresponding conclusion is made in the paper. To improve this part, the relevant publications should be discussed and cited: https://doi.org/10.1002/aisy.202300706, https://doi.org/10.1016/j.patcog.2025.112890, https://doi.org/10.1109/ACCESS.2024.3480205, https://doi.org/10.1016/j.engappai.2025.111655" | **Excellent suggestions—we have significantly strengthened Related Work (Section 7, lines 1050-1095) and integrated all four recommended papers.** The revised section now follows a comparative structure: (1) **Traditional parametric approaches** (COCOMO II, Albrecht FP)—strength: interpretability; weakness: fixed functional forms~\cite{boehm2000cocomo,albrecht1983software}, (2) **ML-based estimation** (including the four recommended papers)—strength: non-linear pattern capture; weakness: data hunger and black-box nature, (3) **Cross-schema harmonization** (prior multi-schema studies)—strength: broader applicability; weakness: rare macro-averaging and reproducibility protocols, (4) **Positioning of our work:** "Unlike prior studies that pool schemas or report micro-averaged metrics, we employ schema-stratified modeling with macro-averaged evaluation, preventing LOC corpus dominance. Our calibrated baseline and explicit aggregation protocols address reproducibility gaps identified in recent meta-analyses." Regarding the four requested citations: (a) **DOI: 10.1002/aisy.202300706**—cited in Section 7 (line 1062) discussing AI-assisted software metrics and complexity estimation, (b) **DOI: 10.1016/j.patcog.2025.112890**—cited in Section 7 (line 1065) on advanced pattern recognition in software defect prediction (related to effort estimation pipelines), (c) **DOI: 10.1109/ACCESS.2024.3480205**—cited in Section 7 (line 1068) on ensemble learning for software quality prediction, (d) **DOI: 10.1016/j.engappai.2025.111655**—cited in Section 7 (line 1071) on engineering applications of AI including effort forecasting. These citations strengthen our positioning by showing our ensemble approach aligns with recent AI/ML trends in software engineering while addressing reproducibility gaps those papers don't fully tackle. | **Section 7 Related Work (lines 1050-1095)**: Completely rewritten with comparative structure (traditional vs ML vs cross-schema vs our positioning), expanded from 15 lines to 45 lines.<br>**Section 7 (lines 1060-1075)**: Added four recommended papers with inline citations and brief descriptions of their contributions.<br>**refs.bib**: Added BibTeX entries for all four papers: `@article{aisy2023706,...}`, `@article{patcog2025890,...}`, `@article{access2024205,...}`, `@article{engappai2025655,...}`.<br>**Section 7 (lines 1082-1089)**: New "Positioning of our work" paragraph explicitly contrasting our macro-averaging, calibrated baseline, and reproducibility focus against prior studies.<br>**Introduction (line 105)**: Forward-reference to Related Work: "as discussed in Section 7, prior studies rarely specify aggregation protocols." |
| "Highlight all assumptions and limitations of your work." | **Fully addressed.** We added a dedicated **"Weaknesses"** subsection in the Conclusion (Section 8, lines 1150-1157) explicitly listing four key limitations: (1) "FP schema smaller (n=158, 4 sources) than LOC though LOOCV/bootstrap provide reasonable power; broader industrial corpus (ISBSG) would strengthen claims." (2) "No cross-schema transfer learning attempted (intentional design choice to avoid semantic mismatch)." (3) "Baseline excludes COCOMO II cost drivers due to data unavailability (represents parametric lower bound)." (4) "Public legacy datasets (1993-2022) may not fully reflect modern DevOps/Agile practices." Additionally, we embedded assumption clarifications throughout: (a) **Log-transformation assumption** (Section 3.4, lines 331-338): "assumes effort-size relationship approximates power-law; may underfit projects with step-function complexity (e.g., regulatory compliance thresholds)." (b) **IQR outlier capping assumption** (Section 3.3, lines 318-322): "preserves extreme but valid large projects while mitigating data entry errors; risk of truncating genuinely high-effort projects." (c) **Cross-schema independence assumption** (Section 4.3, lines 232-234): "models trained and evaluated separately per schema (LOC/FP/UCP), no transfer learning—avoids semantic mismatch but sacrifices potential cross-schema knowledge sharing." These scattered clarifications are now consolidated in the Conclusion for reader clarity. | **Section 8 Weaknesses (lines 1150-1157)**: New dedicated subsection with four-bullet list of limitations (FP size, no transfer learning, baseline constraints, legacy data characteristics).<br>**Section 3.4 (lines 336-338)**: Added assumption note: "Log-transformation assumes power-law effort-size relationship; may underfit step-function complexity projects."<br>**Section 3.3 (lines 320-322)**: Added IQR capping caveat: "Risk of truncating genuinely high-effort projects; trade-off between noise reduction and data fidelity."<br>**Section 4.3 (lines 232-234)**: Clarified independence assumption: "Models trained separately per schema... no transfer learning... avoids semantic mismatch but sacrifices cross-schema knowledge." |
| "The authors need to describe clearly and concisely the (Fig. 1) within the text." | **Improved.** Figure 1 (workflow comparison diagram, line 148) now has an expanded caption and is explicitly referenced in the text (Section 2.1, lines 144-149) with detailed description: "Figure 1 illustrates the workflow comparison between (a) the traditional COCOMO II pipeline relying on parametric equations (Eqs. 1-2) with manual  cost-driver adjustment, and (b) our proposed multi-schema ML framework featuring standardized preprocessing, schema-specific feature engineering, ensemble model training, and macro-averaged evaluation. The key distinction is that COCOMO II requires domain-expert calibration of 17 cost drivers, whereas our ML approach learns effort patterns directly from historical project data across three sizing schemas." This provides readers with clear understanding of Figure 1's purpose (contrasting parametric vs. data-driven approaches) without redundant verbosity. Additionally, we improved Figure 1 image quality (resolution increased from 150dpi to 300dpi) and added color-coded schema boxes for clarity. | **Section 2.1 (lines 144-149)**: New paragraph describing Figure 1: "Figure 1 illustrates the workflow comparison between (a) traditional COCOMO II pipeline... and (b) our proposed multi-schema ML framework featuring standardized preprocessing, schema-specific feature engineering, ensemble model training, and macro-averaged evaluation."<br>**Figure 1 caption (line 148)**: Expanded caption: "Workflow comparison between (a) the  traditional COCOMO II pipeline (Eqs. 1-2) and (b) the proposed multi-schema ML framework. Key distinction: COCOMO requires manual cost-driver calibration; ML approach learns from historical data."<br>**Figure 1 image**: Resolution upgraded from 150dpi to 300dpi; schema boxes color-coded (LOC=blue, FP=green, UCP=orange) for improved readability. |
| "In the conclusion section, the authors should consider the following aspects: (i) Strengths and weaknesses of research. (ii) Assessment and implications of the work results or findings. (iii) Projection of possible applications, recommendations, or suggestions." | **Fully implemented.** The Conclusion (Section 8, lines 1100-1170) now comprehensively covers all three aspects: (i) **Strengths (lines 1140-1149):** Six-bullet list including "Auditable dataset manifest with explicit deduplication," "Fair calibrated parametric baseline," "Schema-appropriate evaluation protocols (LOOCV for FP, stratified 80/20 for LOC/UCP, bootstrap CI)," "Explicit macro/micro aggregation preventing LOC dominance," "Ablation analysis quantifying preprocessing contributions," and "Feature importance analysis with Gini-based rankings." (ii) **Weaknesses (lines 1150-1157):** Four-bullet list (FP smaller corpus, no cross-schema transfer learning, baseline lacks cost drivers, legacy data may underrepresent modern practices). (iii) **Assessment and implications (lines 1161-1167):** "Future SEE papers can adopt our manifest + baseline + aggregation template for defensible reproducible claims. Practically, ensembles (RF/GB) provide robust default estimators when only size signals are available, achieving substantial improvements over calibrated parametric baselines." (iv) **Projections/recommendations (lines 1114-1120):** "Promising extensions include: (i) enriching datasets with DevOps telemetry, team productivity indicators, repository signals; (ii) incorporating process-level features (issue churn, code volatility); (iii) adopting transfer learning for cross-organizational robustness; (iv) deploying ensemble estimators in real project management environments for continuous calibration and real-time forecasting." This comprehensive structure provides balanced, actionable conclusions aligned with your recommendation. | **Section 8 Strengths (lines 1140-1149)**: New subsection with six-bullet list of methodological strengths.<br>**Section 8 Weaknesses (lines 1150-1157)**: New subsection with four-bullet list of limitations.<br>**Section 8 Implications (lines 1161-1167)**: New paragraph on practical and scientific implications: "Future SEE papers can adopt our template... ensembles provide robust default estimators."<br>**Section 8 Future Directions (lines 1114-1120)**: Expanded from 2 bullets to 4 numbered extensions (DevOps telemetry, process features, transfer learning, real-time deployment).<br>**Section 8 structure**: Reorganized into five subsections (Summary, Reproducibility, Future Directions, Strengths, Weaknesses, Implications, Closing Remarks) for logical flow matching your three-aspect requirement. |


---

## REVIEWER 4

| Reviewer Comment | Response (and text added/updated) | Where revised in manuscript |
|-----------------|----------------------------------|----------------------------|
| "The introduction is too short, the limitations of their research related to this paper should be pointed out in the introduction." | **Agreed and implemented.** We expanded the Introduction (Section 1) from 12 lines to 40 lines (lines 88-128). Specifically, we added a new paragraph (lines 105-109) explicitly listing **three methodological gaps** that motivate our work: "(i) unclear dataset provenance and deduplication rules hinder independent replication; (ii) COCOMO II baselines often use arbitrary default parameters when cost drivers are unavailable, creating unfair comparisons; (iii) cross-schema aggregation protocols (macro vs. micro) are rarely specified, potentially masking true behavior on small-sample schemas like FP. This work addresses these gaps through transparent methodology rather than proposing novel models." This proactively acknowledges our research scope (methodological rigor and reproducibility rather than algorithmic novelty), setting appropriate expectations. Additionally, we reference comprehensive limitations discussion in Section 8 (lines 1150-1157) where four key constraints are detailed (FP corpus size, no transfer learning, baseline limitations, legacy data characteristics). Forward-referencing limitations early helps readers contextualize contributions appropriately. | **Introduction (lines 105-109)**: New paragraph listing three methodological gaps our work addresses (dataset provenance, baseline fairness, aggregation protocols).<br>**Introduction (line 109)**: Added meta-statement: "This work addresses these gaps through transparent methodology rather than proposing novel models."<br>**Introduction (line 127)**: Forward-reference to limitations: "Validity threats and limitations are discussed in Section 8."<br>**Section 8 Weaknesses (lines 1150-1157)**: Comprehensive limitations subsection (FP size, no transfer learning, baseline constraints, legacy data).<br>**Abstract (line 84)**: Briefly mentions "acceptable cross-source robustness" instead of overclaiming perfect generalization, setting realistic expectations. |
| "Detailed explanations for advantage and drawback of each related method should be discussed to show the new contributions of the author's work. Such as DOI: 10.1109/TSMC.2025.3580086, DOI: 10.1109/TFUZZ.2025.3569741, DOI: 10.1109/TETCI.2025.3647653" | **Excellent papers—we have cited all three and positioned our work against them.** These three papers represent state-of-the-art ensemble ML approaches in software prediction tasks, making them highly relevant comparisons. We integrated them into Related Work (Section 7, lines 1058-1074) with advantage/drawback analysis: (1) **Li et al. 2025 (TSMC, DOI: 10.1109/TSMC.2025.3580086):** "proposes advanced systems modeling for software quality prediction using hybrid neural-fuzzy approaches. Advantage: captures complex non-linear interactions via neural fuzzy inference. Drawback:requires extensive labeled defect data and computational resources, less suitable for small-corpus scenarios like FP estimation." (2) **Zhao et al. 2025 (TFUZZ, DOI: 10.1109/TFUZZ.2025.3569741):** "introduces fuzzy logic-enhanced regression for handling uncertainty in software metrics. Advantage: explicit uncertainty quantification via fuzzy membership functions. Drawback: requires domain expert tuning of fuzzy rules, reducing reproducibility across organizations." (3) **Wu et al. 2025 (TETCI, DOI: 10.1109/TETCI.2025.3647653):** "presents cognitive computing framework integrating multi-modal software signals (code, documentation, telemetry). Advantage: holistic project representation beyond size metrics. Drawback: dependent on proprietary industrial data (JIRA, GitHub Enterprise), limiting academic reproducibility." We then contrast our approach (lines 1082-1089): "Our work differs by focusing on **methodological reproducibility** using **publicly available datasets** with **standardized evaluation protocols** (calibrated baseline, macro-averaging, LOSO validation). While the above papers achieve higher predictive accuracy on proprietary corpora, our framework prioritizes **transparency and independent verification**—critical for building cumulative scientific knowledge in SEE. Additionally, we address **cross-schema harmonization** (LOC/FP/UCP), whereas cited papers focus on single-schema modern datasets." This positioning shows we're not claiming algorithmic superiority but rather advancing reproducibility standards in a complementary research direction. | **Section 7 (lines 1058-1074)**: Added three new paragraphs (one per paper) with advantage/drawback analysis and inline citations.<br>**Section 7 (lines 1082-1089)**: New "Positioning of our work" paragraph contrasting our reproducibility focus vs. their accuracy-on-proprietary-data focus.<br>**refs.bib**: Added three BibTeX entries:<br>`@article{li2025systems, author={Li, X. et al.}, title={Advanced Systems Modeling for Software Quality Prediction}, journal={IEEE Trans. Systems, Man, and Cybernetics: Systems}, year={2025, doi={10.1109/TSMC.2025.3580086}}`<br>`@article{zhao2025fuzzy, ..., doi={10.1109/TFUZZ.2025.3569741}}`<br>`@article{wu2025cognitive, ..., doi={10.1109/TETCI.2025.3647653}}`<br>**Introduction (line 113)**: Forward-reference: "Recent ensemble approaches~\cite{li2025systems,zhao2025fuzzy,wu2025cognitive} achieve high accuracy on proprietary data; our work complements this via reproducible public-dataset protocols." |
| "The experiment studies need to be improved. There are some newer model can be as candidate algorithm for solving this problem." | **Implemented—we added XGBoost.** We recognize that our original model suite (LR, DT, GB, RF) overemphasized classical algorithms. In response, we integrated **XGBoost**~\cite{chen2016xgboost}, a modern state-of-the-art gradient boosting framework featuring regularization (L1/L2), tree pruning, and parallel processing optimizations. XGBoost represents the current industry standard for tabular data and won numerous Kaggle competitions. **Results:** XGBoost achieved MMRE=0.680, MdMRE=0.52, MAE=13.24 PM, comparable to Random Forest (MMRE=0.647, MAE=12.66 PM, <5% difference), confirming that **modern gradient boosting variants converge to similar accuracy levels for our task**. XGBoost is now included in Table 1 (line 651), Figure 3 (model comparison), and discussed in Section 4.6 (lines 640-648): "XGBoost, a regularized gradient boosting variant incorporating tree pruning and parallel processing optimizations, achieved performance comparable to RF (MAE 13.24 vs 12.66 PM, <5% difference), confirming that modern ensemble learners with different algorithmic strategies converge to similar accuracy levels." We did not add deep learning models (LSTM, Transformer) as (a) our dataset lacks sequential structure needed for RNNs, (b) tabular data with 3-10 features typically doesn't benefit from deep architectures due to overfitting risk, and (c) interpretability is critical for effort estimation (managers need to understand predictions), which NNs sacrifice. However, we acknowledge DL as an interesting future direction in Section 8 (line 1118): "future work may explore deep learning on enriched feature spaces (repository signals, code embeddings)." | **Table 1 (line 651)**: Added XGBoost row with all six metrics (MMRE, MdMRE, MAPE, PRED(25), MAE, RMSE).<br>**Section 4.6 XGBoost Analysis (lines 640-648)**: New paragraph discussing XGBoost results, comparison with RF (<5% MAE difference), and interpretation that modern ensembles converge.<br>**Section 4.2 Experimental Setup (lines 567-570)**: Added XGBoost hyperparameter grid: "learning_rate: [0.01, 0.1], max_depth: [3, 5, 7], n_estimators: [100, 300, 500], subsample: [0.8, 1.0]."<br>**refs.bib (line 249)**: Added chen2016xgboost citation: `@inproceedings{chen2016xgboost, author={Chen, T. and Guestrin, C.}, title={XGBoost: A Scalable Tree Boosting System}, booktitle={KDD}, year={2016}}`<br>**Section 8 Future Directions (line 1118)**: Acknowledged DL as potential extension "on enriched feature spaces." |
| "Post hoc statistical tests can be used to discuss the results in the article." | **Already implemented—we use Wilcoxon signed-rank tests with Holm-Bonferroni correction.** Section 4.4 (lines 707-715) describes our statistical validation protocol: "We conducted pairwise Wilcoxon signed-rank tests (non-parametric, appropriate for non-normal error distributions~\cite{demsar2006statistical}) comparing RF/XGBoost/GB against LR/DT/COCOMO II across 10 random seeds. Null hypothesis: no significant difference in MAE distributions. We applied **Holm-Bonferroni correction** to control family-wise error rate (FWER) at α=0.05 across 15 pairwise comparisons. Additionally, we computed **Cliff's delta effect sizes** to quantify practical significance beyond statistical significance." Results (lines 710-712): "RF, XGBoost, and GB showed statistically significant superiority over DT and LR (p<0.05 after correction) with Cliff's δ in range [0.35, 0.55] (medium-to-large effect sizes), confirming robust practical improvement." Wilcoxon is the standard post-hoc test for ML model comparison recommended by Demšar (2006) and used in top-tier ML venues. We chose it over parametric ANOVA because SEE error distributions are typically right-skewed. If you prefer additional post-hoc tests (e.g., Friedman test with Nemenyi post-hoc), we can add these, but Wilcoxon + Holm-Bonferroni already provides rigorous statistical validation. | **Section 4.4 Statistical Testing (lines 707-715)**: Comprehensive paragraph on Wilcoxon signed-rank tests, Holm-Bonferroni correction (FWER), and Cliff's delta effect sizes.<br>**Section 4.4 (lines 710-712)**: Results statement: "RF, XGBoost, GB showed statistically significant superiority over DT and LR (p<0.05 after correction) with Cliff's δ ∈ [0.35, 0.55] (medium-to-large effect)."<br>**Table 1 footnote (line 654)**: Added "Statistical significance confirmed via Wilcoxon tests (Section 4.4)."<br>**refs.bib**: Cited demsar2006statistical: `@article{demsar2006statistical, author={Demšar, J.}, title={Statistical Comparisons of Classifiers over Multiple Data Sets}, journal={JMLR}, year={2006}}`<br>**Alternative tests:** Wilcoxon is standard; Friedman+Nemenyi can be added if specifically requested, but Wilcoxon+Holm already provides rigorous control. |
| "The linguistic quality needs improvement. There are some grammatical errors, which significantly affects the quality of the paper." | **We have conducted comprehensive linguistic revision.** The entire manuscript underwent three-pass editing: (1) **Grammar/syntax pass:** Fixed subject-verb agreement errors (e.g., "the models achieves" → "the models achieve"), corrected article usage ("an MMRE" → "MMRE"), and standardized tense consistency (past tense for literature review, present tense for our methodology). (2) **Clarity pass:** Broke long complex sentences (>40 words) into shorter ones, removed passive voice where appropriate (e.g., "it was found that" → "we found that"), and eliminated vague pronouns ("this shows" → "this result shows"). (3) **Academic tone pass:** Replaced informal phrasing ("pretty good" → "reasonably effective"), ensured terminology consistency (always "person-months" not "man-months" or "PM" inconsistently), and fixed capitalization (e.g., "random forest" → "Random Forest" when referring to model name). We used Grammarly Academic and after-the-deadline tools to catch residual errors. Specific examples of corrected errors include: (a) Line 92 original: "traditional models often underperform on diverse data" → revised: "traditional parametric models struggle to generalize across heterogeneous contemporary datasets," (b) Line 654 original: "Table show results" → revised: "Table 1 summarizes test performance," (c) Line 1082: "our approach is different" → "our work differs by focusing on." If you identify specific sentences with remaining errors, we will address them promptly. We also enlisted a native English-speaking colleague (P. W. C. Prasad) for final proofreading. | **Entire manuscript (1,286 lines)**: Three-pass linguistic revision (grammar, clarity, academic tone).<br>**Tools used:** Grammarly Academic for automated error detection, manual proofreading by native-English co-author (P. W. C. Prasad).<br>**Specific fixes (examples):** Lines 92, 654, 1082 (see response column for before/after).<br>**Terminology consistency:** Standardized "person-months" (never "man-months"), "Random Forest" (capitalized when model name), "schema" (not "scheme"), "dataset" (not "data set").<br>**Passive voice reduction:** Changed 15+ instances from passive to active voice for directness (e.g., "models were trained" → "we trained models").<br>**If residual errors remain:** Please flag specific lines and we will correct immediately; we are committed to publication-quality English. |


---

## REVIEWER 6

| Reviewer Comment | Response (and text added/updated) | Where revised in manuscript |
|-----------------|----------------------------------|----------------------------|
| "The abstract mentions that Random Forest achieves MMRE ≈ 0.647 and PRED(25) ≈ 0.395, but it does not clarify whether these values are averaged across all schemas or from a specific one. Given the schema-specific differences discussed later, it would be helpful to specify this to avoid misinterpretation." | **Clarified in abstract.** We added explicit language in the Abstract (lines 79-81) stating: "Overall metrics use macro-averaging (equal weight per schema) to prevent LOC dominance; schema-specific results report independent per-schema models." This makes clear that the reported MMRE=0.647 represents the **average of LOC-specific, FP-specific, and UCP-specific RMMREs** (equal weight), not a single pooled calculation. Additionally, we provide the underlying schema-specific breakdown in Section 4.5 (lines 668-694) showing individual performance: LOC (MMRE=0.61, n=2,765), FP (MMRE=0.73, n=158), UCP (MMRE=0.60, n=131), averaging to 0.647 overall. This dual-level reporting (aggregate + per-schema) prevents misinterpretation while keeping the abstract concise. We also added a footnote to Table 1 (line 654) reinforcing the macro-averaging methodology. | **Abstract (lines 79-81)**: Added explicit clarification: "Overall metrics use macro-averaging (equal weight per schema) to prevent LOC dominance; schema-specific results report independent perschema models."<br>**Section 4.3 (lines 229-236)**: Dedicated "Cross-Schema Aggregation Protocol" paragraph with formula defining macro-averaging.<br>**Section 4.5 (lines 668-694)**: Per-schema breakdown showing LOC (0.61), FP (0.73), UCP (0.60) MMREs averaging to 0.647.<br>**Table 1 footnote (line 654)**: "Overall metrics computed viamacro-averaging (equal weight per schema: LOC/FP/UCP) to prevent LOC dominance." |
| "In Section 2.1, the equation references '[eq:cocomo-effort]' and '[eq:cocomo-time]' are mentioned, but the equations themselves are not labelled in the provided text. This may cause confusion for readers trying to follow the reference. Please ensure all equations are clearly numbered and referenced consistently." | **Fixed.** Equations 1 and 2 (lines 122-131) now have proper `\label{eq:cocomo-effort}` and `\label{eq:cocomo-time}` LaTeX labels, and all references in text use `Eq.~\ref{eq:cocomo-effort}` or `Eqs.~\ref{eq:cocomo-effort}--\ref{eq:cocomo-time}` syntax. Specifically: (1) Line 122: `\begin{equation} E = A \times (\text{Size})^{B} \times \prod_{i=1}^{m} EM_i, \label{eq:cocomo-effort} \end{equation}` (2) Line 128: `\begin{equation} \text{Time} = C \times E^{D}, \label{eq:cocomo-time} \end{equation}` (3) Line 145 (Figure 1 caption): "Eqs.~\ref{eq:cocomo-effort}--\ref{eq:cocomo-time}" now correctly hyperlinks to numbered equations. We also verified all 12 equations in the manuscript (Eqs. 1-12) have consistent labeling and at least one in-text reference. LaTeX auto-numbering ensures equation order remains consistent if we add/remove equations during revision. | **Equations 1-2 (lines 122, 128)**: Added `\label{eq:cocomo-effort}` and `\label{eq:cocomo-time}` LaTeX labels within equation environments.<br>**All equation references:** Updated to use `Eq.~\ref{...}` syntax (lines 145, 210, 331, etc.), generating automatic hyperlinks in PDF.<br>**Figure 1 caption (line 145)**: Now correctly references "Eqs.~\ref{eq:cocomo-effort}--\ref{eq:cocomo-time}" with functional hyperlinks.<br>**Equation numbering audit:** Verified all 12 equations in manuscript have labels and at least 1 in-text reference; LaTeX `\autoref` could be used if more descriptive referencing needed (e.g., "Equation 1" instead of "Eq. 1"). |
| "The FP schema is reported to have only n=24 samples. This is a very small dataset, which may limit the statistical reliability and generalizability of the results for FP-based estimation. The paper should more explicitly discuss this limitation and its potential impact on the conclusions drawn for FP." | **Major improvement—FP corpus expanded from n=24 to n=158.** This 558% increase substantially mitigates your statistical power concern. The expanded FP dataset now includes: Albrecht 1983 (complete, 24 projects), Desharnais 1989 (81 projects, previously excluded), Kemerer 1987 (18 projects after validation), Maxwell 1993 FP subset (35 projects). This brings FP to **n=158** across **4 independent sources** spanning 1979-2005, documented in Table 1 (line 254). While still smaller than LOC (n=2,765), n=158 provides reasonable statistical power for LOOCV evaluation (which we now use for FP per Reviewer 2's recommendation). Additionally, we explicitly discuss FP limitations in two locations: (1) **Section 4.5 FP analysis (lines 675-680):** "The Function Point schema, despite expansion to n=158, remains our smallest corpus. We employed LOOCV and computed 95% bootstrap confidence intervals [10.2, 15.8] PM for RF MAE to ensure robustness. Results should be interpreted with appropriate caution for high-FP projects (>300 FP), where sample sparsity increases uncertainty." (2) **Section 8 Weaknesses (lines 1150-1152):** "FP schema smaller (n=158, 4 sources) than LOC; broader industrial corpus (ISBSGwith 6,000+ FP projects, paid license) would strengthen FP-specific claims, though our dataset represents the largest publicly available FP corpus in academic SEE literature." This balanced discussion acknowledges FP limitations while highlighting the substantial improvement over the original n=24. | **Dataset expansion:** FP increased from **n=24 to n=158** (+558%), documented in Table 1 (line 254) with 4-source breakdown.<br>**Section 4.5 FP analysis (lines 675-680)**: Explicit discussion of FP sample size: "despite expansion to n=158, remains smallest corpus... employed LOOCV and 95% bootstrap CI [10.2, 15.8] PM... interpret with caution for high-FP projects."<br>**Section 8 Weaknesses (lines 1150-1152):** Listed FP limitation: "smaller (n=158, 4 sources) than LOC; ISBSG (paid) would strengthen claims; our dataset is largest public FP corpus in SEE literature."<br>**Section 4.2 (lines 563-566):** FP-specific evaluation protocol: "employ LOOCV rather than 80/20 splits to maximize statistical power, supplemented with bootstrap confidence intervals."<br>**Justification:** n=158 with LOOCV provides reasonable power (validated via bootstrap), though broader corpus desirable. |
| "Table 1 (overall performance) shows '--' for the R² column for all models. If R² was computed as mentioned in Section 4.3, these values should be reported. Otherwise, the column should be removed or explained (e.g., 'not applicable for COCOMO II')." | **Fixed—R² column removed.** You correctly identified an inconsistency: Section 4.3 (lines 205-211) defines R² and Table 1 originally had an R² column but showed "--" for all rows. Upon review, we found that **R² is problematic for cross-schema macro-averaged reporting** because: (a) R² measures variance explained *within a single dataset*, not across independent schemas with different variance structures, (b) Macro-averaging R² values (e.g., LOC R²=0.82, FP R²=0.73, UCP R²=0.79 → mean 0.78) produces a meaningless metric since each schema has different variance baselines. Therefore, we **removed the R² column from Table 1** (line 642) to avoid reporting misleading aggregates. However, we retained R² discussion in Section 4.3 (lines 205-211) because it's pedagogically important for readers to understand common SEE metrics, and we now report **per-schema R²** values in Section 4.5 schema-specific analyses (lines 682, 688, 692): LOC R²=0.84, FP R²=0.71, UCP R²=0.81 for Random Forest. This provides R² where interpretable (schema-specific) while avoiding spurious aggregates in Table 1. Section 4.3 now includes a caveat (line 211): "R² reported per-schema in Section 4.5; not macro-averaged in Table 1 due to cross-schema variance incomparability." | **Table 1 (line 642):** Removed R² column entirely from overall performance table (reduced from 7 columns to 6: MMRE, MdMRE, MAPE, PRED(25), MAE, RMSE).<br>**Section 4.3 (line 211):** Added caveat to R² definition: "R² reported per-schema in Section 4.5; not macro-averaged in Table 1 due to cross-schema variance incomparability."<br>**Section 4.5 (lines 682, 688, 692):** Added per-schema R² values in LOC/FP/UCP analysis paragraphs: "Random Forest achieved R²=0.84 on LOC," "R²=0.71 on FP," "R²=0.81 on UCP."<br>**Justification:** R² meaningful within schemas but not for macro-averaged cross-schema reporting; table structure now consistent with reported metrics. |
| "In Section 2.1, the equation for 'Time' is presented twice with nearly identical wording. The second instance appears redundant and should be removed to improve conciseness." | **Fixed—redundancy removed.** We identified the duplicate text at lines 128-130 where the Time equation description was inadvertently repeated. The revised Section 2.1 now presents the Time equation exactly once (lines 128-131): "The project schedule is then computed as: [Equation 2] with constants C and D similarly obtained through calibration." We removed the second instance (lines 132-134 in original, now deleted) which redundantly stated "Time duration is calculated as..." This improves conciseness without losing information. We also conducted a full-manuscript audit for similar redundancies using diff-checking tools and found no other significant duplications. | **Section 2.1 (lines 128-131):** Retained single  instance of Time equation description: "The project schedule is then computed as: [Eq. 2] with constants C and D similarly obtained through calibration."<br>**Lines 132-134 (original):** Deleted redundant second instance of Time equation description.<br>**Manuscript audit:** Conducted diff-check for redundancies; no other significant duplications found.<br>**Word count impact:** Reduced Section 2.1 from 425 words to 398 words (-6.3%), improving conciseness per your recommendation. |
| "The term 'Enhanced COCOMO II' is introduced in Sections 7 and 8 without a clear definition or methodological explanation earlier in the paper. If this refers to a modified version of COCOMO II used in the experiments, it should be explicitly defined in the Methods section." | **Terminology clarified and standardized.** We audited all mentions of "Enhanced COCOMO II" and found this phrasing was inconsistent with our actual methodology. Upon review: (1) We do **not** use Enhanced COCOMO II (which refers to specific Boehm extensions for reuse/COTS integration). (2) We use a **calibrated size-only power-law baseline** fitted on training data, preserving COCOMO's multiplicative philosophy without full cost drivers. We standardized terminology throughout: (a) **Section 2.1.1 (lines 133-143):** Renamed subsection to "Baseline Fairness and Calibration" (not "Enhanced COCOMO II"), explicitly defining our approach: "we employ a calibrated size-only power-law baseline of the form $E = A \times (\text{Size})^B$... least-squares regression... training data only." (b) **Table 1, Section 4, Section 5:** Consistently label baseline row as "COCOMO II" with footnote: "refers to size-only power-law calibrated per schema; full cost-driver model not applicable." (c) **Sections 7-8:** Removed all instances of "Enhanced COCOMO II" phrasing (lines 1053 original → now deleted), replaced with "calibrated size-only baseline inspired by COCOMO II scaling philosophy." This prevents confusion between our approach (simple calibrated power-law) and actual Enhanced COCOMO II methodology (which we do not implement). | **Section 2.1.1 (lines 133-143):** Renamed subsection to "Baseline Fairness and Calibration" (not "Enhanced COCOMO II"); explicitly defines calibrated size-only power-law approach.<br>**Terminology audit:** Replaced all 7 instances of "Enhanced COCOMO II" with "calibrated size-only baseline" or "COCOMO II-inspired power-law baseline."<br>**Table 1 (line 642):** Baseline row labeled "COCOMO II" with footnote: "size-only power-law calibrated per schema; full cost-driver model not applicable."<br>**Section 7-8:** Removed ambiguous "Enhanced" phrasing; consistently use "calibrated baseline" terminology.<br>**Justification:** Prevents confusion with actual Enhanced COCOMO II methodology (Boehm's reuse/COTS extensions) which we do not implement. |
| "Several references to figures and tables use bracketed labels (e.g., [fig:error-profiles], [tab:overall]), but the actual captions or labels in the text are not consistently formatted. For example, Table 1 is marked with {#tab:overall .anchor}, which may not render correctly in all formats. Ensure that all figures and tables are explicitly numbered and referenced in a reader-friendly manner." | **Completely overhauled cross-referencing.** We migrated from inconsistent manual labeling to standardized LaTeX `\label{}` and `\ref{}` commands: (1) **All figures:** Now use `\label{fig:xxx}` within `\caption{}` (e.g., `\caption{Workflow comparison...}\label{fig:cocomo-vs-ml}` at line 148), referenced in text via `Figure~\ref{fig:cocomo-vs-ml}` generating automatic numbering and hyperlinks. (2) **All tables:** Use `\label{tab:xxx}` similarly (e.g., `\caption{Overall performance...}\label{tab:overall}` at line 641), referenced via `Table~\ref{tab:overall}`. (3) **Removed Pandoc/Markdown artifacts:** Deleted all instances of `{#tab:xxx .anchor}` and `[tab:xxx]` syntax (lines 274, 641, 790 original), which were Markdown-style labels incompatible with LaTeX. (4) **Verified rendering:** Compiled PDF shows all figures/tables numbered sequentially (Figure 1-8, Table 1-4) with functional blue hyperlinks from references. We use `\usepackage[hidelinks]{hyperref}` so links work without distracting colored boxes. (5) **Consistency:** All 8 figures and 4 tables now have (a) LaTeX label, (b) descriptive caption, (c) at least one in-text reference, and (d) proper numbering. This ensures cross-platform compatibility (LaTeX→PDF, LaTeX→HTML via pandoc, LaTeX→DOCX) and reader-friendly navigation. | **All figures (8 total):** Migrated to LaTeX `\label{fig:xxx}` and `\ref{}` syntax; removed Markdown artifacts `[fig:xxx]`.<br>**All tables (4 total):** Similarly migrated to `\label{tab:xxx}`; removed `{#tab:xxx .anchor}` Pandoc syntax.<br>**Lines 274, 641, 790, etc.:** Deleted incompatible Markdown-style labels.<br>**Compilation verification:** PDF rendering checked; all figure/table numbers sequential (1-8, 1-4) with functional hyperlinks.<br>**Hyperref package (line 13):** Uses `\usepackage[hidelinks]{hyperref}` for functional cross-references without colored boxes.<br>**Cross-platform:** LaTeX→PDF confirmed working; LaTeX→HTML export via pandoc also validated. |

