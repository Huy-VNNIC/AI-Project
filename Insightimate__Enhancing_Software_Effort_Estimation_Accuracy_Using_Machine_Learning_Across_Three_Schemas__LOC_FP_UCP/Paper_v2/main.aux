\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{boehm2000cocomo}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Paper Organization.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}COCOMO\nobreakspace  {}II Recap}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:cocomo-effort}{{1}{3}{COCOMO~II Recap}{equation.2.1}{}}
\newlabel{eq:cocomo-time}{{2}{3}{COCOMO~II Recap}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Baseline Fairness and Calibration}{3}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Workflow comparison between (a) the traditional COCOMO\nobreakspace  {}II pipeline (Eqs.\nobreakspace  {}\ref  {eq:cocomo-effort}--\ref  {eq:cocomo-time}) and (b) the proposed multi-schema ML framework.\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cocomo-vs-ml}{{1}{3}{Workflow comparison between (a) the traditional COCOMO~II pipeline (Eqs.~\ref {eq:cocomo-effort}--\ref {eq:cocomo-time}) and (b) the proposed multi-schema ML framework.\relax }{figure.caption.2}{}}
\citation{tanveer2023survey,azzeh2019cross}
\citation{kitchenham2001evaluating,foss2003bias}
\citation{kitchenham2001evaluating}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Multi-Schema ML Framework}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Evaluation Metrics}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean Magnitude of Relative Error (MMRE).}{4}{section*.3}\protected@file@percent }
\newlabel{eq:mmre}{{4}{4}{Mean Magnitude of Relative Error (MMRE)}{equation.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Prediction at 25\% (PRED(25)).}{4}{section*.4}\protected@file@percent }
\newlabel{eq:pred25}{{5}{4}{Prediction at 25\% (PRED(25))}{equation.2.5}{}}
\citation{kitchenham2001evaluating}
\citation{boehm2000cocomo,albrecht1983software}
\@writefile{toc}{\contentsline {paragraph}{Mean Absolute Error (MAE).}{5}{section*.5}\protected@file@percent }
\newlabel{eq:mae}{{6}{5}{Mean Absolute Error (MAE)}{equation.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Root Mean Square Error (RMSE).}{5}{section*.6}\protected@file@percent }
\newlabel{eq:rmse}{{7}{5}{Root Mean Square Error (RMSE)}{equation.2.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Coefficient of Determination ($R^2$).}{5}{section*.7}\protected@file@percent }
\newlabel{eq:r2}{{8}{5}{Coefficient of Determination ($R^2$)}{equation.2.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Median Magnitude of Relative Error (MdMRE).}{5}{section*.8}\protected@file@percent }
\newlabel{eq:mdmre}{{9}{5}{Median Magnitude of Relative Error (MdMRE)}{equation.2.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Mean Absolute Percentage Error (MAPE).}{5}{section*.9}\protected@file@percent }
\newlabel{eq:mape}{{10}{5}{Mean Absolute Percentage Error (MAPE)}{equation.2.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Schema Aggregation Protocol.}{5}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dataset Imbalance Justification.}{6}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Confidence Intervals.}{6}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Datasets and Preprocessing}{6}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sources and Schema Partitioning}{6}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data sources and provenance.}{6}{section*.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset provenance and sample sizes after deduplication and expansion across multiple sources.\relax }}{6}{table.caption.14}\protected@file@percent }
\newlabel{tab:dataset-summary}{{1}{6}{Dataset provenance and sample sizes after deduplication and expansion across multiple sources.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Inclusion criteria.}{6}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exclusion and de-duplication.}{7}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Schema definitions.}{7}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unit Harmonization}{7}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Missing Values and Outliers}{7}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comprehensive reference of unit conversions used in the harmonization process. The table summarizes the standardized mappings between source units (LOC, FP, UCP, hours, days, staff-months, and weeks) and their unified target units (KLOC and Person-Months). These conversion factors ensure that heterogeneous datasets follow a consistent scale before being used for cross-source learning and model training.\relax }}{8}{figure.caption.18}\protected@file@percent }
\newlabel{fig:unit-harmonization}{{2}{8}{Comprehensive reference of unit conversions used in the harmonization process. The table summarizes the standardized mappings between source units (LOC, FP, UCP, hours, days, staff-months, and weeks) and their unified target units (KLOC and Person-Months). These conversion factors ensure that heterogeneous datasets follow a consistent scale before being used for cross-source learning and model training.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Handling missing values.}{8}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Outlier detection and capping.}{8}{section*.20}\protected@file@percent }
\newlabel{eq:iqr-clipping}{{11}{8}{Outlier detection and capping}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Scatter and boxplot visualizations showing (top) size–effort relationships before and after unit harmonization, and (bottom) productivity and team size trends across data sources. The harmonized representation eliminates scale discrepancies and improves interpretability across heterogeneous datasets.\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:harmonization-visuals}{{3}{8}{Scatter and boxplot visualizations showing (top) size–effort relationships before and after unit harmonization, and (bottom) productivity and team size trends across data sources. The harmonized representation eliminates scale discrepancies and improves interpretability across heterogeneous datasets.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Feature contribution matrices before and after harmonization via Principal Component Analysis (PCA). After harmonization, feature relationships become more stable and coherent, indicating better alignment of variance structures across datasets for model training.\relax }}{9}{figure.caption.22}\protected@file@percent }
\newlabel{fig:feature-contrib}{{4}{9}{Feature contribution matrices before and after harmonization via Principal Component Analysis (PCA). After harmonization, feature relationships become more stable and coherent, indicating better alignment of variance structures across datasets for model training.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation.}{9}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Distribution Shaping and Correlation}{9}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Size–effort correlation before and after log transformation. The log–log relationship highlights consistent scaling patterns across the LOC, FP, and UCP schemas, reinforcing the suitability of multiplicative models for software effort estimation.\relax }}{9}{figure.caption.24}\protected@file@percent }
\newlabel{fig:size-effort-corr}{{5}{9}{Size–effort correlation before and after log transformation. The log–log relationship highlights consistent scaling patterns across the LOC, FP, and UCP schemas, reinforcing the suitability of multiplicative models for software effort estimation.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Experimental design illustrating the train–test split (80/20) for each schema (LOC, FP, UCP). The power-law trend remains consistent between training and test sets, confirming that the sampling strategy preserves real-world effort–size dynamics.\relax }}{10}{figure.caption.25}\protected@file@percent }
\newlabel{fig:train-test-split}{{6}{10}{Experimental design illustrating the train–test split (80/20) for each schema (LOC, FP, UCP). The power-law trend remains consistent between training and test sets, confirming that the sampling strategy preserves real-world effort–size dynamics.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Hyperparameter optimization curves for Decision Tree and Random Forest models. The Decision Tree plot (left) identifies optimal depth balancing training and validation performance, while the Random Forest plot (right) shows cross-validation improvements with respect to the number of estimators and feature subset ratios.\relax }}{10}{figure.caption.26}\protected@file@percent }
\newlabel{fig:dt-rf-tuning}{{7}{10}{Hyperparameter optimization curves for Decision Tree and Random Forest models. The Decision Tree plot (left) identifies optimal depth balancing training and validation performance, while the Random Forest plot (right) shows cross-validation improvements with respect to the number of estimators and feature subset ratios.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{11}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Train–Test Protocol}{11}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces High-level experimental pipeline (per schema). Data are split into \textbf  {80\% Training} and \textbf  {20\% Test}; \textbf  {5-fold CV} is used for tuning inside training only. The best configuration is refit on full training, evaluated once on test, and results are averaged over \textbf  {10 random seeds}.\relax }}{11}{figure.caption.27}\protected@file@percent }
\newlabel{fig:exp-pipeline}{{8}{11}{High-level experimental pipeline (per schema). Data are split into \textbf {80\% Training} and \textbf {20\% Test}; \textbf {5-fold CV} is used for tuning inside training only. The best configuration is refit on full training, evaluated once on test, and results are averaged over \textbf {10 random seeds}.\relax }{figure.caption.27}{}}
\citation{wilcoxon1945individual}
\citation{breiman2001random}
\citation{holm1979simple}
\citation{macbeth2011cliffs}
\citation{demvsar2006statistical,garcia2010advanced}
\citation{pedregosa2011scikit}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Modeling Details}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Preprocessing.}{12}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Selection.}{12}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Regression (LR).}{12}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decision Tree (DT).}{12}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Random Forest (RF).}{12}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Boosting (GB).}{12}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation Metrics}{12}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Uncertainty \& Significance Testing}{12}{subsection.4.4}\protected@file@percent }
\citation{cruz2019open,lopez2021empirical}
\citation{chen2016xgboost}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Implementation \& Reproducibility}{13}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{13}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Overall Comparison}{13}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Overall test performance across LOC/FP/UCP schemas (macro-averaged, best in \textbf  {bold}).\relax }}{13}{table.caption.34}\protected@file@percent }
\newlabel{tab:overall}{{2}{13}{Overall test performance across LOC/FP/UCP schemas (macro-averaged, best in \textbf {bold}).\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Schema-Specific Analyses}{13}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LOC Schema.}{13}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FP Schema.}{14}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{UCP Schema.}{14}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cross-Schema Discussion.}{14}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Error Profiles and Visual Analyses}{14}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(a) Overall Performance.}{14}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(b) LOC Error Behavior.}{14}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(c) FP Effort Trends.}{14}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(d) Impact of Log and Outlier Control.}{14}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visual error analyses across schemas: (a) aggregate model performance; (b) LOC-based error patterns by project size; (c) FP-based effort trends; (d) effects of log transformation and IQR-based capping. Together, these results highlight the superior stability of ensemble estimators (RF, GB) across varying project scales and distributions.\relax }}{15}{figure.caption.43}\protected@file@percent }
\newlabel{fig:error-profiles}{{9}{15}{Visual error analyses across schemas: (a) aggregate model performance; (b) LOC-based error patterns by project size; (c) FP-based effort trends; (d) effects of log transformation and IQR-based capping. Together, these results highlight the superior stability of ensemble estimators (RF, GB) across varying project scales and distributions.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Leave-One-Source-Out Cross-Validation: Methodology Robustness}{15}{subsection.5.4}\protected@file@percent }
\newlabel{sec:loso}{{5.4}{15}{Leave-One-Source-Out Cross-Validation: Methodology Robustness}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Protocol.}{15}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results Summary.}{16}{section*.45}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Leave-One-Source-Out validation for LOC schema (Random Forest). Each row shows performance when the listed source is held out as test set and remaining 10 sources used for training. Demonstrates cross-source robustness.\relax }}{16}{table.caption.46}\protected@file@percent }
\newlabel{tab:loso-results}{{3}{16}{Leave-One-Source-Out validation for LOC schema (Random Forest). Each row shows performance when the listed source is held out as test set and remaining 10 sources used for training. Demonstrates cross-source robustness.\relax }{table.caption.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Implications.}{16}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion and Practical Implications}{16}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Random Forest Superiority.}{16}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alternative Model Preferences.}{17}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Guidelines for Adoption.}{17}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Insights and Validity.}{17}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Ablation Analysis}{17}{subsection.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation study on Random Forest (LOC schema, mean over 10 seeds).\relax }}{17}{table.caption.52}\protected@file@percent }
\newlabel{tab:ablation}{{4}{17}{Ablation study on Random Forest (LOC schema, mean over 10 seeds).\relax }{table.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Model Interpretability}{17}{subsection.6.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Feature importance (\%) from Random Forest across schemas (mean over 10 seeds).\relax }}{17}{table.caption.53}\protected@file@percent }
\newlabel{tab:feature-importance}{{5}{17}{Feature importance (\%) from Random Forest across schemas (mean over 10 seeds).\relax }{table.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Feature importance distributions across LOC, FP, and UCP schemas. Size metrics (KLOC/FP/UCP) consistently dominate effort prediction, with Time and Developers providing secondary modulation.\relax }}{18}{figure.caption.54}\protected@file@percent }
\newlabel{fig:feature-importance-schemas}{{10}{18}{Feature importance distributions across LOC, FP, and UCP schemas. Size metrics (KLOC/FP/UCP) consistently dominate effort prediction, with Time and Developers providing secondary modulation.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Imbalance Awareness}{18}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Threats to Validity}{18}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Internal Validity.}{18}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{External Validity.}{18}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Construct Validity.}{19}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion Validity.}{19}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary.}{19}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Detailed Limitations}{19}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Function Point Schema Limitations.}{19}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Calibrated Baseline Constraints.}{19}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Selection Scope.}{19}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cross-Schema Transfer Not Attempted.}{19}{section*.63}\protected@file@percent }
\citation{breiman2001random}
\citation{friedman2001greedy}
\citation{pandey2023comprehensive,alqadi2021deep}
\citation{kitchenham2001evaluating}
\citation{foss2003bias}
\citation{nair2020open,cruz2019open}
\@writefile{toc}{\contentsline {paragraph}{Modern DevOps Under representation.}{20}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{20}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Evolution of Software Effort Estimation Methods}{20}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Comparison of Estimation Paradigms}{20}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Validity Gaps in Prior Studies}{20}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Comparison with Prior Work}{20}{subsection.8.4}\protected@file@percent }
\citation{tanveer2023comprehensive,pandey2023comprehensive,alqadi2021deep}
\citation{cruz2019open,lopez2021empirical}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparison with representative SEE studies.\relax }}{21}{table.caption.65}\protected@file@percent }
\newlabel{tab:prior-comparison}{{6}{21}{Comparison with representative SEE studies.\relax }{table.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Research Gap and Contribution}{21}{subsection.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comprehensive overview of related research. (Top-left) Threats to validity across empirical studies; (Top-right) Comparison of estimation paradigms; (Bottom-left) Historical timeline of effort estimation methods; (Bottom-right) Research gap analysis linking empirical validation and industrial adoption.\relax }}{21}{figure.caption.66}\protected@file@percent }
\newlabel{fig:related-work}{{11}{21}{Comprehensive overview of related research. (Top-left) Threats to validity across empirical studies; (Top-right) Comparison of estimation paradigms; (Bottom-left) Historical timeline of effort estimation methods; (Bottom-right) Research gap analysis linking empirical validation and industrial adoption.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion and Reproducibility}{21}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Findings.}{21}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reproducibility Framework.}{21}{section*.68}\protected@file@percent }
\citation{yu2021transfer}
\@writefile{toc}{\contentsline {paragraph}{Future Directions.}{22}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Strengths.}{22}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weaknesses.}{22}{section*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implications.}{22}{section*.72}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closing Remarks.}{22}{section*.73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Visual summary of the proposed framework, including model performance comparison, reproducibility pipeline, and potential future extensions.\relax }}{23}{figure.caption.74}\protected@file@percent }
\newlabel{fig:conclusion}{{12}{23}{Visual summary of the proposed framework, including model performance comparison, reproducibility pipeline, and potential future extensions.\relax }{figure.caption.74}{}}
\bibstyle{unsrtnat}
\bibdata{refs}
\bibcite{boehm2000cocomo}{{1}{2000}{{Boehm}}{{}}}
\bibcite{tanveer2023survey}{{2}{2023{}}{{Tanveer et~al.}}{{Tanveer, Hussain, Zahid, et~al.}}}
\bibcite{azzeh2019cross}{{3}{2019}{{Azzeh and Nassif}}{{}}}
\bibcite{kitchenham2001evaluating}{{4}{2001}{{Kitchenham et~al.}}{{Kitchenham, Pickard, MacDonell, and Shepperd}}}
\bibcite{foss2003bias}{{5}{2003}{{Foss et~al.}}{{Foss, Stensrud, Kitchenham, and Myrtveit}}}
\bibcite{albrecht1983software}{{6}{1983}{{Albrecht and Gaffney}}{{}}}
\bibcite{wilcoxon1945individual}{{7}{1945}{{Wilcoxon}}{{}}}
\bibcite{breiman2001random}{{8}{2001}{{Breiman}}{{}}}
\bibcite{holm1979simple}{{9}{1979}{{Holm}}{{}}}
\bibcite{macbeth2011cliffs}{{10}{2011}{{Macbeth et~al.}}{{Macbeth, Razumiejczyk, and Ledesma}}}
\bibcite{demvsar2006statistical}{{11}{2006}{{Dem{\v {s}}ar}}{{}}}
\bibcite{garcia2010advanced}{{12}{2010}{{Garcia et~al.}}{{Garcia, Fernandez, Luengo, and Herrera}}}
\bibcite{pedregosa2011scikit}{{13}{2011}{{Pedregosa et~al.}}{{}}}
\bibcite{cruz2019open}{{14}{2019}{{Cruz and Abreu}}{{}}}
\bibcite{lopez2021empirical}{{15}{2021}{{Lopez et~al.}}{{Lopez, Rodriguez, and Garcia}}}
\bibcite{chen2016xgboost}{{16}{2016}{{Chen and Guestrin}}{{}}}
\bibcite{friedman2001greedy}{{17}{2001}{{Friedman}}{{}}}
\bibcite{pandey2023comprehensive}{{18}{2023}{{Pandey et~al.}}{{Pandey, Sharma, and Saha}}}
\bibcite{alqadi2021deep}{{19}{2021}{{Alqadi and Abran}}{{}}}
\bibcite{nair2020open}{{20}{2020}{{Nair and Menzies}}{{}}}
\bibcite{tanveer2023comprehensive}{{21}{2023{}}{{Tanveer et~al.}}{{Tanveer, Hussain, Zahid, et~al.}}}
\bibcite{yu2021transfer}{{22}{2021}{{Yu et~al.}}{{Yu, Xia, Lo, and Hassan}}}
\gdef \@abspage@last{25}
