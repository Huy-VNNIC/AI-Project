\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{boehm2000cocomo}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Research Gaps Addressed.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Calibrated Power-Law Baseline (COCOMO-like)}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:baseline}{{2.1}{2}{Calibrated Power-Law Baseline (COCOMO-like)}{subsection.2.1}{}}
\newlabel{eq:cocomo-full}{{1}{2}{Calibrated Power-Law Baseline (COCOMO-like)}{equation.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Fair Baseline Design.}{3}{section*.2}\protected@file@percent }
\newlabel{eq:baseline-calibrated}{{2}{3}{Fair Baseline Design}{equation.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Rationale.}{3}{section*.3}\protected@file@percent }
\newlabel{eq:cocomo-time}{{3}{3}{Rationale}{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Workflow comparison between (a) the traditional COCOMO\nobreakspace  {}II pipeline (Eqs.\nobreakspace  {}\ref  {eq:cocomo-effort}--\ref  {eq:cocomo-time}) and (b) the proposed multi-schema ML framework.\relax }}{3}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cocomo-vs-ml}{{1}{3}{Workflow comparison between (a) the traditional COCOMO~II pipeline (Eqs.~\ref {eq:cocomo-effort}--\ref {eq:cocomo-time}) and (b) the proposed multi-schema ML framework.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Multi-Schema ML Framework}{3}{subsection.2.2}\protected@file@percent }
\citation{tanveer2023survey,azzeh2019cross}
\citation{kitchenham2001evaluating,foss2003bias}
\citation{kitchenham2001evaluating}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Evaluation Metrics}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean Magnitude of Relative Error (MMRE) and Median MRE (MdMRE).}{4}{section*.5}\protected@file@percent }
\newlabel{eq:mmre-mdmre}{{5}{4}{Mean Magnitude of Relative Error (MMRE) and Median MRE (MdMRE)}{equation.2.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Mean Absolute Percentage Error (MAPE).}{4}{section*.6}\protected@file@percent }
\newlabel{eq:mape}{{6}{4}{Mean Absolute Percentage Error (MAPE)}{equation.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Prediction at 25\% (PRED(25)).}{4}{section*.7}\protected@file@percent }
\newlabel{eq:pred25}{{7}{4}{Prediction at 25\% (PRED(25))}{equation.2.7}{}}
\citation{kitchenham2001evaluating}
\citation{karner1993metrics}
\citation{jones2022estimation}
\citation{rodriguez2023dase}
\citation{huynh2023stacking,zenodo7022735}
\@writefile{toc}{\contentsline {paragraph}{Mean Absolute Error (MAE) and Median Absolute Error (MdAE).}{5}{section*.8}\protected@file@percent }
\newlabel{eq:mae-mdae}{{8}{5}{Mean Absolute Error (MAE) and Median Absolute Error (MdAE)}{equation.2.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Root Mean Square Error (RMSE).}{5}{section*.9}\protected@file@percent }
\newlabel{eq:rmse}{{9}{5}{Root Mean Square Error (RMSE)}{equation.2.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Coefficient of Determination ($R^2$).}{5}{section*.10}\protected@file@percent }
\newlabel{eq:r2}{{10}{5}{Coefficient of Determination ($R^2$)}{equation.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Datasets and Preprocessing}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sources and Schema Partitioning}{5}{subsection.3.1}\protected@file@percent }
\newlabel{sec:dataset-manifest}{{3.1}{5}{Sources and Schema Partitioning}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Data sources and provenance.}{5}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Repository cross-validation.}{5}{section*.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset Provenance Manifest (auditable). Counts reported after schema mapping and unit harmonization. Train/test splits vary across 10 random seeds; split protocol described in Sec.\nobreakspace  {}\ref  {sec:exp-setup}.\relax }}{6}{table.caption.11}\protected@file@percent }
\newlabel{tab:dataset-manifest}{{1}{6}{Dataset Provenance Manifest (auditable). Counts reported after schema mapping and unit harmonization. Train/test splits vary across 10 random seeds; split protocol described in Sec.~\ref {sec:exp-setup}.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Inclusion criteria.}{6}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exclusion and de-duplication.}{6}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Leakage control.}{6}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Schema definitions.}{6}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Unit Harmonization}{7}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comprehensive reference of unit conversions used in the harmonization process. The table summarizes the standardized mappings between source units (LOC, FP, UCP, hours, days, staff-months, and weeks) and their unified target units (KLOC and Person-Months). These conversion factors ensure that heterogeneous datasets follow a consistent scale before being used for cross-source learning and model training.\relax }}{7}{figure.caption.18}\protected@file@percent }
\newlabel{fig:unit-harmonization}{{2}{7}{Comprehensive reference of unit conversions used in the harmonization process. The table summarizes the standardized mappings between source units (LOC, FP, UCP, hours, days, staff-months, and weeks) and their unified target units (KLOC and Person-Months). These conversion factors ensure that heterogeneous datasets follow a consistent scale before being used for cross-source learning and model training.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Missing Values and Outliers}{8}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Handling missing values.}{8}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Outlier detection and capping.}{8}{section*.20}\protected@file@percent }
\newlabel{eq:iqr-clipping}{{11}{8}{Outlier detection and capping}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Scatter and boxplot visualizations showing (top) size–effort relationships before and after unit harmonization, and (bottom) productivity and team size trends across data sources. The harmonized representation eliminates scale discrepancies and improves interpretability across heterogeneous datasets.\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:harmonization-visuals}{{3}{8}{Scatter and boxplot visualizations showing (top) size–effort relationships before and after unit harmonization, and (bottom) productivity and team size trends across data sources. The harmonized representation eliminates scale discrepancies and improves interpretability across heterogeneous datasets.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Feature contribution matrices before and after harmonization via Principal Component Analysis (PCA). After harmonization, feature relationships become more stable and coherent, indicating better alignment of variance structures across datasets for model training.\relax }}{9}{figure.caption.22}\protected@file@percent }
\newlabel{fig:feature-contrib}{{4}{9}{Feature contribution matrices before and after harmonization via Principal Component Analysis (PCA). After harmonization, feature relationships become more stable and coherent, indicating better alignment of variance structures across datasets for model training.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation.}{9}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Distribution Shaping and Correlation}{9}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Size–effort correlation before and after log transformation. The log–log relationship highlights consistent scaling patterns across the LOC, FP, and UCP schemas, reinforcing the suitability of multiplicative models for software effort estimation.\relax }}{9}{figure.caption.24}\protected@file@percent }
\newlabel{fig:size-effort-corr}{{5}{9}{Size–effort correlation before and after log transformation. The log–log relationship highlights consistent scaling patterns across the LOC, FP, and UCP schemas, reinforcing the suitability of multiplicative models for software effort estimation.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Experimental design illustrating the train–test split (80/20) for each schema (LOC, FP, UCP). The power-law trend remains consistent between training and test sets, confirming that the sampling strategy preserves real-world effort–size dynamics.\relax }}{10}{figure.caption.25}\protected@file@percent }
\newlabel{fig:train-test-split}{{6}{10}{Experimental design illustrating the train–test split (80/20) for each schema (LOC, FP, UCP). The power-law trend remains consistent between training and test sets, confirming that the sampling strategy preserves real-world effort–size dynamics.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Hyperparameter optimization curves for Decision Tree and Random Forest models. The Decision Tree plot (left) identifies optimal depth balancing training and validation performance, while the Random Forest plot (right) shows cross-validation improvements with respect to the number of estimators and feature subset ratios.\relax }}{10}{figure.caption.26}\protected@file@percent }
\newlabel{fig:dt-rf-tuning}{{7}{10}{Hyperparameter optimization curves for Decision Tree and Random Forest models. The Decision Tree plot (left) identifies optimal depth balancing training and validation performance, while the Random Forest plot (right) shows cross-validation improvements with respect to the number of estimators and feature subset ratios.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{11}{section.4}\protected@file@percent }
\newlabel{sec:exp-setup}{{4}{11}{Experimental Setup}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Train–Test Protocol}{11}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General protocol (LOC, UCP schemas).}{11}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FP-specific protocol for small sample size.}{11}{section*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces High-level experimental pipeline (per schema). Data are split into \textbf  {80\% Training} and \textbf  {20\% Test}; \textbf  {5-fold CV} is used for tuning inside training only. The best configuration is refit on full training, evaluated once on test, and results are averaged over \textbf  {10 random seeds}.\relax }}{12}{figure.caption.29}\protected@file@percent }
\newlabel{fig:exp-pipeline}{{8}{12}{High-level experimental pipeline (per schema). Data are split into \textbf {80\% Training} and \textbf {20\% Test}; \textbf {5-fold CV} is used for tuning inside training only. The best configuration is refit on full training, evaluated once on test, and results are averaged over \textbf {10 random seeds}.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Modeling Details}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Preprocessing.}{12}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Selection.}{12}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Regression (LR).}{12}{section*.32}\protected@file@percent }
\citation{efron1994bootstrap}
\citation{wilcoxon1945individual}
\citation{breiman2001random}
\citation{holm1979simple}
\citation{macbeth2011cliffs}
\citation{demvsar2006statistical,garcia2010advanced}
\citation{pedregosa2011scikit}
\@writefile{toc}{\contentsline {paragraph}{Decision Tree (DT).}{13}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Random Forest (RF).}{13}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Boosting (GB).}{13}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation Metrics}{13}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bootstrap Confidence Intervals (Methodology).}{13}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Macro-Averaging Across Schemas.}{13}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Uncertainty \& Significance Testing}{13}{subsection.4.4}\protected@file@percent }
\citation{cruz2019open,lopez2021empirical}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Implementation \& Reproducibility}{14}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{14}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Aggregation Across Schemas}{14}{subsection.5.1}\protected@file@percent }
\newlabel{sec:aggregation}{{5.1}{14}{Aggregation Across Schemas}{subsection.5.1}{}}
\newlabel{eq:macro}{{12}{14}{Aggregation Across Schemas}{equation.5.12}{}}
\newlabel{eq:micro}{{13}{14}{Aggregation Across Schemas}{equation.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Overall Comparison}{14}{subsection.5.2}\protected@file@percent }
\citation{kitchenham2001evaluating}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Overall test performance (macro-averaged across schemas; best in \textbf  {bold}). Values show mean $\pm $ std across 10 random seeds.\relax }}{15}{table.caption.38}\protected@file@percent }
\newlabel{tab:overall}{{2}{15}{Overall test performance (macro-averaged across schemas; best in \textbf {bold}). Values show mean $\pm $ std across 10 random seeds.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Schema-Specific Analyses}{15}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LOC Schema.}{15}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FP Schema.}{15}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{UCP Schema.}{15}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cross-Schema Discussion.}{15}{section*.42}\protected@file@percent }
\citation{boehm2000software}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Error Profiles and Visual Analyses}{16}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(a) Overall Performance.}{16}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(b) LOC Error Behavior.}{16}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(c) FP Effort Trends.}{16}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(d) Impact of Log and Outlier Control.}{16}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Ablation Study: Impact of Preprocessing}{16}{subsection.5.5}\protected@file@percent }
\newlabel{sec:ablation}{{5.5}{16}{Ablation Study: Impact of Preprocessing}{subsection.5.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Methodology.}{16}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Observed Trends.}{16}{section*.49}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Visual error analyses across schemas: (a) aggregate model performance; (b) LOC-based error patterns by project size; (c) FP-based effort trends; (d) effects of log transformation and IQR-based capping. Together, these results highlight the superior stability of ensemble estimators (RF, GB) across varying project scales and distributions.\relax }}{17}{figure.caption.47}\protected@file@percent }
\newlabel{fig:error-profiles}{{9}{17}{Visual error analyses across schemas: (a) aggregate model performance; (b) LOC-based error patterns by project size; (c) FP-based effort trends; (d) effects of log transformation and IQR-based capping. Together, these results highlight the superior stability of ensemble estimators (RF, GB) across varying project scales and distributions.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {paragraph}{Cumulative Effect.}{17}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alternative Model Preferences.}{17}{section*.52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Ablation analysis visualizing MAE degradation (macro-averaged) when preprocessing components are progressively removed. Error bars show variability across 10 random seeds. Full quantitative results with per-schema breakdowns available in Supplementary Material S3.\relax }}{18}{figure.caption.51}\protected@file@percent }
\newlabel{fig:ablation}{{10}{18}{Ablation analysis visualizing MAE degradation (macro-averaged) when preprocessing components are progressively removed. Error bars show variability across 10 random seeds. Full quantitative results with per-schema breakdowns available in Supplementary Material S3.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {paragraph}{Guidelines for Adoption.}{18}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Insights and Validity.}{18}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Threats to Validity}{18}{section.6}\protected@file@percent }
\citation{breiman2001random}
\citation{friedman2001greedy}
\citation{pandey2023comprehensive,alqadi2021deep}
\@writefile{toc}{\contentsline {paragraph}{Internal Validity.}{19}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{External Validity.}{19}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Construct Validity.}{19}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion Validity.}{19}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary.}{19}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Related Work}{19}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Evolution of Software Effort Estimation Methods}{19}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Comparison of Estimation Paradigms}{19}{subsection.7.2}\protected@file@percent }
\citation{kitchenham2001evaluating}
\citation{foss2003bias}
\citation{nair2020open,cruz2019open}
\citation{tanveer2023comprehensive,pandey2023comprehensive,alqadi2021deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Validity Gaps in Prior Studies}{20}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Research Gap and Contribution}{20}{subsection.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comprehensive overview of related research. (Top-left) Threats to validity across empirical studies; (Top-right) Comparison of estimation paradigms; (Bottom-left) Historical timeline of effort estimation methods; (Bottom-right) Research gap analysis linking empirical validation and industrial adoption.\relax }}{20}{figure.caption.60}\protected@file@percent }
\newlabel{fig:related-work}{{11}{20}{Comprehensive overview of related research. (Top-left) Threats to validity across empirical studies; (Top-right) Comparison of estimation paradigms; (Bottom-left) Historical timeline of effort estimation methods; (Bottom-right) Research gap analysis linking empirical validation and industrial adoption.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion and Reproducibility}{20}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Findings.}{20}{section*.61}\protected@file@percent }
\citation{cruz2019open,lopez2021empirical}
\citation{yu2021transfer}
\@writefile{toc}{\contentsline {paragraph}{Reproducibility Framework.}{21}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future Directions.}{21}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closing Remarks.}{21}{section*.64}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Visual summary of the proposed framework, including model performance comparison, reproducibility pipeline, and potential future extensions.\relax }}{22}{figure.caption.65}\protected@file@percent }
\newlabel{fig:conclusion}{{12}{22}{Visual summary of the proposed framework, including model performance comparison, reproducibility pipeline, and potential future extensions.\relax }{figure.caption.65}{}}
\bibstyle{unsrtnat}
\bibdata{refs}
\bibcite{boehm2000cocomo}{{1}{2000}{{Boehm}}{{}}}
\bibcite{tanveer2023survey}{{2}{2023{}}{{Tanveer et~al.}}{{Tanveer, Hussain, Zahid, et~al.}}}
\bibcite{azzeh2019cross}{{3}{2019}{{Azzeh and Nassif}}{{}}}
\bibcite{kitchenham2001evaluating}{{4}{2001}{{Kitchenham et~al.}}{{Kitchenham, Pickard, MacDonell, and Shepperd}}}
\bibcite{foss2003bias}{{5}{2003}{{Foss et~al.}}{{Foss, Stensrud, Kitchenham, and Myrtveit}}}
\bibcite{karner1993metrics}{{6}{1993}{{Karner}}{{}}}
\bibcite{jones2022estimation}{{7}{2022}{{Jones}}{{}}}
\bibcite{rodriguez2023dase}{{8}{2023}{{Rodr{\'i}guez and Dolado}}{{}}}
\bibcite{huynh2023stacking}{{9}{2023}{{Huynh et~al.}}{{Huynh, Silhavy, Prokopova, and Silhavy}}}
\bibcite{zenodo7022735}{{10}{2023}{{Huynh and Silhavy}}{{}}}
\bibcite{efron1994bootstrap}{{11}{1994}{{Efron and Tibshirani}}{{}}}
\bibcite{wilcoxon1945individual}{{12}{1945}{{Wilcoxon}}{{}}}
\bibcite{breiman2001random}{{13}{2001}{{Breiman}}{{}}}
\bibcite{holm1979simple}{{14}{1979}{{Holm}}{{}}}
\bibcite{macbeth2011cliffs}{{15}{2011}{{Macbeth et~al.}}{{Macbeth, Razumiejczyk, and Ledesma}}}
\bibcite{demvsar2006statistical}{{16}{2006}{{Dem{\v {s}}ar}}{{}}}
\bibcite{garcia2010advanced}{{17}{2010}{{Garcia et~al.}}{{Garcia, Fernandez, Luengo, and Herrera}}}
\bibcite{pedregosa2011scikit}{{18}{2011}{{Pedregosa et~al.}}{{}}}
\bibcite{cruz2019open}{{19}{2019}{{Cruz and Abreu}}{{}}}
\bibcite{lopez2021empirical}{{20}{2021}{{Lopez et~al.}}{{Lopez, Rodriguez, and Garcia}}}
\bibcite{friedman2001greedy}{{21}{2001}{{Friedman}}{{}}}
\bibcite{pandey2023comprehensive}{{22}{2023}{{Pandey et~al.}}{{Pandey, Sharma, and Saha}}}
\bibcite{alqadi2021deep}{{23}{2021}{{Alqadi and Abran}}{{}}}
\bibcite{nair2020open}{{24}{2020}{{Nair and Menzies}}{{}}}
\bibcite{tanveer2023comprehensive}{{25}{2023{}}{{Tanveer et~al.}}{{Tanveer, Hussain, Zahid, et~al.}}}
\bibcite{yu2021transfer}{{26}{2021}{{Yu et~al.}}{{Yu, Xia, Lo, and Hassan}}}
\gdef \@abspage@last{25}
