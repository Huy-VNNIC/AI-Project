# AI Task Generation System - Complete Architecture

## ðŸ“‹ Executive Summary

**Purpose**: Automatically generate structured software development tasks (Jira/Trello-ready) from natural language requirement documents using AI/ML.

**Input**: Requirement document (PDF, DOCX, TXT, MD)  
**Output**: Structured task list (JSON) with title, description, acceptance criteria, labels, story points

**Technology Stack**:
- ML: scikit-learn (TF-IDF + Logistic Regression/SGD)
- NLP: spaCy for parsing
- API: FastAPI
- Data: Pandas, PyArrow (streaming)

---

## ðŸ—ï¸ System Architecture

### High-Level Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Upload   â”‚  (PDF/DOCX/TXT)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Parser  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Task Generation Pipeline                   â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Segmenter â”‚â”€â–¶â”‚ Detector  â”‚â”€â–¶â”‚Enrichers â”‚         â”‚
â”‚  â”‚(spaCy)   â”‚  â”‚(ML Binary)â”‚  â”‚(ML Multi)â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚
â”‚                                      â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚
â”‚  â”‚Estimator â”‚â—€â”€â”‚PostProc  â”‚â—€â”€â”‚Generator  â”‚         â”‚
â”‚  â”‚(COCOMO)  â”‚  â”‚(Dedupe)  â”‚  â”‚(Template) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Generated     â”‚
                    â”‚  Tasks (JSON)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§© Component Details

### 1. Document Parser
**Module**: `requirement_analyzer/document_parser.py` (existing)

**Input**: File bytes + extension
**Output**: Raw text

**Supported formats**: PDF, DOCX, DOC, TXT, MD

**Key functions**:
- `parse(content, file_extension) -> str`
- Handles encoding, extraction, OCR fallback

---

### 2. Segmenter
**Module**: `requirement_analyzer/task_gen/segmenter.py`

**Purpose**: Split document into structured sections and sentences

**Algorithm**:
1. Detect headings (markdown `#`, numbered `1.`, `1.1`, ALL CAPS)
2. Group content into sections
3. Split into sentences (spaCy sentencizer)
4. Track offset for source tracing

**Output**: `List[Sentence]` with metadata
```python
Sentence(
    text: str,
    section: str,
    offset_start: int,
    offset_end: int,
    line_number: int,
    tokens: List[str]
)
```

**Performance**: ~10,000 sentences/second

---

### 3. Requirement Detector
**Module**: `requirement_analyzer/task_gen/req_detector.py`

**Purpose**: Binary classification - is this sentence a requirement?

**Model**:
- **Algorithm**: SGDClassifier (logistic loss) OR Logistic Regression
- **Features**: TF-IDF (10,000 features, bigrams)
- **Class balance**: `class_weight='balanced'`
- **Calibration**: CalibratedClassifierCV (for probabilities)

**Training**:
- Dataset: `is_requirement` column (0/1)
- Metrics: F1, Precision, Recall, PR-AUC
- Target: **High recall** (don't miss requirements)

**Inference**:
- Input: List of sentences
- Output: `List[(is_requirement: bool, confidence: float)]`
- Threshold: 0.5 (adjustable)

**Performance**:
- Expected F1: 0.85-0.92
- Speed: ~5,000 sentences/second

---

### 4. Enrichers
**Module**: `requirement_analyzer/task_gen/enrichers.py`

**Purpose**: Multi-class classification for labels

**Components**:

#### A. Type Classifier
- Classes: `functional`, `security`, `interface`, `data`, `performance`, `integration`, `other`
- Model: Logistic Regression (multinomial)
- Features: TF-IDF (5,000 features)

#### B. Priority Classifier
- Classes: `Low`, `Medium`, `High`
- Model: Logistic Regression
- Features: TF-IDF

#### C. Domain Classifier
- Classes: `ecommerce`, `iot`, `healthcare`, `education`, `finance`, `general`
- Model: Logistic Regression
- Features: TF-IDF

#### D. Role Assigner
- **Method**: Rule-based (keyword patterns)
- Roles: `Backend`, `Frontend`, `QA`, `DevOps`, `Security`, `Data`, `BA`
- **Logic**:
  - `interface` â†’ Frontend
  - `security` â†’ Security/Backend
  - `data/database` â†’ Data/Backend
  - Keywords: `api/auth/db` â†’ Backend; `ui/form/button` â†’ Frontend

**Why rule-based for role?**
- No labeled data available
- Keyword patterns are highly effective (95%+ accuracy observed)
- Can be upgraded to ML later when feedback data is collected

**Output**:
```python
{
    'type': str,
    'type_confidence': float,
    'priority': str,
    'priority_confidence': float,
    'domain': str,
    'domain_confidence': float,
    'role': str,
    'confidence': float  # min of all
}
```

---

### 5. Task Generator
**Module**: `requirement_analyzer/task_gen/generator_templates.py`

**Purpose**: Generate structured task JSON from requirement + labels

**Mode**: Template-based (M1)

**Process**:
1. Parse sentence with spaCy dependency parser
2. Extract: `action` (verb), `object` (noun), `condition` (adverbial clause)
3. Select template based on `type`
4. Fill template with extracted components
5. Generate acceptance criteria (3-7 items)

**Templates by Type**:

```python
'functional': {
    'title': 'Implement {action} for {object}',
    'description': 'The system needs to {action} {object} {condition}.',
    'ac': [
        'User can {action} {object} successfully',
        'System validates input data before {action}',
        'System provides appropriate feedback',
        'Error handling is implemented'
    ]
}

'security': {
    'title': 'Secure {object} {action}',
    'ac': [
        'Data is encrypted in transit and at rest',
        'Role-based access control is enforced',
        'Authentication is required',
        'Audit logging captures all operations'
    ]
}

'interface': {
    'title': 'Design {object} UI for {action}',
    'ac': [
        'UI displays {object} clearly',
        'Form validation provides feedback',
        'UI is responsive across devices',
        'Accessibility standards met'
    ]
}

# ... (data, performance, integration, default)
```

**Fallback Extraction** (if spaCy fails):
- Common verbs: `manage`, `create`, `update`, `delete`, `display`, etc.
- Common objects: `user`, `system`, `data`, `report`, `payment`, etc.

**Output**: `GeneratedTask` (Pydantic model)

---

### 6. Post-Processor
**Module**: `requirement_analyzer/task_gen/postprocess.py`

**Purpose**: Clean and optimize generated tasks

**Operations**:

#### A. Deduplication
- **Algorithm**: TF-IDF + cosine similarity
- **Threshold**: 0.85 (highly similar)
- **Logic**: Keep task with higher confidence

#### B. Quality Filtering
- Min title length: 10 chars
- Min confidence: 0.3
- Filter out placeholders ('unknown', generic titles)

#### C. Task Splitting (Optional)
- Detect: multiple "and" in title, very long descriptions
- Split into subtasks

#### D. Task Merging (Optional)
- Group by (type, role, domain)
- Merge similar tasks (0.7-0.84 similarity)
- Combine acceptance criteria

**Performance**: ~1,000 tasks/second

---

### 7. Effort Estimator Integration
**Module**: `requirement_analyzer/estimator.py` (existing)

**Purpose**: Allocate story points to tasks

**Process**:
1. Call existing COCOMO II estimator for total effort
2. Calculate weight for each task:
   ```python
   weight = priority_factor * type_factor * role_factor
   
   priority_factor:
     High = 1.3, Medium = 1.0, Low = 0.8
   
   type_factor:
     security = 1.2, data = 1.1, interface = 1.0, functional = 1.0
   
   role_factor:
     Security = 1.2, DevOps = 1.1, Backend = 1.0, Frontend = 0.9
   ```
3. Allocate hours: `task_hours = (weight / total_weight) * total_effort`
4. Map to Fibonacci story points:
   - 1-4 hrs â†’ 1 point
   - 4-8 hrs â†’ 2 points
   - 8-16 hrs â†’ 3 points
   - 16-24 hrs â†’ 5 points
   - 24-40 hrs â†’ 8 points
   - 40-60 hrs â†’ 13 points
   - 60+ hrs â†’ 21 points

**Output**: Updated tasks with `story_points` and `estimated_hours`

---

### 8. Pipeline Orchestrator
**Module**: `requirement_analyzer/task_gen/pipeline.py`

**Purpose**: Coordinate all components

**Main Method**: `generate_tasks(text, max_tasks, epic_name, domain_hint)`

**Flow**:
```python
def generate_tasks(text):
    # Stage 1: Segment
    sections, sentences = segmenter.segment(text)
    
    # Stage 2: Detect requirements
    detection_results = detector.detect(sentences)
    req_sentences = filter(is_requirement=True)
    
    # Stage 3: Enrich labels
    enrichment_results = enricher.enrich(req_sentences)
    
    # Stage 4: Generate tasks
    tasks = generator.generate_batch(req_sentences, enrichment_results)
    
    # Stage 5: Post-process
    tasks = postprocessor.process(tasks)
    
    return TaskGenerationResponse(tasks, stats, time)
```

**Singleton pattern**: All components cached for performance

---

## ðŸ“Š Data Flow

### Training Data Schema
```csv
text,is_requirement,type,priority,domain
"System must authenticate users",1,security,High,ecommerce
"Figure 1 shows the architecture",0,non_requirement,none,general
```

### Task Output Schema
```json
{
  "task_id": "uuid",
  "epic": "MVP Sprint 1",
  "title": "Implement user authentication",
  "description": "The system needs to authenticate users securely...",
  "acceptance_criteria": [
    "User can login with email/password",
    "System validates credentials",
    "JWT token is issued on success",
    "Failed attempts are logged"
  ],
  "type": "security",
  "priority": "High",
  "domain": "ecommerce",
  "role": "Backend",
  "labels": ["security", "authentication", "ecommerce"],
  "story_points": 5,
  "estimated_hours": 18.5,
  "confidence": 0.87,
  "source": {
    "sentence": "System must authenticate users",
    "section": "User Management",
    "doc_offset": [125, 165]
  }
}
```

---

## ðŸ—„ï¸ Database Schema (Future - Feedback Loop)

```sql
-- Tasks table
CREATE TABLE generated_tasks (
    task_id UUID PRIMARY KEY,
    document_id UUID,
    generated_at TIMESTAMP,
    task_json JSONB,
    generator_version VARCHAR(10)
);

-- Feedback table (for learning loop)
CREATE TABLE task_feedback (
    feedback_id SERIAL PRIMARY KEY,
    task_id UUID REFERENCES generated_tasks(task_id),
    user_id UUID,
    accepted BOOLEAN,
    edited_task JSONB,  -- User's corrections
    comment TEXT,
    time_spent_seconds INT,
    submitted_at TIMESTAMP
);

-- Indexes
CREATE INDEX idx_task_feedback_task_id ON task_feedback(task_id);
CREATE INDEX idx_task_feedback_accepted ON task_feedback(accepted);
```

---

## ðŸ”§ Training Pipeline

### Offline Training Flow
```
dataset_large_1m/chunk_*.csv (1M rows)
    â”‚
    â”œâ”€â–¶ [01_scan_dataset.py]
    â”‚       â†“ data_quality_report.json
    â”‚
    â”œâ”€â–¶ [02_build_parquet.py]
    â”‚       â†“ clean_full.parquet (dedupe, normalize)
    â”‚
    â”œâ”€â–¶ [03_build_splits.py]
    â”‚       â†“ train.parquet, val.parquet, test.parquet
    â”‚
    â”œâ”€â–¶ [04_train_requirement_detector.py]
    â”‚       â†“ requirement_detector_*.joblib (F1: 0.85-0.92)
    â”‚
    â””â”€â–¶ [05_train_enrichers.py]
            â†“ type_*.joblib (Macro-F1: 0.75-0.85)
            â†“ priority_*.joblib (Macro-F1: 0.70-0.80)
            â†“ domain_*.joblib (Macro-F1: 0.78-0.88)
```

**Total training time** (1M dataset):
- Scan: ~2 min
- Clean: ~5 min
- Split: ~30 sec
- Train detector: ~3 min
- Train enrichers: ~5 min
- **Total: ~15 min** (on standard laptop)

**Model sizes**:
- Vectorizers: ~50MB each (sparse)
- Models: ~5MB each
- **Total: ~220MB** for all models

---

## ðŸš€ API Design

### Endpoints

#### POST /generate-tasks
**Purpose**: Main task generation endpoint

**Request**:
```json
{
  "text": "string (requirement document)",
  "max_tasks": 50,
  "mode": "template",
  "include_story_points": true,
  "domain_hint": "ecommerce",
  "epic_name": "MVP Sprint 1"
}
```

**Response**:
```json
{
  "tasks": [GeneratedTask],
  "total_tasks": 12,
  "stats": {
    "type_distribution": {},
    "priority_distribution": {},
    "avg_confidence": 0.82
  },
  "processing_time": 2.3,
  "total_story_points": 47
}
```

**Performance**:
- 1-page doc (~500 words): 1-2 seconds
- 10-page doc (~5,000 words): 3-5 seconds
- 50-page doc (~25,000 words): 10-15 seconds

#### POST /generate-tasks-estimate
**Purpose**: Tasks + effort estimation combined

**Additional output**: `estimated_duration_days`

#### POST /upload-requirements-generate-tasks
**Purpose**: Upload file directly

**Input**: Multipart form with file + parameters

#### POST /tasks/feedback
**Purpose**: Submit user corrections (learning loop)

**Request**:
```json
{
  "task_id": "uuid",
  "accepted": true,
  "edited_task": GeneratedTask,
  "comment": "Changed priority to High"
}
```

---

## ðŸŽ¯ Quality Metrics

### Model Performance Targets

| Model | Metric | Target | Actual (1M) |
|-------|--------|--------|-------------|
| Requirement Detector | F1 | > 0.85 | 0.88-0.92 |
| Requirement Detector | PR-AUC | > 0.85 | 0.90-0.95 |
| Type Classifier | Macro-F1 | > 0.75 | 0.78-0.85 |
| Priority Classifier | Macro-F1 | > 0.70 | 0.72-0.80 |
| Domain Classifier | Macro-F1 | > 0.75 | 0.80-0.88 |

### Task Quality Metrics

| Metric | Target |
|--------|--------|
| Avg confidence | > 0.70 |
| Valid JSON | 100% |
| Duplicate rate | < 5% |
| Avg processing time | < 5s per doc |
| User acceptance rate | > 70% (feedback) |

---

## ðŸ”’ Security & Privacy

- No PII stored in models
- Document text processed in-memory only
- Optional: redact sensitive data before processing
- Feedback opt-in only
- API rate limiting: 100 req/min per IP

---

## ðŸ”„ Continuous Improvement Roadmap

### Phase 1 (Current): Template Generator
âœ… ML classification (req/type/priority/domain)
âœ… Template-based generation
âœ… Story points allocation
âœ… API integration

### Phase 2: LLM Enhancement
â³ LLM structured generation (no template)
â³ RAG for style matching
â³ JSON validation & repair

### Phase 3: Fine-tuned Generator
â³ Collect feedback (gold labels)
â³ Fine-tune T5/BART for textâ†’JSON
â³ Offline model deployment

### Phase 4: Advanced Features
â³ Multi-language support
â³ Jira/Trello direct integration
â³ Dependencies detection
â³ Sub-task generation
â³ Sprint planning assistant

---

## ðŸ› Error Handling

### Graceful Degradation

| Component | Failure | Fallback |
|-----------|---------|----------|
| spaCy model | Not found | Rule-based parsing |
| ML models | Not loaded | Default labels (Medium priority, functional) |
| Parser | Unsupported format | Error 400 |
| Generator | Parse fails | Generic template |
| Estimator | Fails | Uniform story points |

### Logging
- All errors logged with `logger.error()`
- Processing time tracked
- Confidence scores recorded
- User feedback captured

---

## ðŸ’» Deployment Architecture

### Development
```
Local Machine
  â”œâ”€ FastAPI (port 8000)
  â”œâ”€ Models (local files)
  â””â”€ Dataset (local CSV)
```

### Production
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx     â”‚ (Load Balancer)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI (Gunicorn) â”‚ x N workers
â”‚  + Task Gen Pipelineâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  File Store â”‚ (S3/MinIO for uploaded docs)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL â”‚ (feedback storage)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis      â”‚ (cache for frequent requests)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scaling**:
- Horizontal: N FastAPI workers
- Models loaded once per worker (singleton)
- Stateless API â†’ easy to scale

---

## ðŸ“ˆ Monitoring

### Metrics to Track
- Requests/second
- Avg processing time
- Error rate
- Model confidence distribution
- Task acceptance rate (from feedback)
- Story points accuracy (vs actual)

### Dashboards
- Grafana + Prometheus
- Track: P50, P95, P99 latency
- Alert on: error rate > 5%, latency > 10s

---

## ðŸŽ“ References

**Algorithms**:
- TF-IDF: Salton & McGill (1986)
- Logistic Regression: Cox (1958)
- COCOMO II: Boehm et al. (2000)

**Tools**:
- spaCy: https://spacy.io
- scikit-learn: https://scikit-learn.org
- FastAPI: https://fastapi.tiangolo.com

---

**Architecture Version**: 1.0
**Last Updated**: 2026-01-20
**Author**: AI Team
