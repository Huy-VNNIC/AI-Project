#!/usr/bin/env python3
"""
Clean, deduplicate, and merge data from Jira/Trello
Output: tasks_corpus.jsonl (ready for training)
"""
import os
import sys
import json
import hashlib
import re
from pathlib import Path
from typing import List, Dict
from collections import Counter

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))


class DataCleaner:
    """Clean and normalize task data"""
    
    def __init__(self, min_title_length: int = 5, min_desc_length: int = 20):
        self.min_title_length = min_title_length
        self.min_desc_length = min_desc_length
        self.seen_hashes = set()
    
    def text_hash(self, text: str) -> str:
        """Generate hash for deduplication"""
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def clean_text(self, text: str) -> str:
        """Clean text field"""
        if not text:
            return ""
        
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove URLs (optional - might want to keep)
        # text = re.sub(r'http[s]?://\S+', '', text)
        
        # Remove email addresses (PII)
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
        
        # Remove phone numbers (simple pattern)
        text = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE]', text)
        
        # Remove excessive punctuation
        text = re.sub(r'([!?.]){3,}', r'\1\1', text)
        
        return text.strip()
    
    def is_valid(self, task: Dict) -> bool:
        """Check if task meets quality criteria"""
        title = task.get('title', '')
        description = task.get('description', '')
        
        # Minimum length checks
        if len(title) < self.min_title_length:
            return False
        
        # Too short description (but allow empty if has AC)
        if len(description) < self.min_desc_length:
            if not task.get('acceptance_criteria'):
                return False
        
        # Check for bot/auto-generated patterns
        bot_patterns = [
            r'automated message',
            r'this issue was automatically',
            r'bot created',
            r'generated by'
        ]
        
        combined = (title + ' ' + description).lower()
        for pattern in bot_patterns:
            if re.search(pattern, combined):
                return False
        
        return True
    
    def is_duplicate(self, task: Dict) -> bool:
        """Check if task is duplicate"""
        # Hash title + description
        text = task['title'] + ' ' + task.get('description', '')
        task_hash = self.text_hash(text)
        
        if task_hash in self.seen_hashes:
            return True
        
        self.seen_hashes.add(task_hash)
        return False
    
    def clean_task(self, task: Dict) -> Dict:
        """Clean a single task"""
        cleaned = {
            'source': task.get('source', ''),
            'source_id': task.get('source_id', ''),
            'source_url': task.get('source_url', ''),
            'title': self.clean_text(task.get('title', '')),
            'description': self.clean_text(task.get('description', '')),
            'acceptance_criteria': [
                self.clean_text(ac) for ac in task.get('acceptance_criteria', [])
                if self.clean_text(ac)
            ],
            'labels': task.get('labels', []),
            'components': task.get('components', []),
            'issue_type': task.get('issue_type', ''),
            'priority': task.get('priority', ''),
            'status': task.get('status', ''),
            'created_at': task.get('created_at', ''),
            'updated_at': task.get('updated_at', '')
        }
        
        return cleaned
    
    def process_files(self, input_files: List[Path], output_file: Path):
        """Process multiple JSONL files"""
        print(f"\n{'='*70}")
        print(f"  DATA CLEANER & MERGER")
        print(f"{'='*70}\n")
        
        all_tasks = []
        stats = {
            'total_raw': 0,
            'invalid': 0,
            'duplicates': 0,
            'cleaned': 0
        }
        
        # Load all files
        for input_file in input_files:
            if not input_file.exists():
                print(f"‚ö†Ô∏è  File not found: {input_file}")
                continue
            
            print(f"üìÇ Processing: {input_file.name}")
            
            with open(input_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        task = json.loads(line)
                        stats['total_raw'] += 1
                        
                        # Validate
                        if not self.is_valid(task):
                            stats['invalid'] += 1
                            continue
                        
                        # Check duplicate
                        if self.is_duplicate(task):
                            stats['duplicates'] += 1
                            continue
                        
                        # Clean
                        cleaned = self.clean_task(task)
                        all_tasks.append(cleaned)
                        stats['cleaned'] += 1
                        
                    except json.JSONDecodeError:
                        print(f"   ‚ö†Ô∏è  Line {line_num}: Invalid JSON")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Line {line_num}: {e}")
        
        if not all_tasks:
            print("\n‚ùå No valid tasks after cleaning")
            return
        
        # Save
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for task in all_tasks:
                f.write(json.dumps(task, ensure_ascii=False) + '\n')
        
        print(f"\n‚úÖ Saved {len(all_tasks)} tasks to: {output_file}")
        
        # Detailed stats
        print(f"\nüìä Processing Statistics:")
        print(f"   Total raw tasks: {stats['total_raw']:,}")
        print(f"   Invalid (too short/bot): {stats['invalid']:,}")
        print(f"   Duplicates removed: {stats['duplicates']:,}")
        print(f"   Clean tasks: {stats['cleaned']:,}")
        print(f"   Retention rate: {stats['cleaned']/max(stats['total_raw'],1)*100:.1f}%")
        
        # Breakdown by source
        sources = Counter(task['source'] for task in all_tasks)
        print(f"\n   By source:")
        for source, count in sources.items():
            print(f"     {source}: {count:,}")
        
        # Breakdown by type
        types = Counter(task['issue_type'] for task in all_tasks if task['issue_type'])
        print(f"\n   By type:")
        for itype, count in sorted(types.items(), key=lambda x: -x[1])[:10]:
            print(f"     {itype}: {count:,}")
        
        # Priority distribution
        priorities = Counter(task['priority'] for task in all_tasks if task['priority'])
        print(f"\n   By priority:")
        for priority, count in priorities.items():
            print(f"     {priority}: {count:,}")
        
        # Quality metrics
        with_desc = sum(1 for t in all_tasks if t['description'])
        with_ac = sum(1 for t in all_tasks if t['acceptance_criteria'])
        
        print(f"\n   Quality metrics:")
        print(f"     With description: {with_desc:,} ({with_desc/len(all_tasks)*100:.1f}%)")
        print(f"     With AC: {with_ac:,} ({with_ac/len(all_tasks)*100:.1f}%)")
        
        avg_ac = sum(len(t['acceptance_criteria']) for t in all_tasks) / max(with_ac, 1)
        print(f"     Avg AC per task: {avg_ac:.1f}")


def main():
    # Find all JSONL files in data/external
    data_dir = Path('data/external')
    
    if not data_dir.exists():
        print(f"‚ùå Directory not found: {data_dir}")
        print("   Run pull_jira.py or pull_trello.py first")
        return 1
    
    # Find Jira and Trello files
    input_files = list(data_dir.glob('jira_issues_*.jsonl'))
    input_files.extend(data_dir.glob('trello_cards_*.jsonl'))
    
    if not input_files:
        print(f"‚ùå No JSONL files found in {data_dir}")
        print("   Expected: jira_issues_*.jsonl or trello_cards_*.jsonl")
        return 1
    
    print(f"Found {len(input_files)} file(s) to process:")
    for f in input_files:
        print(f"  - {f.name}")
    
    # Output file
    output_file = data_dir / 'tasks_corpus.jsonl'
    
    # Clean and merge
    cleaner = DataCleaner(min_title_length=5, min_desc_length=20)
    cleaner.process_files(input_files, output_file)
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
